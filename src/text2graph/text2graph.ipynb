{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge graph generation from scientific text\n",
    "\n",
    "In this jupyter notebook we show the demo for the full end to end framework for knowledge graph generation. We first take the input text which is present in the `./Data/TestData/` folder. This folder contains abstracts of all the papers from the website [paperswithcode.com](https://paperswithcode.com/). We use this as our dataset to create reproducible graphs of different modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "The required packages are listed in [the requirements file](./requirements.txt) located under `text2graph` folder. The following command could be used to install these packages in the target Python environment:\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "Detailed Instructions:\n",
    "\n",
    "Below are more detailed sample commands to create a new environment named 't2g' using conda command line, to install the requirements, and to run the notebook using this new environment as its kernel:\n",
    "\n",
    "`conda create --name t2g python=3.6\n",
    "activate t2g\n",
    "cd 'path-to-text2graph-folder'\n",
    "pip install -r requirements.txt\n",
    "python -m ipykernel install --user --name=t2g\n",
    "jupyter notebook`\n",
    "\n",
    "Select the kernel 't2g' in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the libraries\n",
    "\n",
    "Import all the requisite `spacy` libraries. Also instantiate the data folders. The different directories are:\n",
    "\n",
    "* NER Models at `./Models/`\n",
    "* Paper abstracts at `./Data/TestData/`\n",
    "* NER Model outputs at `./Output/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import plac\n",
    "\n",
    "model_dir = './Models/'\n",
    "test_dir = './Data/TestData/'\n",
    "output_dir = './Output/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NER Models\n",
    "\n",
    "Now load the NER models that we saved from the NER pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the abstracts\n",
    "\n",
    "Read the abstracts that are located at the `/Data/TestData/` folder. The abstracts are read and then *tokenized* using the `nltk` function `sent_tokenize`. This creates a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "abstracts = []\n",
    "sentences = []\n",
    "ent_tagged_text = []\n",
    "\n",
    "def tag_entity_text(sentence,text, replacementText):\n",
    "    newString1 = \"\"\n",
    "    for (t,r) in zip(text,replacementText):\n",
    "        newString1 = sentence.replace(t,r)\n",
    "        sentence = newString1\n",
    "    return sentence\n",
    "\n",
    "with open('./Data/TestData/LeNet.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    abstxt = ''.join(str(line) for line in lines)\n",
    "    abstxt = abstxt.lower()\n",
    "    abstracts.append(abstxt)\n",
    "    \n",
    "\n",
    "# tokenize the abstracts into a sentence\n",
    "for abstract in abstracts:\n",
    "    sents = nltk.sent_tokenize(abstract)\n",
    "    for sent in sents:\n",
    "        sentences.append(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NER tagged sentences from our Spacy model\n",
    "\n",
    "Each of these sentences is then sent to `spacy`'s `nlp` function, which tags the entities. A tuple is returned from which we can extract the following:\n",
    "\n",
    "* `.ents` returns all the entities tagged by `spacy`\n",
    "* `.text` gives the entity texts that have been tagged\n",
    "\n",
    "\n",
    "### Convert Spacy tagged sentences to semeval\n",
    "The `spacy` tagged sentences are converted to `semeval` style markup text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<e1>convolutional neural networks</e1> are are a special kind of <e2>multi-layer neural networks</e2>.\n",
      "\n",
      "\n",
      "lenet-5 is our latest convolutional network designed for <e1>handwritten</e1> and <e2>machine-printed character recognition</e2>.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "replacementText = []\n",
    "semeval_tagged = []\n",
    "for sentence in sentences:\n",
    "#     print(sentence + \" : : \")\n",
    "    ner_tagged = nlp(sentence)\n",
    "    tagged_entities = ner_tagged.ents\n",
    "    tuple_length = len(tagged_entities)\n",
    "    semevalify = \"\"\n",
    "    if(tuple_length == 2):\n",
    "        text = []\n",
    "        replacementText = []\n",
    "        for (i,ent) in enumerate(tagged_entities):\n",
    "#             print(str(i) + ent.text)\n",
    "            text.append(ent.text)\n",
    "            replacementText.append('<e' + str(i+1) + '>' + ent.text + '</e' + str(i+1) + '>')\n",
    "            semevalify = tag_entity_text(sentence,text,replacementText)\n",
    "    semeval_tagged.append(semevalify)\n",
    "            \n",
    "semeval_tagged = list(filter(None, semeval_tagged))\n",
    "# print(type(semeval_tagged))\n",
    "for sentence in semeval_tagged:\n",
    "    print(sentence)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the saved Relationship classification module\n",
    "\n",
    "The `semeval` tagged list of sentences `semeval_tagged` is then sent to the next module, which will classify the relationship. For that we first import the requisite libraries. Then the saved relationship extraction model is loaded from `/DCC/src/text2graph/model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dfradkin\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\dfradkin\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_index.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7dac522595a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasTextClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\home\\projects\\DARPA ASKE\\DCC-Github\\src\\text2graph\\models.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}_index.pkl\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_index.pkl'"
     ]
    }
   ],
   "source": [
    "from models import KerasTextClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = KerasTextClassifier()\n",
    "clf.load(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the available relationship classes\n",
    "\n",
    "The following lines of code lists the relationship classes that our model can handle and extract. Since the classes are `label-encoded` we create a dictionary to map them to their actual names. This would be helpful later in knowledge graph triple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Compare\n",
      "1: Conjunction\n",
      "2: Feature-of\n",
      "3: Part-of\n",
      "4: Used-for\n",
      "5: isA\n"
     ]
    }
   ],
   "source": [
    "# Create a label dictionary\n",
    "label_dict = {}\n",
    "for i,c in enumerate(list(clf.encoder.classes_)):\n",
    "    print(str(i) + \": \" + c)\n",
    "    label_dict[i] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the relationship for the classes\n",
    "\n",
    "For the sentences from the abstract, we now classify and predict the relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(semeval_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<e1>convolutional neural networks</e1> are designed to <e2>recognize visual</e2> patterns directly from pixel images with minimal preprocessing.:\n",
      "Used-for\n",
      "\n",
      "lenet-5 is our <e1>latest convolutional network</e1> designed for <e2>handwritten</e2> and machine-printed character recognition.:\n",
      "Used-for\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Show predictions side by side\n",
    "for i,sentence in enumerate(semeval_tagged):\n",
    "    print(sentence + \":\\n\" +  label_dict.get(y_pred[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_graph(sentence,predicted_class):\n",
    "    e1 = re.search('<e1>(.+?)</e1>',sentence)\n",
    "    e2 = re.search('<e2>(.+?)</e2>',sentence)\n",
    "    return (e1.group(1),predicted_class,e2.group(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('convolutional neural networks', 'Used-for', 'recognize visual')\n",
      "('latest convolutional network', 'Used-for', 'handwritten')\n"
     ]
    }
   ],
   "source": [
    "for i,sentence in enumerate(semeval_tagged):\n",
    "    print(generate_graph(sentence,label_dict.get(y_pred[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
