Deep neural networks have been successfully applied to many applications, but one of the major criticisms is their being black boxes - no satisfactory explanation of their 
behavior can be easily offered. Given a neural network f() with input x, one fundamental question to ask is: how does a perturbation in the input space affect the output
prediction? To formally answer this question and bound the behavior of neural networks, a critical step is to compute the uniform bounds of the Jacobian matrix Bfpxq/Bx for 
all x within a certain region. Many recent works on understanding or verifying the behavior of neural networks rely on this quantity. For example, once a (local) Jacobian bound is
computed, one can immediately know the radius of a guaranteed "safe region" in the input space, where no adversarial perturbation can change the output label (Hein and Andriushchenko 
2017; Weng et al. 2018b). This is also referred to as the robustness veriffcation problem. In generative adversarial networks (GANs) (Goodfellow et al. 2014), the training process 
suffers from the gradient vanishing problem and can be very unstable. Adding the Lipschitz constant of the discriminator network as a constraint (Arjovsky, Chintala, and Bottou 2017; 
Miyato et al. 2018) or as a regularizer (Gulrajani et al. 2017) signiffcantly improves the training stability of GANs. For neural networks, the Jacobian matrix Bfpxq/Bx is also 
closely related to its Jacobian matrix with respect to the weights Bfpx;Wq/BW , whose bound directly characterizes the generalization gap in supervised learning and GANs; see, 
e.g., (Vapnik and Vapnik 1998; Sriperumbudur et al. 2009; Bartlett, Foster, and Telgarsky 2017; Arora and Zhang 2018; Zhang et al. 2017). Computing bounds for Jacobian (or gradient) 
is very challenging even for a simple ReLU network, and how to efficiently provide a tight bound is still an open problem for deep neural networks. Previous attempts for computing
Jacobian bounds can be summarized into three categories. Sampling approaches (Wood and Zhang 1996; Weng et al. 2018b) estimate the Jacobian bound by sampling points and estimating 
the maximum gradient, but the computed quantity is usually an under-estimation, and as a result not a certified bound. Another line of work simply bounds the norm of Jacobian matrix 
over the entire domain (i.e. global Lipschitz constant) by the product of operator norms of the weight matrices (Szegedy et al. 2013; Cisse et al. 2017; Elsayed et al. 2018). 
Unfortunately, this is a very loose global upper bound, especially when we are only interested in a small local region of a neural network. Finally, some recent works focus on 
computing Lipschitz constant in ReLU networks: (Raghunathan, Steinhardt, and Liang 2018) solves a semi-definite programming (SDP) problem to give a Lipschitz constant, but its 
computational cost is high and it only applies to 2-layer networks; (Weng et al. 2018b) can be applied to multi-layer ReLU networks but the bound quickly loses its power when the 
network goes deeper. In this paper, we propose a novel recursive algorithm, dubbed RecurJac, for efficiently computing a certified Jacobian bound. Unlike the layer-by-layer 
algorithm (Fast-Lip) for ReLU network in (Weng et al. 2018b), we develop a recursive refinement procedure that significantly outperforms Fast-Lip on ReLU networks, and our algorithm
is general enough to be applied to networks with most common activation functions, not limited to ReLU. Our key observation is that the Jacobian bounds of previous layers can be used 
to reduce the uncertainties of neuron activations in the current layer, and some uncertain neurons can be fixed without affecting the final bound. We can then absorb these fixed 
neurons into the previous layers' weight matrix, which results in bounding Jacobian matrix for another shallower network. This technique can be applied recursively to get a 
tighter final bound. Compared with the non-recursive algorithm (Fast-Lip), RecurJac increases the computation cost by at most H times (H is depth of the network), which is reasonable 
even for relatively large networks. We apply RecurJac to various applications. First, we can investigate the local optimization landscape after obtaining the upper and lower bounds of 
Jacobian matrix, by guaranteeing that no stationary points exist inside a certain region. Experimental results show that the radius of this region steadily decreases when networks 
become deeper. Second, RecurJac can find a local Lipschitz constant, which up to two magnitudes smaller than the state-ofthe-art algorithm without a recursive structure 
(Figure 1). Finally, we can use RecurJac to evaluate the robustness of neural networks, by giving a certified lower bound within which no adversarial examples can be found.