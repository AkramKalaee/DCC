regard (xr, xf ) as one whole random variable, and from (2) we get  arg max  t  = arg max  t  e(xr,xf )∼p (xr,xf )[log σ(t (xr, xf ))] + e(xr,xf )∼q(xr,xf )[log(1 − σ(t (xr, xf )))] e(xr,xf )∼ ˜p(xr)q(xf )[log σ(t (xr, xf )) + log(1 − σ(t (xf , xr)))]  then from (3) we have  arg min  g  = arg min  g  = arg min  g  e(xr,xf )∼q(xr,xf )[h(t (xr, xf ))] exr∼ ˜p(xr),xf∼q(xf )[h(t (xf , xr))] exr∼ ˜p(xr),z∼q(z)[h(t (g(z), xr))]  therefore, we can train a generative model by alternately running the following two steps:  arg max  t  e(xr,xf )∼ ˜p(xr)q(xf )[log σ(t (xr, xf )) + log(1 − σ(t (xf , xr)))]  arg min  g  exr∼ ˜p(xr),xf∼q(xf )[h(t (xf , xr))]  a natural choice of h leads to  e(xr,xf )∼ ˜p(xr)q(xf )[log σ(t (xr, xf )) + log(1 − σ(t (xf , xr)))] e(xr,xf )∼ ˜p(xr)q(xf )[log(1 − σ(t (xr, xf ))) + log σ(t (xf , xr))]  arg max  t  arg max  g  2.2 under wgans  corresponding to (8), we can estimate wasserstein distance between p (xr, xf ) and q(xr, xf ) by  w (p (xr, xf ), q(xr, xf ))  e(xr,xf )∼p (xr,xf )[t (xr, xf )] − e(xr,xf )∼q(xr,xf )[t (xr, xf )] e(xr,xf )∼ ˜p(xr)q(xf )[t (xr, xf ) − t (xf , xr)]  = sup  (cid:107)t(cid:107)l≤1  = sup  (cid:107)t(cid:107)l≤1  hence we can train a generative model by a new min-max game:  arg min  g  arg max t,(cid:107)t(cid:107)l≤1  e(xr,xf )∼ ˜p(xr)q(xf )[t (xr, xf ) − t (xf , xr)]  it is a really pretty result, which allows us to use an exactly symmetrical target to train discriminator and generator.