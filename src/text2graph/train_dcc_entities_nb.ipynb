{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new NER model is trained that contains new entities. The new entities are\n",
    "# defined in the list: ['Method', 'Generic', 'Task', 'Material', 'Eval', 'Other']\n",
    "# The new model takes as input annotated sentences extracted from pdf files,\n",
    "# describing methods, architectures, and applications of Deep Learning. The\n",
    "# sentences have been annotated using Brat (http://brat.nlplab.org/).\n",
    "# The training is done by using the statistical models provided by spaCy\n",
    "# (https://spacy.io/). The trained model can be saved in a user defined folder\n",
    "# for future use.\n",
    "#\n",
    "# This material is based upon work supported by Defense Advanced Research\n",
    "# Projects Agency (DARPA) under Agreement No. HR00111990010\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import time\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "from brat2spacy import *\n",
    "from ner_model_eval import *\n",
    "from test_dcc_entities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new entity label\n",
    "new_entities_list = ['Method', 'Generic', 'Task', 'Material', 'Eval', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'Data/Abstracts-annotated30/'\n",
    "output_dir = 'Models/'\n",
    "test_dir = 'Data/TestData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function that sets up the SpaCy pipeline and entity recognizer. The new entities are defined as a list of strings.\n",
    "# Input -\n",
    "#   model: the name of an existing trained model\n",
    "#   new_model_name: the name of the new entity model\n",
    "#   output_dir: the path of the directory where the new trained model will be saved.\n",
    "#   n_iter: number of training iterations (epochs)\n",
    "# Output -\n",
    "#   The trained entity model stored in the output_dir\n",
    "def main(model=None, new_model_name='DCC_ent', input_dir=input_dir, output_dir=output_dir, test_dir=test_dir, n_iter=50):\n",
    "    # create the training from annotated data produced by using Brat\n",
    "    training_data = create_training_data(input_dir)\n",
    "\n",
    "    # check if the user provides an existing language model\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded existing model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"No model provided, created blank 'en' model\")\n",
    "\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        # otherwise, get it, so we can add labels to it\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add all new entities to the recognizer\n",
    "    for i in range(len(new_entities_list)):\n",
    "      ner.add_label(new_entities_list[i])\n",
    "\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # start the training of the recognizer (and the time)\n",
    "    training_start_time = time.time()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(training_data)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(training_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                       losses=losses)\n",
    "        print('iter:', itn)\n",
    "        print('Losses', losses)\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    print(\"training time: \", training_end_time-training_start_time)\n",
    "\n",
    "\n",
    "    ############################\n",
    "    # test the ner model on a set of text data taken from papers\n",
    "    if test_dir is not None:\n",
    "        test_ner_model(nlp, test_dir)\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    # model evaluation\n",
    "    #\n",
    "    # define a set of examples that will be used as ground truth\n",
    "    examples = [\n",
    "        ('Deep learning is applied in many every day application with great success in object recognition.',\n",
    "         [(0, 13, 'Method'), (77, 95, 'Task')]),\n",
    "        ('Recurrent neural networks are used for forecasting and natural language processing.',\n",
    "         [(0, 25, 'Method'), (39, 50, 'Task'), (55, 82, 'Task')]),\n",
    "        ('Convolutional neural networks are frequently used in object recognition and medical image processing.',\n",
    "         [(0, 25, 'Method'), (39, 50, 'Task'), (55, 82, 'Task')])\n",
    "    ]\n",
    "    res = ner_eval(nlp, examples)\n",
    "    print(\"\\nModel evaluation results:\")\n",
    "    print(res)\n",
    "\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    # save trained model\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"The model was saved to the directory: \", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        #print(\"Loading from\", output_dir)\n",
    "        #nlp2 = spacy.load(output_dir)\n",
    "        #doc2 = nlp2(test_text)\n",
    "        #for ent in doc2.ents:\n",
    "        #    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model provided, created blank 'en' model\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "iter: 0\n",
      "Losses {'ner': 108.21851252304975}\n",
      "iter: 1\n",
      "Losses {'ner': 85.76653762509036}\n",
      "iter: 2\n",
      "Losses {'ner': 81.27281272419792}\n",
      "iter: 3\n",
      "Losses {'ner': 74.9332996900288}\n",
      "iter: 4\n",
      "Losses {'ner': 74.14935253512775}\n",
      "iter: 5\n",
      "Losses {'ner': 69.09996159855076}\n",
      "iter: 6\n",
      "Losses {'ner': 59.436107283175836}\n",
      "iter: 7\n",
      "Losses {'ner': 52.70170359487369}\n",
      "iter: 8\n",
      "Losses {'ner': 51.998610874995855}\n",
      "training time:  127.2059998512268\n",
      "\n",
      "Entities detected in the text: 'non-local methods exploiting the self-similarity of natural signals have been well studied, for example in image analysis and restoration. existing approaches, however, rely on k-nearest neighbors (knn) matching in a ﬁxed feature space. the main hurdle in optimizing this feature space w. r. t. application performance is the non-differentiability of the knn selection rule. to overcome this, we propose a continuous deterministic relaxation of knn selection that maintains differentiability w. r. t. pairwise distances, but retains the original knn as the limit of a temperature parameter approaching zero. to exploit our relaxation, we propose the neural nearest neighbors block (n3 block), a novel non-local processing layer that leverages the principle of self-similarity and can be used as building block in modern neural network architectures.1 we show its effectiveness for the set reasoning task of correspondence classiﬁcation as well as for image restoration, including image denoising and single image super-resolution, where we outperform strong convolutional neural network (cnn) baselines and recent non-local models that rely on knn selection in hand-chosen features spaces.  1'\n",
      "Method non-local methods exploiting 0 28\n",
      "Task self-similarity 33 48\n",
      "Generic natural 52 59\n",
      "Generic image analysis 107 121\n",
      "Generic restoration 126 137\n",
      "Generic approaches 148 158\n",
      "Method knn 198 201\n",
      "Method feature space 222 235\n",
      "Method feature space 272 285\n",
      "Method performance 307 318\n",
      "Generic non- 326 330\n",
      "Generic knn selection 355 368\n",
      "Generic rule 369 373\n",
      "Other original knn 537 549\n",
      "Generic neural nearest 650 664\n",
      "Method novel non-local processing layer 695 727\n",
      "Method self-similarity and 760 779\n",
      "Method neural network 820 834\n",
      "Method effectiveness 863 876\n",
      "Method correspondence classiﬁcation 907 935\n",
      "Method strong convolutional neural network 1051 1086\n",
      "Method cnn 1088 1091\n",
      "Generic baselines 1093 1102\n",
      "Generic knn selection 1144 1157\n",
      "Generic hand- 1161 1166\n",
      "Generic features spaces 1173 1188\n",
      "\n",
      "Entities detected in the text: 'b'Method' b'non-local methods exploiting' 0 28\n",
      "b'Task' b'self-similarity' 33 48\n",
      "b'Generic' b'natural' 52 59\n",
      "b'Generic' b'image analysis' 107 121\n",
      "b'Generic' b'restoration' 126 137\n",
      "b'Generic' b'approaches' 148 158\n",
      "b'Method' b'knn' 198 201\n",
      "b'Method' b'feature space' 222 235\n",
      "b'Method' b'feature space' 272 285\n",
      "b'Method' b'performance' 307 318\n",
      "b'Generic' b'non-' 326 330\n",
      "b'Generic' b'knn selection' 355 368\n",
      "b'Generic' b'rule' 369 373\n",
      "b'Other' b'original knn' 537 549\n",
      "b'Generic' b'neural nearest' 650 664\n",
      "b'Method' b'novel non-local processing layer' 695 727\n",
      "b'Method' b'self-similarity and' 760 779\n",
      "b'Method' b'neural network' 820 834\n",
      "b'Method' b'effectiveness' 863 876\n",
      "b'Method' b'correspondence classi\\xef\\xac\\x81cation' 907 935\n",
      "b'Method' b'strong convolutional neural network' 1051 1086\n",
      "b'Method' b'cnn' 1088 1091\n",
      "b'Generic' b'baselines' 1093 1102\n",
      "b'Generic' b'knn selection' 1144 1157\n",
      "b'Generic' b'hand-' 1161 1166\n",
      "b'Generic' b'features spaces' 1173 1188\n",
      "'\n",
      "Method b'neural network 605 621\n",
      "Method convolutional neural network 746 774\n",
      "Method spaces 937 943\n",
      "\n",
      "Entities detected in the text: 'b'Method' b\"b'neural network\" 605 621\n",
      "b'Method' b'convolutional neural network' 746 774\n",
      "b'Method' b'spaces' 937 943\n",
      "'\n",
      "Method b\"b'neural network 10 28\n",
      "Method b'convolutional neural network 48 78\n",
      "Method 774 84 87\n",
      "\n",
      "Entities detected in the text: 'b'Method' b'b\"b\\'neural network' 10 28\n",
      "b'Method' b\"b'convolutional neural network\" 48 78\n",
      "b'Method' b'774' 84 87\n",
      "'\n",
      "Method b\"b'convolutional neural network 49 81\n",
      "\n",
      "Entities detected in the text: 'Deep neural networks have been successfully applied to many applications, but one of the major criticisms is their being black boxes - no satisfactory explanation of their \n",
      "behavior can be easily offered. Given a neural network f() with input x, one fundamental question to ask is: how does a perturbation in the input space affect the output\n",
      "prediction? To formally answer this question and bound the behavior of neural networks, a critical step is to compute the uniform bounds of the Jacobian matrix Bfpxq/Bx for \n",
      "all x within a certain region. Many recent works on understanding or verifying the behavior of neural networks rely on this quantity. For example, once a (local) Jacobian bound is\n",
      "computed, one can immediately know the radius of a guaranteed \"safe region\" in the input space, where no adversarial perturbation can change the output label (Hein and Andriushchenko \n",
      "2017; Weng et al. 2018b). This is also referred to as the robustness veriffcation problem. In generative adversarial networks (GANs) (Goodfellow et al. 2014), the training process \n",
      "suffers from the gradient vanishing problem and can be very unstable. Adding the Lipschitz constant of the discriminator network as a constraint (Arjovsky, Chintala, and Bottou 2017; \n",
      "Miyato et al. 2018) or as a regularizer (Gulrajani et al. 2017) signiffcantly improves the training stability of GANs. For neural networks, the Jacobian matrix Bfpxq/Bx is also \n",
      "closely related to its Jacobian matrix with respect to the weights Bfpx;Wq/BW , whose bound directly characterizes the generalization gap in supervised learning and GANs; see, \n",
      "e.g., (Vapnik and Vapnik 1998; Sriperumbudur et al. 2009; Bartlett, Foster, and Telgarsky 2017; Arora and Zhang 2018; Zhang et al. 2017). Computing bounds for Jacobian (or gradient) \n",
      "is very challenging even for a simple ReLU network, and how to efficiently provide a tight bound is still an open problem for deep neural networks. Previous attempts for computing\n",
      "Jacobian bounds can be summarized into three categories. Sampling approaches (Wood and Zhang 1996; Weng et al. 2018b) estimate the Jacobian bound by sampling points and estimating \n",
      "the maximum gradient, but the computed quantity is usually an under-estimation, and as a result not a certified bound. Another line of work simply bounds the norm of Jacobian matrix \n",
      "over the entire domain (i.e. global Lipschitz constant) by the product of operator norms of the weight matrices (Szegedy et al. 2013; Cisse et al. 2017; Elsayed et al. 2018). \n",
      "Unfortunately, this is a very loose global upper bound, especially when we are only interested in a small local region of a neural network. Finally, some recent works focus on \n",
      "computing Lipschitz constant in ReLU networks: (Raghunathan, Steinhardt, and Liang 2018) solves a semi-definite programming (SDP) problem to give a Lipschitz constant, but its \n",
      "computational cost is high and it only applies to 2-layer networks; (Weng et al. 2018b) can be applied to multi-layer ReLU networks but the bound quickly loses its power when the \n",
      "network goes deeper. In this paper, we propose a novel recursive algorithm, dubbed RecurJac, for efficiently computing a certified Jacobian bound. Unlike the layer-by-layer \n",
      "algorithm (Fast-Lip) for ReLU network in (Weng et al. 2018b), we develop a recursive refinement procedure that significantly outperforms Fast-Lip on ReLU networks, and our algorithm\n",
      "is general enough to be applied to networks with most common activation functions, not limited to ReLU. Our key observation is that the Jacobian bounds of previous layers can be used \n",
      "to reduce the uncertainties of neuron activations in the current layer, and some uncertain neurons can be fixed without affecting the final bound. We can then absorb these fixed \n",
      "neurons into the previous layers' weight matrix, which results in bounding Jacobian matrix for another shallower network. This technique can be applied recursively to get a \n",
      "tighter final bound. Compared with the non-recursive algorithm (Fast-Lip), RecurJac increases the computation cost by at most H times (H is depth of the network), which is reasonable \n",
      "even for relatively large networks. We apply RecurJac to various applications. First, we can investigate the local optimization landscape after obtaining the upper and lower bounds of \n",
      "Jacobian matrix, by guaranteeing that no stationary points exist inside a certain region. Experimental results show that the radius of this region steadily decreases when networks \n",
      "become deeper. Second, RecurJac can find a local Lipschitz constant, which up to two magnitudes smaller than the state-ofthe-art algorithm without a recursive structure \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Figure 1). Finally, we can use RecurJac to evaluate the robustness of neural networks, by giving a certified lower bound within which no adversarial examples can be found.'\n",
      "Method Deep neural networks 0 20\n",
      "Generic major criticisms 89 105\n",
      "Generic behavior 173 181\n",
      "Method neural network 213 227\n",
      "Generic input space 313 324\n",
      "Generic critical step 433 446\n",
      "Generic Jacobian matrix 487 502\n",
      "Generic x within a 521 531\n",
      "Task certain region 532 546\n",
      "Method quantity 641 649\n",
      "Generic Jacobian bound 679 693\n",
      "Material know 727 731\n",
      "Generic safe region 760 771\n",
      "Generic input space 780 791\n",
      "Generic robustness veriffcation 939 962\n",
      "Method generative adversarial networks 975 1006\n",
      "Method GANs 1008 1012\n",
      "Generic training process 1044 1060\n",
      "Generic Lipschitz constant 1143 1161\n",
      "Method discriminator network 1169 1190\n",
      "Generic 2017 1304 1308\n",
      "Method neural networks 1369 1384\n",
      "Generic Jacobian matrix 1390 1405\n",
      "Generic Jacobian matrix 1447 1462\n",
      "Generic generalization gap 1543 1561\n",
      "Method Telgarsky 2017; Arora 1681 1702\n",
      "Generic 2017 1732 1736\n",
      "Method deep neural networks 1910 1930\n",
      "Method Previous attempts for computing\n",
      "Jacobian bounds can 1932 1983\n",
      "Material approaches 2030 2040\n",
      "Generic Wood 2042 2046\n",
      "Generic Jacobian bound 2095 2109\n",
      "Generic sampling points 2113 2128\n",
      "Generic maximum gradient 2149 2165\n",
      "Method under-estimation 2207 2223\n",
      "Generic Jacobian matrix 2311 2326\n",
      "Generic entire domain 2337 2350\n",
      "Generic global Lipschitz 2357 2373\n",
      "Method operator norms 2402 2416\n",
      "Generic weight matrices 2424 2439\n",
      "Generic very loose 2529 2539\n",
      "Method small local region 2604 2622\n",
      "Method neural network 2628 2642\n",
      "Task some recent 2653 2664\n",
      "Generic networks 2718 2726\n",
      "Generic Liang 2758 2763\n",
      "Generic semi-definite 2779 2792\n",
      "Task SDP 2806 2809\n",
      "Method computational cost is 2858 2879\n",
      "Method multi-layer ReLU networks but the bound quickly loses its power when the \n",
      "network goes deeper. In this paper, we 2964 3076\n",
      "Method novel recursive algorithm 3087 3112\n",
      "Method RecurJac 3121 3129\n",
      "Generic layer- 3196 3202\n",
      "Method Fast-Lip 3223 3231\n",
      "Generic ReLU network 3237 3249\n",
      "Method recursive refinement procedure 3287 3317\n",
      "Method Fast 3349 3353\n",
      "Task our algorithm 3380 3393\n",
      "Task most common 3443 3454\n",
      "Generic Jacobian bounds 3530 3545\n",
      "Generic previous layers 3549 3564\n",
      "Task neuron activations 3609 3627\n",
      "Material some uncertain neurons 3654 3676\n",
      "Generic previous layers 3774 3789\n",
      "Generic weight matrix 3791 3804\n",
      "Generic Jacobian matrix 3832 3847\n",
      "Method shallower network 3860 3877\n",
      "Generic non- 3970 3974\n",
      "Method recursive algorithm 3974 3993\n",
      "Task Fast- 3995 4000\n",
      "Generic , RecurJac 4004 4014\n",
      "Generic computation cost 4029 4045\n",
      "Method RecurJac 4160 4168\n",
      "Generic Jacobian matrix 4300 4315\n",
      "Generic stationary points 4341 4358\n",
      "Generic certain region 4374 4388\n",
      "Method Second 4496 4502\n",
      "Generic RecurJac 4504 4512\n",
      "Generic state- 4594 4600\n",
      "Method recursive structure \n",
      " 4630 4651\n",
      "Method 1 4659 4660\n",
      "\n",
      "Entities detected in the text: 'b'Method' b'Deep neural networks' 0 20\n",
      "b'Generic' b'major criticisms' 89 105\n",
      "b'Generic' b'behavior' 173 181\n",
      "b'Method' b'neural network' 213 227\n",
      "b'Generic' b'input space' 313 324\n",
      "b'Generic' b'critical step' 433 446\n",
      "b'Generic' b'Jacobian matrix' 487 502\n",
      "b'Generic' b'x within a' 521 531\n",
      "b'Task' b'certain region' 532 546\n",
      "b'Method' b'quantity' 641 649\n",
      "b'Generic' b'Jacobian bound' 679 693\n",
      "b'Material' b'know' 727 731\n",
      "b'Generic' b'safe region' 760 771\n",
      "b'Generic' b'input space' 780 791\n",
      "b'Generic' b'robustness veriffcation' 939 962\n",
      "b'Method' b'generative adversarial networks' 975 1006\n",
      "b'Method' b'GANs' 1008 1012\n",
      "b'Generic' b'training process' 1044 1060\n",
      "b'Generic' b'Lipschitz constant' 1143 1161\n",
      "b'Method' b'discriminator network' 1169 1190\n",
      "b'Generic' b'2017' 1304 1308\n",
      "b'Method' b'neural networks' 1369 1384\n",
      "b'Generic' b'Jacobian matrix' 1390 1405\n",
      "b'Generic' b'Jacobian matrix' 1447 1462\n",
      "b'Generic' b'generalization gap' 1543 1561\n",
      "b'Method' b'Telgarsky 2017; Arora' 1681 1702\n",
      "b'Generic' b'2017' 1732 1736\n",
      "b'Method' b'deep neural networks' 1910 1930\n",
      "b'Method' b'Previous attempts for computing\\nJacobian bounds can' 1932 1983\n",
      "b'Material' b'approaches' 2030 2040\n",
      "b'Generic' b'Wood' 2042 2046\n",
      "b'Generic' b'Jacobian bound' 2095 2109\n",
      "b'Generic' b'sampling points' 2113 2128\n",
      "b'Generic' b'maximum gradient' 2149 2165\n",
      "b'Method' b'under-estimation' 2207 2223\n",
      "b'Generic' b'Jacobian matrix' 2311 2326\n",
      "b'Generic' b'entire domain' 2337 2350\n",
      "b'Generic' b'global Lipschitz' 2357 2373\n",
      "b'Method' b'operator norms' 2402 2416\n",
      "b'Generic' b'weight matrices' 2424 2439\n",
      "b'Generic' b'very loose' 2529 2539\n",
      "b'Method' b'small local region' 2604 2622\n",
      "b'Method' b'neural network' 2628 2642\n",
      "b'Task' b'some recent' 2653 2664\n",
      "b'Generic' b'networks' 2718 2726\n",
      "b'Generic' b'Liang' 2758 2763\n",
      "b'Generic' b'semi-definite' 2779 2792\n",
      "b'Task' b'SDP' 2806 2809\n",
      "b'Method' b'computational cost is' 2858 2879\n",
      "b'Method' b'multi-layer ReLU networks but the bound quickly loses its power when the \\nnetwork goes deeper. In this paper, we' 2964 3076\n",
      "b'Method' b'novel recursive algorithm' 3087 3112\n",
      "b'Method' b'RecurJac' 3121 3129\n",
      "b'Generic' b'layer-' 3196 3202\n",
      "b'Method' b'Fast-Lip' 3223 3231\n",
      "b'Generic' b'ReLU network' 3237 3249\n",
      "b'Method' b'recursive refinement procedure' 3287 3317\n",
      "b'Method' b'Fast' 3349 3353\n",
      "b'Task' b'our algorithm' 3380 3393\n",
      "b'Task' b'most common' 3443 3454\n",
      "b'Generic' b'Jacobian bounds' 3530 3545\n",
      "b'Generic' b'previous layers' 3549 3564\n",
      "b'Task' b'neuron activations' 3609 3627\n",
      "b'Material' b'some uncertain neurons' 3654 3676\n",
      "b'Generic' b'previous layers' 3774 3789\n",
      "b'Generic' b'weight matrix' 3791 3804\n",
      "b'Generic' b'Jacobian matrix' 3832 3847\n",
      "b'Method' b'shallower network' 3860 3877\n",
      "b'Generic' b'non-' 3970 3974\n",
      "b'Method' b'recursive algorithm' 3974 3993\n",
      "b'Task' b'Fast-' 3995 4000\n",
      "b'Generic' b', RecurJac' 4004 4014\n",
      "b'Generic' b'computation cost' 4029 4045\n",
      "b'Method' b'RecurJac' 4160 4168\n",
      "b'Generic' b'Jacobian matrix' 4300 4315\n",
      "b'Generic' b'stationary points' 4341 4358\n",
      "b'Generic' b'certain region' 4374 4388\n",
      "b'Method' b'Second' 4496 4502\n",
      "b'Generic' b'RecurJac' 4504 4512\n",
      "b'Generic' b'state-' 4594 4600\n",
      "b'Method' b'recursive structure \\n' 4630 4651\n",
      "b'Method' b'1' 4659 4660\n",
      "'\n",
      "Method b'Deep neural networks 10 32\n",
      "Method b'neural network 118 134\n",
      "Method b'critical step 189 204\n",
      "Generic b'Jacobian matrix 225 242\n",
      "Material space 467 472\n",
      "Method b'discriminator network 704 727\n",
      "Generic b'Jacobian matrix 818 835\n",
      "Generic b'Jacobian matrix 858 875\n",
      "Method b'deep neural networks 1014 1036\n",
      "Method b'under-estimation 1319 1337\n",
      "Generic b'Jacobian matrix 1360 1377\n",
      "Material 2337 1417 1421\n",
      "Material norms 1489 1494\n",
      "Method b'neural network 1633 1649\n",
      "Method b'computational cost 1830 1850\n",
      "Generic \\nnetwork 1950 1959\n",
      "Method recursive algorithm' 3087 3112\n",
      " 2020 2051\n",
      "Method 3393 2295 2299\n",
      "Generic uncertain neurons 2472 2489\n",
      "Generic b'Jacobian matrix 2590 2607\n",
      "Generic ' 3970 2677 2683\n",
      "Method RecurJac 2774 2782\n",
      "Method b'Jacobian matrix' 2878 2896\n",
      "Generic b'stationary points 2918 2937\n",
      "Generic structure \\n 3104 3116\n",
      "\n",
      "Entities detected in the text: 'b'Method' b\"b'Deep neural networks\" 10 32\n",
      "b'Method' b\"b'neural network\" 118 134\n",
      "b'Method' b\"b'critical step\" 189 204\n",
      "b'Generic' b\"b'Jacobian matrix\" 225 242\n",
      "b'Material' b'space' 467 472\n",
      "b'Method' b\"b'discriminator network\" 704 727\n",
      "b'Generic' b\"b'Jacobian matrix\" 818 835\n",
      "b'Generic' b\"b'Jacobian matrix\" 858 875\n",
      "b'Method' b\"b'deep neural networks\" 1014 1036\n",
      "b'Method' b\"b'under-estimation\" 1319 1337\n",
      "b'Generic' b\"b'Jacobian matrix\" 1360 1377\n",
      "b'Material' b'2337' 1417 1421\n",
      "b'Material' b'norms' 1489 1494\n",
      "b'Method' b\"b'neural network\" 1633 1649\n",
      "b'Method' b\"b'computational cost\" 1830 1850\n",
      "b'Generic' b'\\\\nnetwork' 1950 1959\n",
      "b'Method' b\"recursive algorithm' 3087 3112\\n\" 2020 2051\n",
      "b'Method' b'3393' 2295 2299\n",
      "b'Generic' b'uncertain neurons' 2472 2489\n",
      "b'Generic' b\"b'Jacobian matrix\" 2590 2607\n",
      "b'Generic' b\"' 3970\" 2677 2683\n",
      "b'Method' b'RecurJac' 2774 2782\n",
      "b'Method' b\"b'Jacobian matrix'\" 2878 2896\n",
      "b'Generic' b\"b'stationary points\" 2918 2937\n",
      "b'Generic' b'structure \\\\n' 3104 3116\n",
      "'\n",
      "Method b\"b'Deep neural networks 10 34\n",
      "Method b\"b'neural network 52 70\n",
      "Method b\"b'critical step 90 107\n",
      "Generic b\"b'Jacobian matrix 128 147\n",
      "Method b\"b'discriminator network 196 221\n",
      "Generic b\"b'Jacobian matrix 242 261\n",
      "Generic b\"b'Jacobian matrix 282 301\n",
      "Method b\"b'deep neural networks 321 345\n",
      "Method 1014 1036\n",
      " 347 357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method b\"b'under-estimation 367 387\n",
      "Generic b\"b'Jacobian matrix 410 429\n",
      "Method b\"b'neural network 512 530\n",
      "Method b\"b'computational cost 552 574\n",
      "Generic b\"b'Jacobian matrix 758 777\n",
      "Generic ' 3970 802 808\n",
      "Generic 2677 2683 810 819\n",
      "Generic ' 2774 840 846\n",
      "Generic b\"b'Jacobian matrix 862 881\n",
      "Material 2878 884 888\n",
      "Generic b\"b'stationary points 905 926\n",
      "\n",
      "Entities detected in the text: 'b'Method' b'b\"b\\'Deep neural networks' 10 34\n",
      "b'Method' b'b\"b\\'neural network' 52 70\n",
      "b'Method' b'b\"b\\'critical step' 90 107\n",
      "b'Generic' b'b\"b\\'Jacobian matrix' 128 147\n",
      "b'Method' b'b\"b\\'discriminator network' 196 221\n",
      "b'Generic' b'b\"b\\'Jacobian matrix' 242 261\n",
      "b'Generic' b'b\"b\\'Jacobian matrix' 282 301\n",
      "b'Method' b'b\"b\\'deep neural networks' 321 345\n",
      "b'Method' b'1014 1036\\n' 347 357\n",
      "b'Method' b'b\"b\\'under-estimation' 367 387\n",
      "b'Generic' b'b\"b\\'Jacobian matrix' 410 429\n",
      "b'Method' b'b\"b\\'neural network' 512 530\n",
      "b'Method' b'b\"b\\'computational cost' 552 574\n",
      "b'Generic' b'b\"b\\'Jacobian matrix' 758 777\n",
      "b'Generic' b\"' 3970\" 802 808\n",
      "b'Generic' b'2677 2683' 810 819\n",
      "b'Generic' b\"' 2774\" 840 846\n",
      "b'Generic' b'b\"b\\'Jacobian matrix' 862 881\n",
      "b'Material' b'2878' 884 888\n",
      "b'Generic' b'b\"b\\'stationary points' 905 926\n",
      "'\n",
      "Method b'b\"b\\'Deep neural networks 10 37\n",
      "Method b'b\"b\\'neural network 55 76\n",
      "Method b'b\"b\\'critical step 94 114\n",
      "Method b'b\"b\\'Jacobian matrix' 134 157\n",
      "Method b'b\"b\\'discriminator network 176 204\n",
      "Method b'b\"b\\'Jacobian matrix' 225 248\n",
      "Generic b'b\"b\\'Jacobian matrix 268 290\n",
      "Method b'b\"b\\'deep neural networks 310 337\n",
      "Eval 1036\\n 364 370\n",
      "Method b'b\"b\\'under-estimation 390 413\n",
      "Generic b'b\"b\\'Jacobian matrix 434 456\n",
      "Method b'b\"b\\'neural network 476 497\n",
      "Eval cost 538 542\n",
      "Generic b'b\"b\\'Jacobian matrix 563 585\n",
      "Generic ' 3970 608 614\n",
      "Material \" 614 615\n",
      "Generic b'b\"b\\'Jacobian matrix 696 718\n",
      "Material b'2878 740 746\n",
      "Material 905 793 796\n",
      "\n",
      "Model evaluation results:\n",
      "{'uas': 0.0, 'las': 0.0, 'ents_p': 80.0, 'ents_r': 80.0, 'ents_f': 80.00000000000001, 'tags_acc': 0.0, 'token_acc': 100.0}\n",
      "The model was saved to the directory:  Models\n"
     ]
    }
   ],
   "source": [
    "nitr = 9\n",
    "main(model=None, new_model_name='DCC_ent_nb', input_dir=input_dir, output_dir=output_dir, test_dir=test_dir, n_iter=nitr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcc2",
   "language": "python",
   "name": "dcc2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
