{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new NER model is trained that contains new entities. The new entities are\n",
    "# defined in the list: ['Method', 'Generic', 'Task', 'Material', 'Eval', 'Other']\n",
    "# The new model takes as input annotated sentences extracted from pdf files,\n",
    "# describing methods, architectures, and applications of Deep Learning. The\n",
    "# sentences have been annotated using Brat (http://brat.nlplab.org/).\n",
    "# The training is done by using the statistical models provided by spaCy\n",
    "# (https://spacy.io/). The trained model can be saved in a user defined folder\n",
    "# for future use.\n",
    "#\n",
    "# This material is based upon work supported by Defense Advanced Research\n",
    "# Projects Agency (DARPA) under Agreement No. HR00111990010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import time\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "from brat2spacy import create_training_data\n",
    "from ner_utils import ner_eval, test_ner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new entity labels\n",
    "new_entities_list = ['Method', 'Generic', 'Task', 'Material', 'Eval', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './Data/Abstracts-annotated/'\n",
    "model_dir = './Models/'\n",
    "test_dir = './Data/TestData/'\n",
    "output_dir = './Output/'\n",
    "n_iter = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function that sets up the SpaCy pipeline and entity recognizer. The new entities are defined as a list of strings.\n",
    "# Input -\n",
    "#   model: the name of an existing trained model\n",
    "#   new_model_name: the name of the new entity model\n",
    "#   output_dir: the path of the directory where the new trained model will be saved.\n",
    "#   n_iter: number of training iterations (epochs)\n",
    "# Output -\n",
    "#   The trained entity model stored in the output_dir\n",
    "def main(model=None, new_model_name='DCC_ent', input_dir=input_dir, saved_model_dir=model_dir, output_dir=output_dir, test_dir=test_dir, n_iter=n_iter):\n",
    "    # create the training from annotated data produced by using Brat\n",
    "    training_data = create_training_data(input_dir)\n",
    "\n",
    "    # check if the user provides an existing language model\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded existing model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"No model provided, created blank 'en' model\")\n",
    "\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        # otherwise, get it, so we can add labels to it\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add all new entities to the recognizer\n",
    "    for i in range(len(new_entities_list)):\n",
    "      ner.add_label(new_entities_list[i])\n",
    "\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # start the training of the recognizer (and the time)\n",
    "    training_start_time = time.time()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(training_data)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(training_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                       losses=losses)\n",
    "        print('iter:', itn)\n",
    "        print('Losses', losses)\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    print(\"training time: \", training_end_time-training_start_time)\n",
    "\n",
    "\n",
    "    ############################\n",
    "    # test the ner model on a set of text data taken from papers\n",
    "    # (if the user does not provide text data, no testing will be performed)\n",
    "    if test_dir is not None:\n",
    "        # test_ner_model(nlp, test_dir)\n",
    "        test_ner_model(nlp, test_dir, output_dir)\n",
    "\n",
    "    ##########################\n",
    "    # model evaluation\n",
    "    #\n",
    "    # define a set of examples that will be used as ground truth\n",
    "    examples = [\n",
    "        ('Deep learning is applied in many every day application with great success in object recognition.',\n",
    "         [(0, 13, 'Method'), (77, 95, 'Task')]),\n",
    "        ('Recurrent neural networks are used for forecasting and natural language processing.',\n",
    "         [(0, 25, 'Method'), (39, 50, 'Task'), (55, 82, 'Task')]),\n",
    "        ('Convolutional neural networks are frequently used in object recognition and medical image processing.',\n",
    "         [(0, 25, 'Method'), (39, 50, 'Task'), (55, 82, 'Task')])\n",
    "    ]\n",
    "    res = ner_eval(nlp, examples)\n",
    "    print(\"\\nModel evaluation results:\")\n",
    "    print(res)\n",
    "\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    # save trained model\n",
    "    # (if the user does not provide a directory, the trained model will not be saved)\n",
    "    if saved_model_dir is not None:\n",
    "        saved_model_dir = Path(saved_model_dir)\n",
    "        if not saved_model_dir.exists():\n",
    "            saved_model_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(saved_model_dir)\n",
    "        print(\"The model was saved to the directory: \", saved_model_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        #print(\"Loading from\", output_dir)\n",
    "        #nlp2 = spacy.load(output_dir)\n",
    "        #doc2 = nlp2(test_text)\n",
    "        #for ent in doc2.ents:\n",
    "        #    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model provided, created blank 'en' model\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "iter: 0\n",
      "Losses {'ner': 192.771567211117}\n",
      "iter: 1\n",
      "Losses {'ner': 157.51821308484642}\n",
      "iter: 2\n",
      "Losses {'ner': 136.9222355215826}\n",
      "iter: 3\n",
      "Losses {'ner': 116.46061687250176}\n",
      "iter: 4\n",
      "Losses {'ner': 98.26374210054867}\n",
      "iter: 5\n",
      "Losses {'ner': 84.09783016325973}\n",
      "iter: 6\n",
      "Losses {'ner': 69.3227387772029}\n",
      "iter: 7\n",
      "Losses {'ner': 60.512604210877825}\n",
      "iter: 8\n",
      "Losses {'ner': 52.453447866341676}\n",
      "iter: 9\n",
      "Losses {'ner': 48.25221036223087}\n",
      "iter: 10\n",
      "Losses {'ner': 41.69436203638071}\n",
      "iter: 11\n",
      "Losses {'ner': 39.936312764276536}\n",
      "iter: 12\n",
      "Losses {'ner': 35.64109284182328}\n",
      "iter: 13\n",
      "Losses {'ner': 29.215231745360235}\n",
      "iter: 14\n",
      "Losses {'ner': 32.40128022715007}\n",
      "iter: 15\n",
      "Losses {'ner': 26.470775085939362}\n",
      "iter: 16\n",
      "Losses {'ner': 26.064767471744158}\n",
      "iter: 17\n",
      "Losses {'ner': 23.669235532078805}\n",
      "iter: 18\n",
      "Losses {'ner': 22.569212980668482}\n",
      "iter: 19\n",
      "Losses {'ner': 19.753191048010656}\n",
      "training time:  459.58295369148254\n",
      "\n",
      "Model evaluation results:\n",
      "{'uas': 0.0, 'las': 0.0, 'ents_p': 100.0, 'ents_r': 60.0, 'ents_f': 74.99999999999999, 'tags_acc': 0.0, 'token_acc': 100.0}\n",
      "The model was saved to the directory:  Models\n"
     ]
    }
   ],
   "source": [
    "main(model=None, new_model_name='DCC_ent', input_dir=input_dir, saved_model_dir=model_dir, output_dir=output_dir, test_dir=test_dir, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
