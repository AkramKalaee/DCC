In experiments, we find that there is a trade off between thecontextual capacity of the decoder and the amount of encoding information used.We show that with the right decoder, VAE can outperform LSTM language models.We demonstrate perplexity gains on two datasets, representing the firstpositive experimental result on the use VAE for generative modeling of text.Further, we conduct an in-depth investigation of the use of VAE (with our newdecoding architecture) for semi-supervised and unsupervised labeling tasks,demonstrating gains over several strong baselines.