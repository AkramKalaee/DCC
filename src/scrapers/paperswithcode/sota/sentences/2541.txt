Experiment with different activation functions (maxout, ReLU-family, tanh)
show that the proposed initialization leads to learning of very deep nets that
(i) produces networks with test accuracy better or equal to standard methods
and (ii) is at least as fast as the complex schemes proposed specifically for
very deep nets such as FitNets (Romero et al.