Wepropose the Universal Transformer (UT), a parallel-in-time self-attentiverecurrent sequence model which can be cast as a generalization of theTransformer model and which addresses these issues.