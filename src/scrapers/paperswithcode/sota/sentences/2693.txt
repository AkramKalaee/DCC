We alsoapply a purely byte-level mLSTM on the WikiText-2 dataset to achieve acharacter level entropy of 1.26 bits/char, corresponding to a word levelperplexity of 88.8, which is comparable to word level LSTMs regularised insimilar ways on the same task.