We introduce a Sparsely-GatedMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forwardsub-networks.