Recently, neural models pretrained on a language modeling task, such as ELMo(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin etal., 2018), have achieved impressive results on various natural languageprocessing tasks such as question-answering and natural language inference.