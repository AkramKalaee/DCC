Specifically, we append two types of attention modules ontop of traditional dilated FCN, which model the semantic interdependencies inspatial and channel dimensions respectively.