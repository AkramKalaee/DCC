We train our networks using tens of millions of traininginstances and evaluate it on two standard collections: a homogeneous newscollection(Robust) and a heterogeneous large-scale web collection (ClueWeb).Our experiments indicate that employing proper objective functions and lettingthe networks to learn the input representation based on weakly supervised dataleads to impressive performance, with over 13% and 35% MAP improvements overthe BM25 model on the Robust and the ClueWeb collections.