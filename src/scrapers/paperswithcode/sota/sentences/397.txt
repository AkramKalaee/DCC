We propose Weighted Transformer, a Transformer with
modified attention layers, that not only outperforms the baseline network in
BLEU score but also converges 15-40% faster.