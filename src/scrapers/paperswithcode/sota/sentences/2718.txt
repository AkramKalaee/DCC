Attention models have been intensively studied to improve NLP tasks such asmachine comprehension via both question-aware passage attention model andself-matching attention model.