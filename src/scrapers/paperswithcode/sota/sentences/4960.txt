Transformer networks have a potential of learning longer-term dependency, butare limited by a fixed-length context in the setting of language modeling.