Our proposed method performs better than standardWGAN and enables stable training of a wide variety of GAN architectures withalmost no hyperparameter tuning, including 101-layer ResNets and languagemodels over discrete data.