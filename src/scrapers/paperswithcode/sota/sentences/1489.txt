We also show improved performance for word-based languagemodels on news reports (GigaWord), books (Project Gutenberg) and Wikipediaarticles (WikiText-103) --- the latter achieving a state-of-the-art perplexityof 29.2.