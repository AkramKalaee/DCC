In this paper, we show how universal sentence representations trainedusing the supervised data of the Stanford Natural Language Inference datasetscan consistently outperform unsupervised methods like SkipThought vectors on awide range of transfer tasks.