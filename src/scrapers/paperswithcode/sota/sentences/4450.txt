Weperform an exhaustive study on techniques such as character ConvolutionalNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.Our best single model significantly improves state-of-the-art perplexity from51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),while an ensemble of models sets a new record by improving perplexity from 41.0down to 23.7.