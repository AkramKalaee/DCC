We introduce a novel theoretical framework that facilitatesbetter learning in language modeling, and show that our framework leads totying together the input embedding and the output projection matrices, greatlyreducing the number of trainable variables.