Deploying neural NLP models to mobile devices requires compressing
the word embeddings without any significant sacrifices in performance.