However, inherent structure in our world and bias inour language tend to be a simpler signal for learning than visual modalities,resulting in models that ignore visual information, leading to an inflatedsense of their capability.We propose to counter these language priors for the task of Visual QuestionAnswering (VQA) and make vision (the V in VQA) matter!