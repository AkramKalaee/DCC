Applying weightpruning on top of knowledge distillation results in a student model that has 13times fewer parameters than the original teacher model, with a decrease of 0.4BLEU.