We introduce a novel theoretical framework that facilitates
better learning in language modeling, and show that our framework leads to
tying together the input embedding and the output projection matrices, greatly
reducing the number of trainable variables.