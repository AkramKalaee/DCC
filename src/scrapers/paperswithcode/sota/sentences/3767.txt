We train an array of large RNN language models that operate at word or
character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a
customized corpus for this task and show that diversity of training data plays
an important role in test performance.