In this paper we investigate the performance of different types of rectifiedactivation functions in convolutional neural network: standard rectified linearunit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectifiedlinear unit (PReLU) and a new randomized leaky rectified linear units (RReLU).We evaluate these activation function on standard image classification task.Our experiments suggest that incorporating a non-zero slope for negative partin rectified activation units could consistently improve the results.