Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process.