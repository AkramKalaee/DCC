Moreover, stackinglayers with nonlinear activations is hard to approximate the intrinsic lineartransformations between feature representations.In this paper, we investigate the effect of erasing ReLUs of certain layersand apply it to various representative architectures following deterministicrules.