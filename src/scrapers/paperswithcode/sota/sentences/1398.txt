To improve parallelism andtherefore decrease training time, our attention mechanism connects the bottomlayer of the decoder to the top layer of the encoder.