We propose vector-based multi-head attention that includes the widely used max pooling, mean pooling, and scalar self-attention as special cases.