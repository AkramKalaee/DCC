The focus of this work is to make hypernetworks usefulfor deep convolutional networks and long recurrent networks, wherehypernetworks can be viewed as relaxed form of weight-sharing across layers.Our main result is that hypernetworks can generate non-shared weights for LSTMand achieve near state-of-the-art results on a variety of sequence modellingtasks including character-level language modelling, handwriting generation andneural machine translation, challenging the weight-sharing paradigm forrecurrent networks.