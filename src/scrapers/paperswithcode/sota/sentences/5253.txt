Attention mechanism has been used as an ancillary means to help RNN or CNN.However, the Transformer (Vaswani et al., 2017) recently recorded thestate-of-the-art performance in machine translation with a dramatic reductionin training time by solely using attention.