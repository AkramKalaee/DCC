Bidirectional long short-term memory (bi-LSTM) networks have recently provensuccessful for various NLP sequence modeling tasks, but little is known abouttheir reliance to input representations, target languages, data set size, andlabel noise.