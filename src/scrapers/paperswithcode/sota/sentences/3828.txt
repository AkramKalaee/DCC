We present a new approach for pretraining a bi-directional transformer modelthat provides significant performance gains across a variety of languageunderstanding problems.