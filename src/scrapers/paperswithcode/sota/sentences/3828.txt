We incorporate several techniques that were mainly developed in natural language generation tasks, e.g., machine translation and summarization, and demonstrate that the sequence-to-sequence model achieves the current top-notch parsersâ€™ performance (almost) without requiring any explicit task-specific knowledge or architecture of constituent parsing.