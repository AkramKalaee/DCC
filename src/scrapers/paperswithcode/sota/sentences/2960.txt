We demonstrate the ability of our methodto improve language modeling performance by up to 7.91 perplexity and reducetraining iterations by up to $61\%$, in addition to its flexibility in enablingsnapshot ensembling and use with adversarial training.