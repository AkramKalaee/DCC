Specifically, we append two types of attention modules on
top of traditional dilated FCN, which model the semantic interdependencies in
spatial and channel dimensions respectively.