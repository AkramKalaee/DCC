On both Pronoun Disambiguation and Winograd Schema challenges,our models outperform previous state-of-the-art methods by a large margin,without using expensive annotated knowledge bases or hand-engineered features.We train an array of large RNN language models that operate at word orcharacter level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and acustomized corpus for this task and show that diversity of training data playsan important role in test performance.