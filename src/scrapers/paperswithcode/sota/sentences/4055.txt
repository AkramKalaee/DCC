We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems.