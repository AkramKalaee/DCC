Our use of gated
linear units eases gradient propagation and we equip each decoder layer with a
separate attention module.