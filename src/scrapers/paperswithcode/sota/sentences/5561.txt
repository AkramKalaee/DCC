Recent work has shown that self-attention is an effective way of modeling textual sequences.