First, a reattention mechanism is proposed to refine currentattentions by directly accessing to past attentions that are temporallymemorized in a multi-round alignment architecture, so as to avoid the problemsof attention redundancy and attention deficiency.