Unlike recentlanguage representation models, BERT is designed to pre-train deepbidirectional representations by jointly conditioning on both left and rightcontext in all layers.