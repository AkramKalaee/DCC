Batch-splitting (data-parallelism) is the dominant distributed Deep Neural
Network (DNN) training strategy, due to its universal applicability and its
amenability to Single-Program-Multiple-Data (SPMD) programming.