Motivated by the Transformer,Directional Self Attention Network (Shen et al., 2017), a fully attention-basedsentence encoder, was proposed.