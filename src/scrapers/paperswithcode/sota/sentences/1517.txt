The number of operations required by thisapproach scales linearly in the input length, whereas self-attention isquadratic.