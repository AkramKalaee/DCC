Specifically, we propose fixed-updateinitialization (Fixup), an initialization motivated by solving the explodingand vanishing gradient problem at the beginning of training via properlyrescaling a standard initialization.