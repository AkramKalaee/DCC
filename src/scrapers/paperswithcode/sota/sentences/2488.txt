In this work, we extend this approachto multiple languages and show the effectiveness of cross-lingual pretraining.We propose two methods to learn cross-lingual language models (XLMs): oneunsupervised that only relies on monolingual data, and one supervised thatleverages parallel data with a new cross-lingual language model objective.