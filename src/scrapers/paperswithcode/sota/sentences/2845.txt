We also
apply a purely byte-level mLSTM on the WikiText-2 dataset to achieve a
character level entropy of 1.26 bits/char, corresponding to a word level
perplexity of 88.8, which is comparable to word level LSTMs regularised in
similar ways on the same task.