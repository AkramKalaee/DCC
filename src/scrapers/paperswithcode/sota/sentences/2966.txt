Therefore, we propose theintroduction of batch normalisation units into deep feedforward neural networkswith piecewise linear activations, which drives a more balanced use of theseactivation units, where each region of the activation function is trained witha relatively large proportion of training samples.