Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences.