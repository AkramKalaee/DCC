As asolution, we propose a novel neural architecture, Transformer-XL, that enablesTransformer to learn dependency beyond a fixed length without disruptingtemporal coherence.