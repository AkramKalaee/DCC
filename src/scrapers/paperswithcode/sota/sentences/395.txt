Instead, it uses only self-attention and feed-forward layers.