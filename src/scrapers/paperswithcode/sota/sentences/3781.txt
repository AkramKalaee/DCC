We show that dropout training is best understood as performing MAP estimationconcurrently for a family of conditional models whose objectives are themselveslower bounded by the original dropout objective.