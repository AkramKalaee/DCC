In this paper, we integrate both soft and hard attention into one
context fusion model, "reinforced self-attention (ReSA)", for the mutual
benefit of each other.