We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.