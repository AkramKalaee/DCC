Toanswer this question, we evaluate an attention-based encoder-decoder with asubword-level encoder and a character-level decoder on four languagepairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.Our experiments show that the models with a character-level decoder outperformthe ones with a subword-level decoder on all of the four language pairs.Furthermore, the ensembles of neural models with a character-level decoderoutperform the state-of-the-art non-neural machine translation systems onEn-Cs, En-De and En-Fi and perform comparably on En-Ru.