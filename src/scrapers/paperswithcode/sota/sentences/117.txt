Our two-way attention mechanism is a general
framework independent of the underlying representation learning, and it has
been applied to both convolutional neural networks (CNNs) and recurrent neural
networks (RNNs) in our studies.