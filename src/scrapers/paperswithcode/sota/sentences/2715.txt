Withnegligible overhead in the number of parameters and training time, our PastDecode Regularization (PDR) method achieves a word level perplexity of 55.6 onthe Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax.We also show gains by using PDR in combination with a mixture-of-softmaxes,achieving a word level perplexity of 53.8 and 60.5 on these datasets.