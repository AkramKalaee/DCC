We propose Weighted Transformer, a Transformer withmodified attention layers, that not only outperforms the baseline network inBLEU score but also converges 15-40% faster.