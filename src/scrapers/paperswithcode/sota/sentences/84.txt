Batch-splitting (data-parallelism) is the dominant distributed Deep NeuralNetwork (DNN) training strategy, due to its universal applicability and itsamenability to Single-Program-Multiple-Data (SPMD) programming.