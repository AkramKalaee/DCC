First, a reattention mechanism is proposed to refine current
attentions by directly accessing to past attentions that are temporally
memorized in a multi-round alignment architecture, so as to avoid the problems
of attention redundancy and attention deficiency.