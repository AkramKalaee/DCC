Although being able to model non-local semantic information, asequence LSTM can lose information from the AMR graph structure, and thus faceschallenges with large graphs, which result in long sequences.