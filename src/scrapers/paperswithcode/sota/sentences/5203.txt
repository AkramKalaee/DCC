We evaluate baselines based on currentmethods for multi-task and transfer learning and find that they do notimmediately give substantial improvements over the aggregate performance oftraining a separate model per task, indicating room for improvement indeveloping general and robust NLU systems.