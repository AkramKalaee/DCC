The new loss
function is provably continuous, and experiments show that it stabilizes and
accelerates training, giving image generation models that outperform
state-of-the art methods on $160 \times 160$ CelebA and $64 \times 64$
unconditional ImageNet.