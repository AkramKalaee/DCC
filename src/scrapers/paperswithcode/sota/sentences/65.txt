Bidirectional long short-term memory (bi-LSTM) networks have recently proven
successful for various NLP sequence modeling tasks, but little is known about
their reliance to input representations, target languages, data set size, and
label noise.