We introduce a model that avoids thisautoregressive property and produces its outputs in parallel, allowing an orderof magnitude lower latency during inference.