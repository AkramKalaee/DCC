(2015) by incorporating a pre-trained
bidirectional transformer language model, known as BERT (Devlin et al., 2018).