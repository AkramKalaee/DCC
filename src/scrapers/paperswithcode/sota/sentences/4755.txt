Second, we propose to replace hyperbolic tangent with ReLU activations.This variation couples well with batch normalization and could help the modellearn long-term dependencies without numerical issues.Results show that the proposed architecture, called Light GRU (Li-GRU), notonly reduces the per-epoch training time by more than 30% over a standard GRU,but also consistently improves the recognition accuracy across different tasks,input features, noisy conditions, as well as across different ASR paradigms,ranging from standard DNN-HMM speech recognizers to end-to-end CTC models.