The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic.