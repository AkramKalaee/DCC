Recent work has shown that self-attentionis an effective way of modeling textual sequences.