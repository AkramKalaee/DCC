Word embeddings are effective intermediate representations for capturingsemantic regularities between words, when learning the representations of textsequences.