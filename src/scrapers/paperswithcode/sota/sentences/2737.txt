We present two simple ways of reducing the number of parameters andaccelerating the training of large Long Short-Term Memory (LSTM) networks: thefirst one is "matrix factorization by design" of LSTM matrix into the productof two smaller matrices, and the second one is partitioning of LSTM matrix, itsinputs and states into the independent groups.