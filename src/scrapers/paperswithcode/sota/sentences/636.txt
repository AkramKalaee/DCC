As a result, the pre-trained BERT representations can befine-tuned with just one additional output layer to create state-of-the-artmodels for a wide range of tasks, such as question answering and languageinference, without substantial task-specific architecture modifications.BERT is conceptually simple and empirically powerful.