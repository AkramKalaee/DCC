Second, proceed from the firstto the final layer, normalizing the variance of the output of each layer to beequal to one.Experiment with different activation functions (maxout, ReLU-family, tanh)show that the proposed initialization leads to learning of very deep nets that(i) produces networks with test accuracy better or equal to standard methodsand (ii) is at least as fast as the complex schemes proposed specifically forvery deep nets such as FitNets (Romero et al.