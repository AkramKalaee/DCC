We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.