We propose a new simple network architecture, the Transformer, basedsolely on attention mechanisms, dispensing with recurrence and convolutionsentirely.