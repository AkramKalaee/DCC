We take a standard neural architecture for this task, and showthat by providing rich contextualized word representations from a largepre-trained language model as well as allowing the model to choose betweencontext-dependent and context-independent word representations, we can obtaindramatic improvements and reach performance comparable to state-of-the-art onthe competitive SQuAD dataset.