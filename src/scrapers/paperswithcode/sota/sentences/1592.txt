Self-attention is a useful mechanism to build generative models for language
and images.