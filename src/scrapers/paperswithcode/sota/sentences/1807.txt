In contrast to ReLUs, ELUs havenegative values which allows them to push mean unit activations closer to zerolike batch normalization but with lower computational complexity.