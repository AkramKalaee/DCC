Using TPU
meshes of up to 512 cores, we train Transformer models with up to 5 billion
parameters, surpassing state of the art results on WMT'14 English-to-French
translation task and the one-billion-word language modeling benchmark.