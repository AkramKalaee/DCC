We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) --the latter achieving a state-of-the-art perplexity of 29.2.