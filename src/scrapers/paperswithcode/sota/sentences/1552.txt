By shallow fusion, we report up to 27% relative improvements in WER over the attention baseline without a language model.