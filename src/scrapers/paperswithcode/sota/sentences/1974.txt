We apply the MoE to the tasks oflanguage modeling and machine translation, where model capacity is critical forabsorbing the vast quantities of knowledge available in the training corpora.We present model architectures in which a MoE with up to 137 billion parametersis applied convolutionally between stacked LSTM layers.