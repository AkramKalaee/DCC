The baseline unpruned Kneser-Ney 5-gram model achievesperplexity 67.6; a combination of techniques leads to 35% reduction inperplexity, or 10% reduction in cross-entropy (bits), over that baseline.The benchmark is available as a code.google.com project; besides the scriptsneeded to rebuild the training/held-out data, it also makes availablelog-probability values for each word in each of ten held-out data sets, foreach of the baseline n-gram models.