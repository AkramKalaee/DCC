In contrast to previous work, which combines NMTmodels with separately trained language models, we note that encoder-decoderNMT architectures already have the capacity to learn the same information as alanguage model, and we explore strategies to train with monolingual datawithout changing the neural network architecture.