Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks.