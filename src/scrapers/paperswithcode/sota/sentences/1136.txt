In thiswork, we present linguistically-informed self-attention (LISA): a neuralnetwork model that combines multi-head self-attention with multi-task learningacross dependency parsing, part-of-speech tagging, predicate detection and SRL.Unlike previous models which require significant pre-processing to preparelinguistic features, LISA can incorporate syntax using merely raw tokens asinput, encoding the sequence only once to simultaneously perform parsing,predicate detection and role labeling for all predicates.