We show that dropout training is best understood as performing MAP estimation
concurrently for a family of conditional models whose objectives are themselves
lower bounded by the original dropout objective.