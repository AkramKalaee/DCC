(2015) by incorporating a pre-trainedbidirectional transformer language model, known as BERT (Devlin et al., 2018).MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI,SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.2%(1.8% absolute improvement).