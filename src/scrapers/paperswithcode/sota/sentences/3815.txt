Continuous word representation (aka word embedding) is a basic building blockin many neural network-based models used in natural language processing tasks.Although it is widely accepted that words with similar semantics should beclose to each other in the embedding space, we find that word embeddingslearned in several tasks are biased towards word frequency: the embeddings ofhigh-frequency and low-frequency words lie in different subregions of theembedding space, and the embedding of a rare word and a popular word can be farfrom each other even if they are semantically similar.