Our solutionsubstitutes the conventionally used succession of BatchNorm + Activation layerswith a single plugin layer, hence avoiding invasive framework surgery whileproviding straightforward applicability for existing deep learning frameworks.We obtain memory savings of up to 50% by dropping intermediate results and byrecovering required information during the backward pass through the inversionof stored forward results, with only minor increase (0.8-2%) in computationtime.