Through knowledge distillation,
the use of input token fertilities as a latent variable, and policy gradient
fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative
to the autoregressive Transformer network used as a teacher.