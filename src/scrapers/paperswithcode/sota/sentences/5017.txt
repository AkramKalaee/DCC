Specifically, wepropose to train two identical copies of an RNN (that share parameters) withdifferent dropout masks while minimizing the difference between their(pre-softmax) predictions.