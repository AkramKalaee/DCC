In this paper, we showthat a deep (64-layer) transformer model with fixed context outperforms RNNvariants by a large margin, achieving state of the art on two popularbenchmarks: 1.13 bits per character on text8 and 1.06 on enwik8.