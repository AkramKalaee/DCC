Reducing the number ofparameters while preserving essentially the same predictive performance iscritically important for operating deep neural networks in memory constrainedenvironments such as GPUs or embedded devices.In this paper we show how kernel methods, in particular a single Fastfoodlayer, can be used to replace all fully connected layers in a deepconvolutional neural network.