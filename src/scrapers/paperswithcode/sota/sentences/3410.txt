We demonstrate thatstandard knowledge distillation applied to word-level prediction can beeffective for NMT, and also introduce two novel sequence-level versions ofknowledge distillation that further improve performance, and somewhatsurprisingly, seem to eliminate the need for beam search (even when applied onthe original teacher model).