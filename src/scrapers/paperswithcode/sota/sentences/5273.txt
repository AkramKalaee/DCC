In contrast to the conventional stacked LSTMs whereonly hidden states are fed as input to the next layer, our architecture acceptsboth hidden and memory cell states of the preceding layer and fuses informationfrom the left and the lower context using the soft gating mechanism of LSTMs.Thus the proposed stacked LSTM architecture modulates the amount of informationto be delivered not only in horizontal recurrence but also in verticalconnections, from which useful features extracted from lower layers areeffectively conveyed to upper layers.