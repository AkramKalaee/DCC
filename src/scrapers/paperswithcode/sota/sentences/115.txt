Our two-way attention mechanism is a generalframework independent of the underlying representation learning, and it hasbeen applied to both convolutional neural networks (CNNs) and recurrent neuralnetworks (RNNs) in our studies.