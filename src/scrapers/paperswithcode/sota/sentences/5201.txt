In pursuit of this objective, we introducethe General Language Understanding Evaluation benchmark (GLUE), a tool forevaluating and analyzing the performance of models across a diverse range ofexisting NLU tasks.