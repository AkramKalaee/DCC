In addition, the proposed
attention-based architecture delivered the best results even with a smaller
number of trainable parameters.