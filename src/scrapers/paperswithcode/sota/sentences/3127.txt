We demonstrate the ability of our method
to improve language modeling performance by up to 7.91 perplexity and reduce
training iterations by up to $61\%$, in addition to its flexibility in enabling
snapshot ensembling and use with adversarial training.