In this paper, we show how universal sentence representations trained
using the supervised data of the Stanford Natural Language Inference datasets
can consistently outperform unsupervised methods like SkipThought vectors on a
wide range of transfer tasks.