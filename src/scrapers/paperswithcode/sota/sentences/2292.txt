Instead of MLP, we employ maxout MLPto learn a variety of piecewise linear activation functions and to mediate theproblem of vanishing gradients that can occur when using rectifier units.Moreover, batch normalization is applied to reduce the saturation of maxoutunits by pre-conditioning the model and dropout is applied to preventoverfitting.