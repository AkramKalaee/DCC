Soft attention mechanisms show promisingperformance in modeling local/global dependencies by soft probabilities betweenevery two tokens, but they are not effective and efficient when applied to longsentences.