We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input.