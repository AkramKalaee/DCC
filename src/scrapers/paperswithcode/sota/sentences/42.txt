Experiments show that for machinetranslation and visual question answering, inefficient exact latent variablemodels outperform standard neural attention, but these gains go away when usinghard attention based training.