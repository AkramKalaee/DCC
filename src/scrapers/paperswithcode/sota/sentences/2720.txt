Soft attention mechanisms show promising
performance in modeling local/global dependencies by soft probabilities between
every two tokens, but they are not effective and efficient when applied to long
sentences.