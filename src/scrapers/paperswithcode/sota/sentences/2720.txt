First, PhaseCond, anarchitecture of multi-layered attention models, consists of multiple phaseseach implementing a stack of attention layers producing passage representationsand a stack of inner or outer fusion layers regulating the information flow.Second, we extend and improve the dot-product attention function for PhaseCondby simultaneously encoding multiple question and passage embedding layers fromdifferent perspectives.