In this work, we compare three datasets which build on the paradigm defined
in SQuAD for question answering: SQuAD 2.0, QuAC, and CoQA. We compare these
three datasets along several of their new features: (1) unanswerable questions,
(2) multi-turn interactions, and (3) abstractive answers. We show that the
datasets provide complementary coverage of the first two aspects, but weak
coverage of the third. Because of the datasets' structural similarity, a single
extractive model can be easily adapted to any of the datasets. We show that
this model can improve baseline results on both SQuAD 2.0 and CoQA. Despite the
core similarity between the datasets, models trained on one dataset are
ineffective on another dataset, but we do find moderate performance improvement
through pretraining. To encourage evaluation of methods on all of these
datasets, we release code for conversion between them.