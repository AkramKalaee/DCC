Transformer networks have a potential of learning longer-term dependency, but
are limited by a fixed-length context in the setting of language modeling. As a
solution, we propose a novel neural architecture, Transformer-XL, that enables
Transformer to learn dependency beyond a fixed length without disrupting
temporal coherence. Concretely, it consists of a segment-level recurrence
mechanism and a novel positional encoding scheme. Our method not only enables
capturing longer-term dependency, but also resolves the problem of context
fragmentation. As a result, Transformer-XL learns dependency that is about 80%
longer than RNNs and 450% longer than vanilla Transformers, achieves better
performance on both short and long sequences, and is up to 1,800+ times faster
than vanilla Transformer during evaluation. Additionally, we improve the
state-of-the-art (SoTA) results of bpc/perplexity from 1.06 to 0.99 on enwiki8,
from 1.13 to 1.08 on text8, from 20.5 to 18.3 on WikiText-103, from 23.7 to
21.8 on One Billion Word, and from 55.3 to 54.5 on Penn Treebank (without
finetuning). Our code, pretrained models, and hyperparameters are available in
both Tensorflow and PyTorch.