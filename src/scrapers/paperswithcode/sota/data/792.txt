Machine translation is highly sensitive to the size and quality of the
training data, which has led to an increasing interest in collecting and
filtering large parallel corpora. In this paper, we propose a new method for
this task based on multilingual sentence embeddings. Our approach uses an
encoder-decoder trained over an initial parallel corpus to build multilingual
sentence representations, which are then incorporated into a new margin-based
method to score, mine and filter parallel sentences. In contrast to previous
approaches, which rely on nearest neighbor retrieval with a hard threshold over
cosine similarity, our proposed method accounts for the scale inconsistencies
of this measure, considering the margin between a given sentence pair and its
closest candidates instead. Our experiments show large improvements over
existing methods. We outperform the best published results on the BUCC shared
task on parallel corpus mining by more than 10 F1 points. We also improve the
precision from 48.9 to 83.3 on the reconstruction of 11.3M English-French
sentence pairs of the UN corpus. Finally, filtering the English-German
ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014,
an improvement of more than one point over the best official filtered version.