Knowledge graphs are structured representations of real world facts. However,
they typically contain only a small subset of all possible facts. Link
prediction is a task of inferring missing facts based on existing ones. We
propose TuckER, a relatively simple but powerful linear model based on Tucker
decomposition of the binary tensor representation of knowledge graph triples.
TuckER outperforms all previous state-of-the-art models across standard link
prediction datasets. We prove that TuckER is a fully expressive model, deriving
the bound on its entity and relation embedding dimensionality for full
expressiveness which is several orders of magnitude smaller than the bound of
previous state-of-the-art models ComplEx and SimplE. We further show that
several previously introduced linear models can be viewed as special cases of
TuckER.