main	call	Trainer.__init__
reduce_max	has_arg0	1
gumbel_softmax	call	cast
log	followed_by	squeeze
_sum_log_exp	call	log
Model.__init__	call	scalar
sample_step	followed_by	comm_func_eval
sample	call	reshape
sample	call	size
random_normal	followed_by	sqrt
get_variable	has_arg0	weights
reduce_sum	followed_by	log
exponential_decay	has_name	decaying_learning_rate
model_mean_and_variance	call	reduce_sum
cast	followed_by	stop_gradient
reshape	call	range
List	followed_by	stop_gradient
GaussianMixture.__init__	call	reduce_sum
Trainer.__init__	call	Model.__init__
expand_dims	has_arg0	1
log_prob	call	squeeze
GaussianMixture.__init__	call	Categorical
_sum_log_exp	followed_by	exp
Variable	followed_by	exponential_decay
Saver	followed_by	FileWriter
model_mean_and_variance	call	range
ones	followed_by	reduce_sum
cast	call	greater
get_f_div_loss	call	stop_gradient
Trainer.__init__	call	optimize_adagrad
sample	call	dynamic_stitch
Trainer.__init__	call	Variable
Session	followed_by	device
_simulate_mixture_target	call	GaussianMixture.__init__
GaussianMixture.__init__	call	ones
log	followed_by	reduce_sum
reduce_sum	has_axis	1
Trainer.__init__	followed_by	train
variable_scope	followed_by	get_variable
get_variable	has_arg0	mu
GaussianMixture.__init__	call	exp
main	call	ConfigProto
phi	call	sign
zeros	has_arg0	(n_components, dim)
Model.__init__	call	get_variable
comm_func_eval	call	exsqr
train	call	sample_step
main	call	set_random_seed
gumbel_softmax	call	stop_gradient
main	call	device
stop_gradient	followed_by	_sum_log_exp
greater	has_arg0	0.5
get_variable	followed_by	zeros
FileWriter	followed_by	optimize_adagrad
exp	followed_by	reduce_sum
run_single_step	followed_by	evaluate_step
equal	call	reduce_max
Variable	has_name	global_step
evaluate_step	call	sample_step
GaussianMixture.__init__	call	log
expand_dims	has_arg0	2
softmax	has_arg0	1
gumbel_softmax_sample	followed_by	shape
log_prob	call	_sum_log_exp
main	call	train
_simulate_mixture_target	call	variable_scope
Trainer.__init__	call	FileWriter
_simulate_mixture_target	followed_by	set_random_seed
eval_run	followed_by	eval_run
softmax	followed_by	log
evaluate_step	call	comm_func_eval
_sum_log_exp	call	reduce_sum
exponential_decay	followed_by	merge_all
range	followed_by	size
sqrt	followed_by	dynamic_stitch
Trainer.__init__	call	Saver
reduce_sum	followed_by	range
Saver	has_max_to_keep	1
device	has_arg0	/cpu:0
shape	followed_by	cast
range	has_arg0	0
cast	call	equal
random_uniform	followed_by	log
reduce_sum	followed_by	reduce_sum
sample	followed_by	reshape
phi	call	reduce_sum
zeros	followed_by	ones
reduce_sum	has_axis	2
log_prob	call	exp
reduce_max	followed_by	exp
expand_dims	call	log
gumbel_softmax	followed_by	ones
sample_step	call	sample
gumbel_softmax_sample	call	sample_gumbel
gumbel_softmax	call	gumbel_softmax_sample
scalar	has_arg0	loss
Model.__init__	followed_by	Variable
eval_run	call	mean
merge_all	followed_by	Saver
get_variable	followed_by	GaussianMixture.__init__
ex	followed_by	exsqr
sample	call	dynamic_partition
gumbel_softmax_sample	call	softmax
Categorical	has_probs	self._weights
gumbel_softmax	call	shape
get_variable	call	ones
Model.__init__	call	variable_scope
Trainer.__init__	call	exponential_decay
exp	followed_by	softmax
RMSPropOptimizer	has_decay	0.9
GaussianMixture.__init__	call	softmax
log	call	log
reduce_sum	has_axis	0
phi	call	reduce_max
dynamic_partition	followed_by	range
range	followed_by	range
stop_gradient	followed_by	stop_gradient
exp	followed_by	sign
sqrt	followed_by	sample
sample	followed_by	sample
log_prob	call	reduce_sum
reduce_sum	followed_by	reduce_max
variable_scope	has_arg0	p_target
get_f_div_loss	call	log_prob
sample_step	followed_by	eval_run
phi	call	cast
Model.__init__	call	get_f_div_loss
expand_dims	followed_by	expand_dims
mean	followed_by	mean
sample_gumbel	call	random_uniform
sample_gumbel	followed_by	softmax
phi	followed_by	stop_gradient
model_mean_and_variance	followed_by	model_mean_and_variance
dynamic_partition	has_num_partitions	self.n_components
Trainer.__init__	call	latest_checkpoint
device	followed_by	_simulate_mixture_target
GaussianMixture.__init__	call	gumbel_softmax
sample_gumbel	call	log
size	followed_by	random_normal
comm_func_eval	call	ex
train	call	evaluate_step
cast	followed_by	reduce_sum
main	call	Mixture
Variable	has_arg0	0
expand_dims	has_arg0	0
expand_dims	followed_by	reduce_sum
get_variable	has_arg0	log_var
_simulate_mixture_target	call	get_variable
sample	followed_by	log_prob
train	call	range
reduce_sum	followed_by	random_normal
GaussianMixture.__init__	followed_by	get_f_div_loss
sample	call	random_normal
train	call	run_single_step
reduce_sum	followed_by	Categorical
log_prob	call	List
ConfigProto	call	GPUOptions
Mixture	followed_by	ConfigProto
set_random_seed	followed_by	Trainer.__init__
expand_dims	call	reduce_max
get_f_div_loss	followed_by	scalar
sign	followed_by	cast
ones	followed_by	GaussianMixture.__init__
expand_dims	call	exp
phi	call	exp
main	call	Session
optimize_adagrad	followed_by	latest_checkpoint
eval_run	call	model_mean_and_variance
ones	has_dtype	tf.float32
reshape	followed_by	dynamic_partition
range	has_arg0	1
reduce_max	has_arg0	0
log_prob	followed_by	log_prob
ex	call	mean
.trainer	call	main
ConfigProto	followed_by	Session
sample	call	range
get_f_div_loss	call	reduce_sum
range	followed_by	run_single_step
log_prob	call	stop_gradient
stop_gradient	followed_by	reduce_sum
get_variable	has_dtype	tf.float32
train	call	eval_run
Model.__init__	call	GaussianMixture.__init__
_simulate_mixture_target	call	ones
reduce_sum	call	expand_dims
optimize_adagrad	call	RMSPropOptimizer
reduce_sum	followed_by	expand_dims
sample	call	reduce_sum
exsqr	call	mean
random_uniform	has_maxval	1
sign	call	expand_dims
Trainer.__init__	call	merge_all
model_mean_and_variance	followed_by	mean
log_prob	call	log
sample	call	sqrt
sample	call	sample
get_f_div_loss	call	phi
_sum_log_exp	call	expand_dims
get_variable	followed_by	get_variable
log_prob	followed_by	phi
mean	has_axis	0
log	followed_by	gumbel_softmax
get_f_div_loss	call	sample
evaluate_step	followed_by	sample_step
_simulate_mixture_target	call	zeros
main	call	_simulate_mixture_target
reduce_sum	followed_by	logZ
logZ	call	log
logZ	call	stop_gradient
List	followed_by	reduce_sum
stop_gradient	followed_by	stop_gradient
models.MultiVariateGaussian.log_prob	call	reduce_sum
models.MultiVariateGaussian.log_prob	call	logZ
stop_gradient	followed_by	List
reduce_sum	has_axis	1
models.MultiVariateGaussian.log_prob	call	stop_gradient
models.MultiVariateGaussian.log_prob	call	List
stop_gradient	followed_by	log
get_klqp_loss	call	log_prob
train	call	min
Trainer.__init__	call	exponential_decay
mean	followed_by	mean
exponential_decay	followed_by	merge_all
.bnn_trainer	call	main
scalar	followed_by	scalar
ConfigProto	call	GPUOptions
get_klqp_loss	call	draw_samples
scalar	has_arg0	batch_ll
reshape	has_arg0	(k * nh, d)
load_uci_dataset	followed_by	Trainer.__init__
log_prob	call	log
stop_gradient	followed_by	reduce_sum
Variable	has_name	global_step
merge_all	followed_by	Saver
reduce_mean	call	reduce_logsumexp
min	followed_by	run_single_step
get_error_and_ll	call	predict
expand_dims	has_arg0	1
mean	has_arg0	0
Trainer.__init__	call	merge_all
random_normal	followed_by	sqrt
reduce_mean	followed_by	sqrt
get_variable	has_arg0	log_variance
phi	call	reduce_max
main	call	Session
log_prior	followed_by	log_prob
draw_samples	call	sqrt
predict	call	relu
Trainer.__init__	call	Saver
optimize_adam	call	AdamOptimizer
exp	followed_by	draw_samples
log_likelihood_factor	call	reduce_mean
Trainer.__init__	call	optimize_adam
Model.__init__	call	get_klqp_loss
exp	followed_by	reduce_sum
sqrt	call	reduce_mean
Model.__init__	call	placeholder
phi	call	cast
Trainer.__init__	call	latest_checkpoint
log_likelihood_factor	followed_by	log_prior
log_prior	call	reduce_sum
expand_dims	followed_by	expand_dims
reduce_sum	has_axis	1
ConfigProto	followed_by	Session
reduce_mean	has_axis	0
get_error_and_ll	followed_by	get_feed_dict
get_klqp_loss	call	phi
train	call	shuffle
placeholder	followed_by	placeholder
stop_gradient	followed_by	log
proposal_q.__init__	call	get_variable
stop_gradient	followed_by	stop_gradient
reduce_max	followed_by	exp
run_single_step	call	get_feed_dict
main	call	load_uci_dataset
expand_dims	followed_by	reduce_mean
scalar	followed_by	get_error_and_ll
reduce_logsumexp	has_axis	1
reduce_sum	followed_by	reduce_max
greater	has_arg0	0.5
Model.__init__	call	scalar
sign	call	expand_dims
run_single_step	followed_by	log_step_message
get_variable	followed_by	get_variable
get_variable	call	constant
Trainer.__init__	call	Variable
train	call	run_single_step
log_likelihood_factor	call	predict
main	call	Trainer.__init__
predict	call	reshape
reduce_mean	call	reduce_mean
get_error_and_ll	call	log
placeholder	followed_by	proposal_q.__init__
predict	call	matmul
Model.__init__	call	get_error_and_ll
log	followed_by	expand_dims
get_error_and_ll	call	exp
load_uci_dataset	call	mean
Saver	has_max_to_keep	1
reshape	has_arg0	(1, k * nh)
reshape	has_arg0	(1, -1)
device	followed_by	load_uci_dataset
cast	followed_by	reduce_sum
log_prob	call	stop_gradient
log	followed_by	reduce_sum
Variable	followed_by	exponential_decay
get_variable	has_arg0	mean
Model.__init__	call	proposal_q.__init__
get_variable	has_dtype	tf.float32
Variable	has_arg0	0
reduce_sum	call	reshape
log_prob	followed_by	phi
phi	call	exp
reshape	followed_by	matmul
reduce_logsumexp	call	log
get_klqp_loss	call	exp
get_error_and_ll	call	reduce_mean
log_likelihood_factor	call	log
Trainer.__init__	call	FileWriter
scalar	has_arg0	kl_loss
exp	followed_by	sign
draw_samples	followed_by	predict
train	call	log_step_message
main	call	ConfigProto
sign	followed_by	cast
reshape	followed_by	reshape
get_klqp_loss	call	stop_gradient
reshape	has_arg0	(-1, nh)
proposal_q.__init__	followed_by	get_klqp_loss
get_variable	followed_by	get_parameters_q
load_uci_dataset	call	squeeze
FileWriter	followed_by	optimize_adam
matmul	followed_by	relu
main	call	train
get_error_and_ll	followed_by	scalar
get_error_and_ll	call	sqrt
phi	call	sign
relu	followed_by	reshape
expand_dims	has_arg0	0
Trainer.__init__	followed_by	train
train	call	get_error_and_ll
variable_scope	followed_by	get_variable
proposal_q.__init__	call	get_parameters_q
log_step_message	followed_by	get_error_and_ll
Trainer.__init__	call	Model.__init__
get_klqp_loss	call	log_prior
phi	followed_by	stop_gradient
Session	followed_by	device
main	call	device
get_klqp_loss	call	log_likelihood_factor
log_likelihood_factor	call	expand_dims
exponential_decay	has_name	decaying_learning_rate
Saver	followed_by	FileWriter
proposal_q.__init__	call	variable_scope
get_parameters_q	call	exp
cast	call	greater
log_prior	call	log
get_klqp_loss	followed_by	scalar
Model.__init__	followed_by	Variable
log_prob	call	reduce_sum
draw_samples	followed_by	log_likelihood_factor
scalar	has_arg0	batch_rmse
reduce_mean	has_arg0	1
train	call	get_feed_dict
draw_samples	call	random_normal
get_variable	has_arg0	log_v_noise
phi	call	reduce_sum
get_error_and_ll	call	draw_samples
shuffle	followed_by	min
reshape	has_arg0	(-1, k)
predict	followed_by	log
squeeze	followed_by	squeeze
reshape	call	reduce_sum
get_error_and_ll	call	expand_dims
get_klqp_loss	call	reduce_sum
mean	followed_by	squeeze
optimize_adam	followed_by	latest_checkpoint
expand_dims	followed_by	transpose
models.GaussianMixture.log_gradient	call	List
reduce_sum	has_axis	2
expand_dims	has_arg0	0
posterior	call	exp
models.GaussianMixture.log_gradient	call	expand_dims
log	followed_by	reduce_sum
reduce_sum	followed_by	expand_dims
reduce_sum	followed_by	log
List	followed_by	expand_dims
posterior	call	reduce_sum
expand_dims	call	log
matmul	followed_by	squeeze
reduce_sum	has_axis	1
models.GaussianMixture.log_gradient	call	matmul
_sum_log_exp	call	log
expand_dims	has_arg0	1
models.GaussianMixture.log_gradient	call	posterior
matmul	call	transpose
expand_dims	call	exp
_sum_log_exp	followed_by	exp
squeeze	has_axis	1
matmul	call	expand_dims
posterior	followed_by	matmul
expand_dims	followed_by	posterior
models.GaussianMixture.log_gradient	call	squeeze
expand_dims	call	transpose
expand_dims	followed_by	expand_dims
_sum_log_exp	call	reduce_sum
reduce_max	has_arg0	0
exp	followed_by	reduce_sum
_sum_log_exp	call	expand_dims
expand_dims	followed_by	reduce_sum
expand_dims	call	reduce_max
posterior	call	_sum_log_exp
