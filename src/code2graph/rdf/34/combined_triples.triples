DEFINE_integer	followed_by	DEFINE_float
DEFINE_boolean	followed_by	DEFINE_float
DEFINE_boolean	call	DEFINE_boolean
DEFINE_integer	call	DEFINE_integer
.enas.ptb.main	call	run
DEFINE_float	call	DEFINE_float
.enas.ptb.main	call	DEFINE_string
DEFINE_integer	followed_by	DEFINE_integer
DEFINE_boolean	followed_by	DEFINE_integer
.enas.ptb.main	call	DEFINE_boolean
.enas.ptb.main	call	DEFINE_integer
DEFINE_string	call	DEFINE_string
DEFINE_integer	followed_by	DEFINE_boolean
DEFINE_boolean	followed_by	DEFINE_boolean
DEFINE_string	followed_by	DEFINE_boolean
.enas.ptb.main	call	DEFINE_float
DEFINE_float	followed_by	DEFINE_boolean
DEFINE_float	followed_by	DEFINE_integer
DEFINE_integer	followed_by	run
DEFINE_boolean	followed_by	DEFINE_string
DEFINE_integer	followed_by	DEFINE_string
DEFINE_string	followed_by	DEFINE_string
DEFINE_string	followed_by	DEFINE_integer
DEFINE_float	followed_by	DEFINE_float
Variable	has_arg0	0
shape	followed_by	shuffle_batch
__init__	call	shuffle_batch
__init__	call	transpose
__init__	call	device
device	followed_by	shape
shape	followed_by	batch
__init__	call	shape
device	has_arg0	/cpu:0
enas.cifar10.micro_child.MicroChild.__init__	call	Variable
shuffle_batch	followed_by	map_fn
Variable	has_name	global_step
__init__	followed_by	Variable
enas.cifar10.micro_child.MicroChild.__init__	call	__init__
__init__	call	batch
batch	followed_by	transpose
map_fn	followed_by	transpose
__init__	call	map_fn
transpose	followed_by	shape
exp	followed_by	reduce_sum
_build_params	call	Variable
connect_controller	followed_by	build_trainer
variable_scope	has_arg0	embedding
get_variable	has_arg0	w_2
to_int32	followed_by	reshape
_model	call	identity
embedding_lookup	call	concat
PTBEnasChild.__init__	call	reshape
exponential_decay	followed_by	maximum
train	call	Graph
cond	followed_by	MomentumOptimizer
variable_scope	has_arg0	starting_states
_build_train	call	_get_log_probs
to_float	call	range
variable_scope	has_arg0	lstm
to_float	followed_by	exp
ptb_input_producer	call	size
get_variable	followed_by	variable_scope
ptb_input_producer	call	identity
where	call	zeros_like
get_ops	call	PTBEnasController.__init__
gradients	followed_by	global_norm
stack	followed_by	reduce_sum
strided_slice	followed_by	strided_slice
_model	call	where
sigmoid	followed_by	tanh
range	has_arg0	1
convert_to_tensor	followed_by	size
stack_lstm	followed_by	matmul
transpose	followed_by	reshape
zeros	has_dtype	tf.float32
Saver	has_max_to_keep	10
get_train_ops	call	global_norm
AdamOptimizer	followed_by	SyncReplicasOptimizer
size	followed_by	convert_to_tensor
Variable	followed_by	assign_sub
_build_valid	call	reduce_sum
CheckpointSaverHook	followed_by	SingularMonitoredSession
assign_sub	followed_by	control_dependencies
identity	followed_by	Variable
GradientDescentOptimizer	followed_by	AdamOptimizer
_build_valid	call	_model
random_uniform	has_dtype	tf.float32
get_variable	followed_by	range
_build_sampler	call	concat
constant	has_dtype	tf.int32
_build_params	call	zeros
reshape	followed_by	multinomial
_build_valid_rl	call	_model
stack_lstm	call	lstm
stop_gradient	followed_by	exp
reshape	followed_by	assign
_build_train	followed_by	_build_valid
_build_train	call	trainable_variables
MomentumOptimizer	followed_by	GradientDescentOptimizer
PTBEnasController.__init__	followed_by	Adam
logical_and	call	equal
Variable	has_arg0	0.0
get_train_ops	call	cond
Variable	followed_by	trainable_variables
tanh	followed_by	multinomial
_build_sampler	call	stop_gradient
_model	call	logical_and
range	followed_by	variable_scope
stop_gradient	followed_by	_get_log_probs
_model	call	embedding_lookup
_get_log_probs	call	sparse_softmax_cross_entropy_with_logits
stop_gradient	followed_by	embedding_lookup
device	followed_by	identity
range	followed_by	_gen_mask
get_train_ops	call	clip_by_global_norm
get_train_ops	call	gradients
build_trainer	call	control_dependencies
lstm	call	split
PTBEnasChild.__init__	followed_by	SGD
clip_by_global_norm	followed_by	clip_by_norm
Variable	followed_by	Variable
constant	has_dtype	tf.bool
matmul	followed_by	sparse_softmax_cross_entropy_with_logits
split	followed_by	sigmoid
identity	has_name	epoch_size
train	call	get_ops
train	call	run
_build_test	call	stop_gradient
get_ops	call	build_trainer
enas.ptb.main.main	call	Logger.__init__
zeros	followed_by	stack_lstm
cond	call	greater_equal
to_float	followed_by	Variable
_build_sampler	call	tanh
size	followed_by	reshape
maximum	has_arg0	0
_gen_mask	call	random_uniform
matmul	followed_by	reshape
ptb_input_producer	call	device
reduce_any	followed_by	concat
_model	call	reshape
concat	followed_by	tanh
enas.ptb.main.main	call	print_user_flags
get_train_ops	call	add_n
variable_scope	followed_by	zeros
_model	call	control_dependencies
reduce_sum	followed_by	Variable
connect_controller	call	_build_params
variable_scope	has_arg0	attention
to_float	followed_by	to_float
_build_sampler	call	zeros
logical_and	followed_by	reduce_any
size	followed_by	size
global_norm	followed_by	sqrt
embedding_lookup	followed_by	concat
get_variable	has_arg0	w_prev
equal	followed_by	equal
reshape	followed_by	ptb_input_producer
multinomial	has_arg0	1
PTBEnasController.__init__	call	_create_params
reduce_sum	followed_by	to_float
get_variable	has_arg0	g_emb
tanh	followed_by	tanh
concat	followed_by	reduce_sum
sqrt	followed_by	sqrt
run	followed_by	run
ptb_input_producer	call	strided_slice
zeros_like	followed_by	ones_like
SGD	followed_by	PTBEnasController.__init__
_build_sampler	call	sparse_softmax_cross_entropy_with_logits
ptb_input_producer	call	reshape
device	has_arg0	/cpu:0
_build_sampler	call	stack_lstm
_model	call	while_loop
build_trainer	call	get_train_ops
get_train_ops	call	Variable
get_train_ops	call	SyncReplicasOptimizer
build_trainer	call	trainable_variables
_build_sampler	call	matmul
build_trainer	call	assign_sub
connect_controller	call	_build_train
constant	followed_by	while_loop
_build_test	call	_get_log_probs
get_ops	call	connect_controller
assign	call	zeros_like
get_ops	call	SGD
_build_train	call	Variable
split	has_axis	1
_gen_mask	followed_by	constant
_create_params	call	variable_scope
multinomial	followed_by	to_int32
matmul	followed_by	matmul
assign	followed_by	assign
tanh	followed_by	matmul
Graph	followed_by	get_ops
build_trainer	call	Variable
Variable	followed_by	cond
connect_controller	call	_build_valid
tanh	followed_by	to_float
_model	call	transpose
_create_params	call	get_variable
Variable	has_name	last_reset
range	followed_by	logical_and
random_uniform	has_dtype	tf.int32
concat	followed_by	exp
get_train_ops	call	sqrt
get_train_ops	call	GradientDescentOptimizer
train	call	size
_get_log_probs	followed_by	reduce_sum
_model	call	range
trainable_variables	followed_by	count_model_params
maximum	followed_by	cond
_model	call	reduce_sum
add_n	followed_by	gradients
ptb_input_producer	followed_by	reshape
exponential_decay	call	maximum
IndexedSlices	followed_by	clip_by_norm
get_train_ops	call	IndexedSlices
exp	followed_by	concat
_build_params	call	range
exp	followed_by	trainable_variables
exp	call	reduce_mean
variable_scope	has_arg0	softmax
connect_controller	call	_build_test
to_float	followed_by	stop_gradient
to_float	followed_by	stack_lstm
_build_valid_rl	call	reduce_mean
matmul	followed_by	tanh
zeros	followed_by	range
variable_scope	followed_by	get_variable
_build_sampler	call	reshape
_build_test	call	_model
get_ops	followed_by	Saver
lstm	call	matmul
ptb_input_producer	call	random_uniform
build_trainer	call	identity
variable_scope	followed_by	variable_scope
_model	call	reduce_any
clip_by_norm	followed_by	IndexedSlices
train	call	Saver
Saver	followed_by	CheckpointSaverHook
exp	followed_by	stop_gradient
get_variable	has_arg0	w_1
get_train_ops	call	clip_by_norm
PTBEnasController.__init__	call	_build_sampler
get_train_ops	call	maximum
MomentumOptimizer	has_arg0	0.9
Variable	has_name	train_step
build_trainer	followed_by	connect_controller
constant	has_arg0	0
_build_valid	followed_by	_build_valid_rl
Variable	has_name	T_i
_build_test	call	reduce_sum
_model	followed_by	stop_gradient
_build_sampler	call	embedding_lookup
cond	call	less
assign	followed_by	control_dependencies
reduce_sum	followed_by	add_n
get_train_ops	call	exponential_decay
SingularMonitoredSession	has_checkpoint_dir	FLAGS.output_dir
range	has_arg0	0
get_train_ops	call	MomentumOptimizer
concat	followed_by	concat
_build_sampler	call	to_float
zeros	has_dtype	np.float32
PTBEnasChild.__init__	call	ptb_input_producer
_build_sampler	call	multinomial
_get_log_probs	call	matmul
zeros	followed_by	zeros
Logger.__init__	followed_by	print_user_flags
get_train_ops	call	AdamOptimizer
random_uniform	followed_by	floor
get_ops	call	PTBEnasChild.__init__
_model	call	_gen_mask
_model	followed_by	_get_log_probs
matmul	followed_by	concat
_build_sampler	call	reduce_sum
_build_params	followed_by	_build_train
build_trainer	call	exp
_build_sampler	call	exp
matmul	followed_by	split
build_trainer	call	stop_gradient
Variable	followed_by	get_train_ops
equal	has_arg0	0
_build_valid	call	stop_gradient
_build_sampler	call	to_int32
where	followed_by	reshape
_model	call	ones_like
sqrt	call	reduce_sum
_build_valid_rl	call	_get_log_probs
_model	call	zeros_like
_model	call	TensorArray
random_uniform	followed_by	reduce_sum
Adam	followed_by	connect_controller
control_dependencies	followed_by	identity
concat	followed_by	where
reshape	followed_by	tanh
size	followed_by	Graph
SingularMonitoredSession	followed_by	run
range	followed_by	Variable
train	followed_by	train
embedding_lookup	followed_by	to_float
_model	call	constant
Variable	has_name	global_step
_build_valid_rl	call	stop_gradient
get_variable	followed_by	get_variable
_gen_mask	followed_by	_gen_mask
_build_train	call	count_model_params
embedding_lookup	followed_by	_gen_mask
_get_log_probs	followed_by	reduce_mean
sqrt	followed_by	clip_by_global_norm
split	has_arg0	4
train	call	SingularMonitoredSession
count_model_params	followed_by	reduce_sum
get_variable	has_arg0	v
_build_params	call	variable_scope
variable_scope	has_arg0	rnn
load	followed_by	size
_gen_mask	call	floor
_model	call	assign
while_loop	followed_by	stack
matmul	call	concat
reduce_mean	followed_by	stop_gradient
_build_params	call	get_variable
reduce_sum	followed_by	transpose
concat	has_axis	0
reshape	followed_by	range
_build_train	call	reduce_sum
concat	has_axis	1
_build_train	call	_model
print_user_flags	followed_by	train
_build_train	call	to_float
_build_valid	call	_get_log_probs
_gen_mask	followed_by	zeros_like
reduce_sum	followed_by	strided_slice
build_trainer	call	to_float
cond	followed_by	exponential_decay
sigmoid	followed_by	sigmoid
variable_scope	followed_by	range
sparse_softmax_cross_entropy_with_logits	followed_by	exp
convert_to_tensor	has_dtype	tf.int32
ptb_input_producer	call	reduce_sum
_build_train	call	get_train_ops
reshape	followed_by	device
build_trainer	call	reduce_sum
to_float	followed_by	reshape
identity	followed_by	random_uniform
_create_params	followed_by	_build_sampler
get_ops	call	Adam
reshape	followed_by	sparse_softmax_cross_entropy_with_logits
lstm	call	sigmoid
connect_controller	call	_build_valid_rl
ptb_input_producer	call	convert_to_tensor
constant	followed_by	range
clip_by_norm	followed_by	Variable
_model	call	stack
_build_valid_rl	followed_by	_build_test
lstm	call	tanh
train	call	load
trainable_variables	followed_by	get_train_ops
Variable	has_arg0	0
_model	call	concat
train	call	CheckpointSaverHook
get_variable	has_arg0	w
get_train_ops	call	reduce_sum
_build_train	call	exp
TensorArray	followed_by	embedding_lookup
assign	call	stop_gradient
enas.ptb.main.main	call	train
ones_like	followed_by	constant
DEFINE_string	followed_by	DEFINE_integer
DEFINE_boolean	followed_by	DEFINE_string
.enas.cifar10.main	call	DEFINE_boolean
DEFINE_integer	followed_by	DEFINE_boolean
DEFINE_float	followed_by	DEFINE_float
.enas.cifar10.main	call	DEFINE_string
.enas.cifar10.main	call	DEFINE_float
.enas.cifar10.main	call	run
DEFINE_float	followed_by	DEFINE_integer
DEFINE_boolean	call	DEFINE_boolean
DEFINE_string	followed_by	DEFINE_string
DEFINE_integer	followed_by	DEFINE_integer
DEFINE_boolean	followed_by	DEFINE_integer
DEFINE_boolean	followed_by	DEFINE_float
DEFINE_string	followed_by	DEFINE_boolean
DEFINE_float	call	DEFINE_float
DEFINE_integer	followed_by	DEFINE_float
.enas.cifar10.main	call	DEFINE_integer
DEFINE_string	call	DEFINE_string
DEFINE_integer	call	DEFINE_integer
DEFINE_boolean	followed_by	DEFINE_boolean
DEFINE_float	followed_by	DEFINE_string
DEFINE_integer	followed_by	run
variable_scope	has_arg0	softmax
sigmoid	followed_by	tanh
variable_scope	has_arg0	critic
_build_sampler	call	reshape
_create_params	followed_by	_build_sampler
concat	has_axis	0
variable_scope	has_arg0	lstm
tanh	followed_by	tanh
concat	followed_by	concat
_build_sampler	call	embedding_lookup
lstm	call	split
concat	followed_by	exp
matmul	followed_by	split
_build_sampler	call	exp
stack_lstm	call	lstm
get_variable	has_arg0	g_emb
_build_sampler	call	concat
reduce_sum	followed_by	to_float
_build_sampler	call	multinomial
_create_params	call	variable_scope
_build_sampler	call	to_int32
to_int32	followed_by	reshape
tanh	followed_by	multinomial
variable_scope	followed_by	variable_scope
zeros	followed_by	zeros
concat	has_axis	1
embedding_lookup	followed_by	concat
_create_params	call	get_variable
_build_sampler	call	matmul
_build_sampler	call	sparse_softmax_cross_entropy_with_logits
multinomial	followed_by	to_int32
variable_scope	has_arg0	embedding
variable_scope	followed_by	get_variable
lstm	call	matmul
lstm	call	tanh
multinomial	has_arg0	1
sparse_softmax_cross_entropy_with_logits	followed_by	embedding_lookup
_build_sampler	call	tanh
lstm	call	sigmoid
_build_sampler	call	stack_lstm
split	followed_by	sigmoid
exp	call	to_float
zeros	followed_by	stack_lstm
get_variable	has_arg0	w
enas.cifar10.controller.ConvController.__init__	call	_create_params
enas.cifar10.controller.ConvController.__init__	call	_build_sampler
zeros	has_dtype	tf.float32
split	has_arg0	4
split	has_axis	1
stack_lstm	followed_by	stop_gradient
matmul	followed_by	tanh
reshape	followed_by	sparse_softmax_cross_entropy_with_logits
sigmoid	followed_by	sigmoid
_build_sampler	call	stop_gradient
get_variable	followed_by	variable_scope
get_variable	followed_by	get_variable
exp	call	reduce_sum
matmul	call	concat
stop_gradient	followed_by	matmul
_build_sampler	call	zeros
get_train_ops	call	reduce_sum
MomentumOptimizer	followed_by	GradientDescentOptimizer
enas.cifar10.controller.ConvController.build_trainer	call	Adam
Variable	has_name	last_reset
concat	has_axis	0
exponential_decay	followed_by	maximum
reduce_sum	followed_by	Variable
MomentumOptimizer	has_arg0	0.9
get_train_ops	call	MomentumOptimizer
get_train_ops	call	clip_by_global_norm
control_dependencies	followed_by	identity
maximum	followed_by	cond
reduce_sum	followed_by	reduce_mean
enas.cifar10.controller.ConvController.build_trainer	call	reduce_sum
reduce_sum	followed_by	add_n
Variable	followed_by	Variable
Variable	followed_by	trainable_variables
enas.cifar10.controller.ConvController.build_trainer	call	assign_sub
Variable	has_name	train_step
matmul	followed_by	reduce_sum
gradients	followed_by	global_norm
cond	followed_by	MomentumOptimizer
clip_by_norm	followed_by	Variable
assign_sub	followed_by	control_dependencies
get_train_ops	followed_by	group
exponential_decay	call	maximum
enas.cifar10.controller.ConvController.build_trainer	call	control_dependencies
IndexedSlices	followed_by	clip_by_norm
enas.cifar10.controller.ConvController.build_trainer	call	get_train_ops
concat	followed_by	matmul
maximum	has_arg0	0
Variable	followed_by	cond
to_float	followed_by	to_float
sqrt	followed_by	sqrt
enas.cifar10.controller.ConvController.build_trainer	call	identity
trainable_variables	followed_by	get_train_ops
get_train_ops	call	IndexedSlices
sqrt	call	reduce_sum
Variable	has_arg0	0
get_train_ops	call	exponential_decay
Variable	has_arg0	0.0
clip_by_global_norm	followed_by	clip_by_norm
enas.cifar10.controller.ConvController.build_trainer	call	concat
enas.cifar10.controller.ConvController.build_trainer	call	matmul
Variable	followed_by	get_train_ops
enas.cifar10.controller.ConvController.build_trainer	call	trainable_variables
add_n	followed_by	gradients
get_train_ops	call	maximum
enas.cifar10.controller.ConvController.build_trainer	call	to_float
get_train_ops	call	gradients
Variable	has_name	critic_train_step
enas.cifar10.controller.ConvController.build_trainer	call	group
enas.cifar10.controller.ConvController.build_trainer	call	reduce_mean
get_train_ops	call	GradientDescentOptimizer
cond	call	greater_equal
Variable	followed_by	assign_sub
get_train_ops	call	sqrt
identity	followed_by	Variable
to_float	followed_by	concat
enas.cifar10.controller.ConvController.build_trainer	call	Variable
cond	call	less
get_train_ops	call	cond
reduce_mean	followed_by	Variable
clip_by_norm	followed_by	IndexedSlices
Adam	followed_by	reduce_sum
global_norm	followed_by	sqrt
get_train_ops	call	add_n
get_train_ops	call	clip_by_norm
cond	followed_by	exponential_decay
reduce_mean	followed_by	reduce_mean
GradientDescentOptimizer	followed_by	AdamOptimizer
get_train_ops	followed_by	Adam
get_train_ops	call	global_norm
get_train_ops	call	SyncReplicasOptimizer
AdamOptimizer	followed_by	SyncReplicasOptimizer
Variable	has_name	T_i
sqrt	followed_by	clip_by_global_norm
get_train_ops	call	Variable
get_train_ops	call	AdamOptimizer
get_train_ops	call	IndexedSlices
global_norm	followed_by	sqrt
sqrt	followed_by	clip_by_global_norm
cond	followed_by	MomentumOptimizer
reduce_sum	followed_by	add_n
sparse_softmax_cross_entropy_with_logits	has_labels	self.y_train
Variable	followed_by	cond
enas.cifar10.models.Model._build_train	call	sparse_softmax_cross_entropy_with_logits
get_train_ops	call	gradients
sqrt	followed_by	sqrt
get_train_ops	call	MomentumOptimizer
exponential_decay	call	maximum
equal	followed_by	to_int32
enas.cifar10.models.Model._build_train	call	equal
count_model_params	followed_by	Variable
IndexedSlices	followed_by	clip_by_norm
sparse_softmax_cross_entropy_with_logits	followed_by	reduce_mean
enas.cifar10.models.Model._build_train	call	trainable_variables
get_train_ops	call	AdamOptimizer
to_int32	followed_by	reduce_sum
GradientDescentOptimizer	followed_by	AdamOptimizer
get_train_ops	call	maximum
MomentumOptimizer	followed_by	GradientDescentOptimizer
maximum	has_arg0	0
get_train_ops	call	reduce_sum
sqrt	call	reduce_sum
get_train_ops	call	add_n
trainable_variables	followed_by	count_model_params
get_train_ops	call	sqrt
enas.cifar10.models.Model._build_train	call	reduce_sum
enas.cifar10.models.Model._build_train	call	count_model_params
get_train_ops	call	SyncReplicasOptimizer
get_train_ops	call	cond
Variable	has_arg0	0
enas.cifar10.models.Model._build_train	call	_model
Variable	has_name	global_step
get_train_ops	call	exponential_decay
cond	call	less
enas.cifar10.models.Model._build_train	call	reduce_mean
get_train_ops	call	GradientDescentOptimizer
AdamOptimizer	followed_by	SyncReplicasOptimizer
_model	followed_by	sparse_softmax_cross_entropy_with_logits
clip_by_global_norm	followed_by	clip_by_norm
clip_by_norm	followed_by	Variable
Variable	followed_by	Variable
cond	followed_by	exponential_decay
cond	call	greater_equal
Variable	has_name	T_i
enas.cifar10.models.Model._build_train	call	to_int32
gradients	followed_by	global_norm
Variable	followed_by	get_train_ops
argmax	has_axis	1
clip_by_norm	followed_by	IndexedSlices
maximum	followed_by	cond
enas.cifar10.models.Model._build_train	call	get_train_ops
get_train_ops	call	clip_by_global_norm
get_train_ops	call	clip_by_norm
add_n	followed_by	gradients
enas.cifar10.models.Model._build_train	call	Variable
to_int32	followed_by	equal
MomentumOptimizer	has_arg0	0.9
get_train_ops	call	Variable
Variable	has_name	last_reset
reduce_mean	followed_by	argmax
reduce_sum	followed_by	trainable_variables
get_train_ops	call	global_norm
exponential_decay	followed_by	maximum
enas.cifar10.models.Model._build_train	call	argmax
argmax	followed_by	to_int32
batch_norm_with_mask	call	scatter_sub
create_weight	followed_by	create_weight
conv2d	has_arg0	SAME
to_int32	followed_by	reshape
_conv_branch	call	conv2d
clip_by_norm	followed_by	Variable
equal	followed_by	variable_scope
case	call	constant
GradientDescentOptimizer	followed_by	AdamOptimizer
Saver	followed_by	CheckpointSaverHook
_build_test	call	equal
variable_scope	has_arg0	skip
_pool_branch	call	batch_norm
norm	followed_by	Adam
range	followed_by	cond
_fixed_layer	call	variable_scope
batch_norm_with_mask	followed_by	relu
_build_train	call	sparse_softmax_cross_entropy_with_logits
range	has_arg0	0
conv2d	followed_by	concat
get_train_ops	call	add_n
_enas_layer	call	relu
logical_or	followed_by	boolean_mask
_conv_branch	call	create_weight
create_weight	followed_by	relu
connect_controller	call	_build_test
relu	followed_by	conv2d
connect_controller	call	_build_train
_build_train	call	trainable_variables
variable_scope	has_arg0	conv_1
to_float	followed_by	reduce_sum
avg_pool	followed_by	variable_scope
_enas_layer	call	equal
avg_pool	has_arg0	VALID
get_train_ops	call	IndexedSlices
_enas_layer	call	reshape
read_data	followed_by	read_data
_fixed_layer	followed_by	variable_scope
Variable	followed_by	trainable_variables
range	followed_by	variable_scope
batch_norm	call	variable_scope
get_ops	call	build_trainer
variable_scope	has_arg0	path2_conv
get_variable	followed_by	fused_batch_norm
_enas_layer	call	_conv_branch
_conv_branch	call	transpose
equal	has_arg0	5
CheckpointSaverHook	followed_by	ConfigProto
reshape	followed_by	reshape
global_norm	followed_by	sqrt
_fixed_layer	call	create_weight
exponential_decay	followed_by	maximum
IndexedSlices	followed_by	clip_by_norm
_pool_branch	call	average_pooling2d
variable_scope	followed_by	_factorized_reduction
load	followed_by	reshape
_model	followed_by	argmax
_conv_branch	call	range
get_variable	has_arg0	offset
transpose	followed_by	conv2d
variable_scope	followed_by	_conv_branch
get_train_ops	call	SyncReplicasOptimizer
_enas_layer	call	boolean_mask
constant	has_arg0	0
variable_scope	followed_by	range
sqrt	followed_by	clip_by_global_norm
to_int32	followed_by	equal
train	call	ConfigProto
_fixed_layer	call	_pool_branch
reshape	followed_by	variable_scope
AdamOptimizer	followed_by	SyncReplicasOptimizer
create_weight	followed_by	matmul
boolean_mask	followed_by	reshape
get_train_ops	call	MomentumOptimizer
get_train_ops	call	sqrt
_build_test	call	_model
sparse_softmax_cross_entropy_with_logits	has_labels	self.y_train
conv2d	followed_by	range
variable_scope	has_arg0	conv_1x1
_pool_branch	followed_by	variable_scope
build_trainer	call	to_float
create_weight	followed_by	transpose
conv2d	followed_by	pad
Variable	has_arg0	0
boolean_mask	followed_by	get_variable
build_trainer	call	trainable_variables
clip_by_global_norm	followed_by	clip_by_norm
_enas_layer	call	cond
_build_train	call	equal
_conv_branch	call	logical_and
print_user_flags	followed_by	train
concat	followed_by	shape
argmax	followed_by	to_int32
Graph	followed_by	get_ops
_enas_layer	call	logical_and
create_weight	call	get_variable
mean	followed_by	reshape
batch_norm_with_mask	call	variable_scope
_build_train	call	Variable
variable_scope	followed_by	create_weight
equal	followed_by	zeros_like
_build_train	call	argmax
_fixed_layer	call	relu
_model	call	variable_scope
_model	call	range
concat	has_axis	3
avg_pool	has_data_format	self.data_format
batch_norm	call	identity
get_train_ops	call	exponential_decay
_get_C	followed_by	create_weight
_model	call	_factorized_reduction
enas.cifar10.main.main	call	Logger.__init__
_conv_branch	call	batch_norm
variable_scope	has_arg0	pool
train	call	CheckpointSaverHook
MomentumOptimizer	followed_by	GradientDescentOptimizer
add_n	followed_by	gradients
read_data	call	reshape
cond	call	equal
train	call	read_data
batch_norm_with_mask	call	fused_batch_norm
pad	followed_by	avg_pool
fused_batch_norm	followed_by	control_dependencies
variable_scope	followed_by	get_variable
equal	has_arg0	2
concat	followed_by	batch_norm
_enas_layer	call	batch_norm
get_variable	has_arg0	moving_mean
batch_norm	call	fused_batch_norm
_build_train	call	reduce_mean
_conv_branch	call	batch_norm_with_mask
average_pooling2d	followed_by	max_pooling2d
equal	has_arg0	0
Adam	followed_by	connect_controller
create_weight	followed_by	concat
batch_norm	followed_by	_get_strides
_build_train	call	get_train_ops
variable_scope	has_arg0	branch_1
_pool_branch	call	variable_scope
cond	call	greater_equal
_model	followed_by	sparse_softmax_cross_entropy_with_logits
variable_scope	has_arg0	branch_5
build_trainer	call	control_dependencies
batch_norm	call	get_variable
argmax	has_axis	1
batch_norm	followed_by	create_weight
equal	has_arg0	4
get_ops	call	Adam
transpose	followed_by	reshape
_build_train	followed_by	_build_valid
_build_valid	call	_model
sqrt	call	reduce_sum
_model	call	conv2d
variable_scope	has_arg0	inp_conv_1
variable_scope	has_arg0	path1_conv
Variable	followed_by	get_train_ops
SingularMonitoredSession	has_checkpoint_dir	FLAGS.output_dir
transpose	followed_by	transpose
_factorized_reduction	call	create_weight
get_train_ops	call	clip_by_norm
_enas_layer	call	constant
get_ops	followed_by	Saver
fused_batch_norm	followed_by	boolean_mask
_read_data	call	reshape
get_train_ops	call	AdamOptimizer
_fixed_layer	call	range
reshape	followed_by	concat
separable_conv2d	has_data_format	self.data_format
_fixed_layer	call	_conv_branch
variable_scope	has_arg0	stem_conv
trainable_variables	followed_by	get_train_ops
dropout	followed_by	variable_scope
global_avg_pool	followed_by	dropout
count_model_params	followed_by	Variable
variable_scope	has_arg0	branch_0
_factorized_reduction	call	batch_norm
_factorized_reduction	call	avg_pool
_build_train	call	reduce_sum
_fixed_layer	call	conv2d
boolean_mask	followed_by	fused_batch_norm
_model	call	matmul
_model	call	_enas_layer
batch_norm_with_mask	call	get_variable
variable_scope	has_arg0	fc
pad	followed_by	pad
maximum	has_arg0	0
_enas_layer	call	conv2d
_conv_branch	followed_by	equal
_model	call	dropout
scatter_sub	followed_by	control_dependencies
_build_train	call	count_model_params
_read_data	followed_by	mean
range	followed_by	logical_and
read_data	followed_by	Graph
_build_valid	followed_by	_build_test
shape	followed_by	reshape
sparse_softmax_cross_entropy_with_logits	followed_by	reduce_mean
Variable	has_name	train_step
_conv_branch	call	relu
get_train_ops	call	clip_by_global_norm
variable_scope	has_arg0	path_conv
reduce_sum	followed_by	add_n
get_variable	has_arg0	moving_variance
batch_norm	followed_by	variable_scope
variable_scope	followed_by	variable_scope
identity	followed_by	Variable
_conv_branch	call	reshape
get_ops	call	connect_controller
_enas_layer	call	create_weight
equal	has_arg0	3
equal	followed_by	to_int32
get_train_ops	call	reduce_sum
reshape	followed_by	transpose
trainable_variables	followed_by	count_model_params
exponential_decay	call	maximum
build_trainer	call	reduce_sum
batch_norm_with_mask	call	reshape
train	call	get_ops
_enas_layer	call	add_n
Variable	has_name	T_i
maximum	followed_by	cond
range	followed_by	range
equal	has_arg0	1
variable_scope	has_arg0	branch_3
variable_scope	has_arg0	branch_4
_enas_layer	call	range
_conv_branch	call	variable_scope
Variable	followed_by	assign_sub
get_train_ops	call	GradientDescentOptimizer
_factorized_reduction	call	conv2d
to_float	followed_by	to_float
_fixed_layer	call	batch_norm
_enas_layer	call	shape
_enas_layer	call	variable_scope
reduce_sum	followed_by	trainable_variables
_model	call	global_avg_pool
enas.cifar10.main.main	call	print_user_flags
cond	followed_by	add_n
enas.cifar10.main.main	call	train
batch_norm_with_mask	call	where
Variable	has_name	global_step
get_train_ops	call	Variable
get_variable	followed_by	get_variable
Saver	has_max_to_keep	2
build_trainer	call	assign_sub
Variable	has_arg0	0.0
_model	call	create_weight
identity	followed_by	fused_batch_norm
_read_data	call	load
reduce_sum	followed_by	Variable
_model	call	batch_norm
clip_by_norm	followed_by	IndexedSlices
_build_valid	call	to_int32
build_trainer	call	Variable
relu	followed_by	variable_scope
variable_scope	followed_by	_enas_layer
build_trainer	followed_by	connect_controller
batch_norm	followed_by	relu
equal	followed_by	case
Variable	followed_by	Variable
_read_data	followed_by	_read_data
_conv_branch	call	separable_conv2d
conv2d	has_data_format	self.data_format
reduce_mean	followed_by	argmax
greater	has_arg0	0
boolean_mask	followed_by	scatter_sub
read_data	call	_read_data
where	followed_by	to_int32
range	has_dtype	tf.int32
average_pooling2d	has_arg0	SAME
_pool_branch	followed_by	equal
concat	followed_by	concat
connect_controller	followed_by	build_trainer
get_train_ops	call	gradients
constant	followed_by	range
Logger.__init__	followed_by	print_user_flags
MomentumOptimizer	has_arg0	0.9
get_train_ops	call	cond
_pool_branch	call	conv2d
reshape	followed_by	conv2d
_factorized_reduction	call	_get_C
control_dependencies	followed_by	identity
_factorized_reduction	call	variable_scope
variable_scope	has_arg0	branch_2
sqrt	followed_by	sqrt
batch_norm_with_mask	call	boolean_mask
_factorized_reduction	call	pad
get_variable	followed_by	boolean_mask
gradients	followed_by	global_norm
_pool_branch	call	relu
_get_strides	followed_by	avg_pool
get_train_ops	call	global_norm
create_weight	followed_by	separable_conv2d
_factorized_reduction	followed_by	global_avg_pool
_build_train	call	to_int32
batch_norm_with_mask	call	to_int32
train	call	Graph
add_n	followed_by	batch_norm
_fixed_layer	call	concat
concat	followed_by	relu
build_trainer	call	get_train_ops
_build_valid	call	equal
Variable	followed_by	cond
logical_and	followed_by	batch_norm_with_mask
variable_scope	followed_by	average_pooling2d
train	call	SingularMonitoredSession
connect_controller	call	_build_valid
_build_train	call	_model
_pool_branch	call	create_weight
cond	followed_by	MomentumOptimizer
ConfigProto	followed_by	SingularMonitoredSession
read_data	call	mean
identity	followed_by	boolean_mask
Variable	has_name	last_reset
batch_norm_with_mask	followed_by	create_weight
variable_scope	followed_by	_get_C
train	call	Saver
conv2d	followed_by	batch_norm
get_train_ops	call	maximum
case	followed_by	variable_scope
_enas_layer	call	case
range	followed_by	concat
_build_valid	call	reduce_sum
batch_norm_with_mask	call	identity
_enas_layer	call	_pool_branch
batch_norm	call	control_dependencies
_enas_layer	call	concat
_build_test	call	reduce_sum
to_int32	followed_by	reduce_sum
_factorized_reduction	call	_get_strides
_read_data	call	transpose
_enas_layer	call	logical_or
_enas_layer	followed_by	_fixed_layer
_build_test	call	argmax
create_weight	followed_by	constant
concat	has_axis	1
get_variable	has_arg0	scale
_pool_branch	call	max_pooling2d
_build_valid	call	argmax
cond	followed_by	exponential_decay
global_avg_pool	call	reduce_mean
reduce_mean	followed_by	reduce_mean
variable_scope	followed_by	_pool_branch
logical_and	followed_by	logical_or
reshape	followed_by	separable_conv2d
get_ops	call	norm
create_weight	followed_by	conv2d
_model	call	_fixed_layer
separable_conv2d	followed_by	range
_conv_branch	followed_by	variable_scope
variable_scope	has_arg0	final_conv
cond	call	less
_factorized_reduction	call	concat
_build_test	call	to_int32
boolean_mask	followed_by	boolean_mask
scatter_sub	followed_by	scatter_sub
relu	call	where
max_pooling2d	has_arg0	SAME
cond	call	zeros_like
batch_norm_with_mask	call	control_dependencies
batch_norm	followed_by	range
build_trainer	call	identity
assign_sub	followed_by	control_dependencies
concat	followed_by	variable_scope
where	call	greater
separable_conv2d	followed_by	batch_norm
maximum	followed_by	cond
Variable	has_name	last_reset
clip_by_global_norm	followed_by	clip_by_norm
clip_by_norm	followed_by	Variable
Variable	followed_by	Variable
get_train_ops	call	GradientDescentOptimizer
sqrt	call	reduce_sum
get_train_ops	call	reduce_sum
IndexedSlices	followed_by	clip_by_norm
exponential_decay	call	maximum
constant	has_arg0	0.0
gradients	followed_by	global_norm
enas.cifar10.micro_controller.MicroController.build_trainer	call	control_dependencies
sqrt	followed_by	clip_by_global_norm
get_train_ops	call	IndexedSlices
GradientDescentOptimizer	followed_by	AdamOptimizer
get_train_ops	call	global_norm
AdamOptimizer	followed_by	SyncReplicasOptimizer
MomentumOptimizer	followed_by	GradientDescentOptimizer
enas.cifar10.micro_controller.MicroController.build_trainer	call	identity
Variable	followed_by	assign_sub
to_float	followed_by	reduce_sum
get_train_ops	call	cond
get_train_ops	call	gradients
sqrt	followed_by	sqrt
control_dependencies	followed_by	identity
get_train_ops	followed_by	constant
enas.cifar10.micro_controller.MicroController.build_trainer	call	to_float
get_train_ops	call	maximum
constant	has_dtype	tf.float32
Variable	followed_by	cond
enas.cifar10.micro_controller.MicroController.build_trainer	call	assign_sub
to_float	followed_by	to_float
exponential_decay	followed_by	maximum
cond	followed_by	exponential_decay
enas.cifar10.micro_controller.MicroController.build_trainer	call	reduce_sum
cond	call	less
get_train_ops	call	MomentumOptimizer
get_train_ops	call	AdamOptimizer
trainable_variables	followed_by	get_train_ops
Variable	has_name	T_i
identity	followed_by	Variable
assign_sub	followed_by	control_dependencies
enas.cifar10.micro_controller.MicroController.build_trainer	call	constant
add_n	followed_by	gradients
get_train_ops	call	clip_by_global_norm
maximum	has_arg0	0
cond	followed_by	MomentumOptimizer
reduce_sum	followed_by	Variable
enas.cifar10.micro_controller.MicroController.build_trainer	call	trainable_variables
get_train_ops	call	exponential_decay
get_train_ops	call	sqrt
get_train_ops	call	Variable
get_train_ops	call	add_n
Variable	has_arg0	0.0
global_norm	followed_by	sqrt
Variable	has_name	train_step
MomentumOptimizer	has_arg0	0.9
reduce_sum	followed_by	add_n
enas.cifar10.micro_controller.MicroController.build_trainer	call	Variable
enas.cifar10.micro_controller.MicroController.build_trainer	call	get_train_ops
Variable	followed_by	trainable_variables
cond	call	greater_equal
Variable	has_arg0	0
get_train_ops	call	SyncReplicasOptimizer
get_train_ops	call	clip_by_norm
clip_by_norm	followed_by	IndexedSlices
_build_test	call	equal
zeros	has_dtype	tf.float32
average_pooling2d	followed_by	max_pooling2d
random_uniform	followed_by	floor
_enas_layer	call	conv2d
_factorized_reduction	call	variable_scope
batch_norm	call	control_dependencies
_enas_conv	call	reshape
_apply_drop_path	followed_by	variable_scope
_fixed_conv	call	batch_norm
_maybe_calibrate_size	followed_by	range
add_n	followed_by	where
_build_train	call	reduce_mean
_enas_cell	call	stack
_build_train	call	to_int32
stack	followed_by	gather
variable_scope	followed_by	_fixed_conv
count_model_params	followed_by	relu
relu	call	where
_enas_conv	call	zeros
_model	call	_fixed_layer
AdamOptimizer	followed_by	SyncReplicasOptimizer
shape	followed_by	shape
reshape	followed_by	shape
equal	has_arg0	0
_fixed_combine	call	_factorized_reduction
to_int32	followed_by	reduce_sum
_factorized_reduction	call	_get_strides
GradientDescentOptimizer	followed_by	AdamOptimizer
_factorized_reduction	call	_get_C
_fixed_layer	call	average_pooling2d
variable_scope	has_arg0	path2_conv
_factorized_reduction	call	avg_pool
_factorized_reduction	followed_by	concat
reduce_mean	followed_by	argmax
variable_scope	has_arg0	final_combine
to_int32	followed_by	reshape
get_train_ops	call	global_norm
max_pooling2d	followed_by	create_weight
concat	has_axis	1
_enas_cell	followed_by	one_hot
_enas_layer	call	to_int32
_build_train	call	sparse_softmax_cross_entropy_with_logits
sparse_softmax_cross_entropy_with_logits	has_labels	self.y_train
identity	followed_by	fused_batch_norm
get_variable	followed_by	fused_batch_norm
_enas_layer	call	stack
_get_C	followed_by	_get_strides
max_pooling2d	followed_by	_get_C
Reduction	followed_by	variable_scope
_maybe_calibrate_size	call	batch_norm
_build_valid	call	argmax
get_variable	has_arg0	offset
variable_scope	followed_by	max_pooling2d
cond	followed_by	MomentumOptimizer
_model	call	Reduction
batch_norm	call	fused_batch_norm
cond	call	less
create_weight	call	get_variable
_fixed_conv	call	create_weight
_factorized_reduction	call	conv2d
_model	call	variable_scope
maximum	followed_by	cond
create_weight	followed_by	relu
_fixed_layer	call	variable_scope
relu	followed_by	global_avg_pool
_get_C	followed_by	variable_scope
_maybe_calibrate_size	call	create_weight
minimum	followed_by	drop_path
exponential_decay	followed_by	maximum
reshape	followed_by	create_weight
pad	followed_by	avg_pool
_enas_cell	call	_enas_conv
_enas_layer	call	gather
Variable	has_name	last_reset
create_weight	followed_by	matmul
_build_train	call	equal
_model	call	_get_C
_enas_cell	call	reshape
reshape	followed_by	relu
_build_test	call	to_int32
add_n	followed_by	gradients
ones	followed_by	create_weight
_fixed_layer	call	_get_C
_model	call	conv2d
_apply_drop_path	call	to_float
variable_scope	has_arg0	stem_conv
_enas_cell	call	create_weight
variable_scope	has_arg0	avg_pool
batch_norm	followed_by	_apply_drop_path
_build_train	followed_by	_build_valid
_enas_layer	call	add_n
Variable	has_name	T_i
concat	followed_by	concat
_enas_cell	call	batch_norm
_apply_drop_path	call	minimum
_factorized_reduction	followed_by	create_weight
_fixed_layer	followed_by	Reduction
_fixed_combine	call	variable_scope
Variable	followed_by	cond
_build_train	call	_model
normal	followed_by	_factorized_reduction
_maybe_calibrate_size	call	_get_HW
_enas_cell	call	max_pooling2d
max_pooling2d	has_arg0	SAME
_fixed_layer	call	_fixed_combine
_model	call	_factorized_reduction
variable_scope	followed_by	range
batch_norm	call	get_variable
_enas_layer	call	transpose
IndexedSlices	followed_by	clip_by_norm
_model	followed_by	argmax
range	followed_by	stack
get_variable	has_arg0	moving_variance
batch_norm	followed_by	relu
control_dependencies	followed_by	identity
batch_norm	followed_by	_get_strides
_enas_conv	followed_by	_enas_conv
_model	call	normal
get_train_ops	call	AdamOptimizer
_factorized_reduction	followed_by	_enas_layer
variable_scope	has_arg0	layer_base
variable_scope	has_arg0	aux_head
one_hot	followed_by	variable_scope
to_float	followed_by	to_float
_enas_cell	call	average_pooling2d
conv2d	followed_by	concat
_model	call	_get_HW
clip_by_global_norm	followed_by	clip_by_norm
_get_C	followed_by	average_pooling2d
get_variable	has_arg0	scale
relu	followed_by	conv2d
reduce_mean	followed_by	reduce_mean
create_weight	followed_by	create_weight
to_float	followed_by	minimum
clip_by_norm	followed_by	IndexedSlices
variable_scope	followed_by	_factorized_reduction
_build_valid	followed_by	_build_test
get_train_ops	call	GradientDescentOptimizer
get_variable	followed_by	get_variable
_model	call	range
trainable_variables	followed_by	count_model_params
_get_C	followed_by	create_weight
_build_test	call	argmax
MomentumOptimizer	followed_by	GradientDescentOptimizer
_fixed_conv	call	relu
one_hot	followed_by	add_n
get_train_ops	call	clip_by_global_norm
create_weight	followed_by	reshape
variable_scope	has_arg0	proj
_fixed_layer	call	conv2d
_model	call	trainable_variables
variable_scope	has_arg0	conv
sqrt	call	reduce_sum
_maybe_calibrate_size	call	conv2d
_fixed_conv	call	_get_strides
pad	followed_by	pad
count_model_params	followed_by	get_train_ops
_build_train	call	get_train_ops
_fixed_layer	call	_apply_drop_path
relu	followed_by	variable_scope
_enas_layer	call	shape
_get_HW	followed_by	create_weight
random_uniform	has_dtype	tf.float32
variable_scope	has_arg0	fc
_model	call	average_pooling2d
reduce_mean	followed_by	sparse_softmax_cross_entropy_with_logits
variable_scope	followed_by	_enas_cell
get_train_ops	call	Variable
gather	followed_by	reshape
gather	has_axis	0
_fixed_combine	call	min
_factorized_reduction	followed_by	variable_scope
min	followed_by	variable_scope
_maybe_calibrate_size	call	_factorized_reduction
_build_valid	call	equal
_fixed_layer	call	batch_norm
_enas_conv	call	fused_batch_norm
gradients	followed_by	global_norm
where	call	equal
_fixed_combine	call	_get_HW
_build_train	call	reduce_sum
_enas_cell	call	_get_C
_get_strides	followed_by	variable_scope
global_avg_pool	call	reduce_mean
sqrt	followed_by	clip_by_global_norm
_get_C	followed_by	_get_HW
get_variable	has_arg0	moving_mean
variable_scope	has_arg0	y_conv
create_weight	followed_by	conv2d
enas.cifar10.micro_child.MicroChild.connect_controller	call	_build_test
relu	followed_by	separable_conv2d
_enas_cell	call	conv2d
_fixed_conv	followed_by	_get_C
reduce_sum	followed_by	trainable_variables
_enas_layer	call	where
get_train_ops	call	gradients
_model	call	_enas_layer
reshape	followed_by	size
relu	followed_by	average_pooling2d
_factorized_reduction	call	pad
variable_scope	followed_by	_get_C
_model	call	batch_norm
global_avg_pool	followed_by	dropout
_model	call	count_model_params
reduce_sum	followed_by	add_n
_enas_cell	call	variable_scope
fused_batch_norm	followed_by	control_dependencies
variable_scope	has_arg0	final_conv
gather	followed_by	shape
_maybe_calibrate_size	call	_get_C
_enas_layer	followed_by	_fixed_layer
avg_pool	followed_by	variable_scope
_factorized_reduction	call	concat
stack	followed_by	variable_scope
get_train_ops	call	clip_by_norm
variable_scope	followed_by	get_variable
variable_scope	has_arg0	y
variable_scope	followed_by	create_weight
variable_scope	has_arg0	path1_conv
_model	call	create_weight
zeros	followed_by	ones
_fixed_conv	call	variable_scope
cond	call	greater_equal
conv2d	has_arg0	VALID
variable_scope	followed_by	variable_scope
_fixed_layer	call	_factorized_reduction
get_train_ops	call	exponential_decay
_enas_layer	call	_maybe_calibrate_size
one_hot	has_dtype	tf.int32
conv2d	followed_by	pad
global_norm	followed_by	sqrt
_maybe_calibrate_size	call	variable_scope
get_train_ops	call	MomentumOptimizer
global_avg_pool	followed_by	create_weight
_fixed_layer	call	range
greater	has_arg0	0
Variable	has_arg0	0
_get_strides	followed_by	avg_pool
range	followed_by	variable_scope
_fixed_layer	call	relu
zeros	followed_by	range
_factorized_reduction	call	batch_norm
variable_scope	has_arg0	pool_y
relu	followed_by	_factorized_reduction
_enas_conv	call	variable_scope
_maybe_calibrate_size	followed_by	variable_scope
_build_valid	call	reduce_sum
conv2d	has_arg0	SAME
_fixed_conv	call	_get_C
dropout	followed_by	variable_scope
_fixed_combine	call	concat
concat	has_axis	3
maximum	has_arg0	0
get_train_ops	call	add_n
_get_HW	followed_by	variable_scope
reshape	followed_by	variable_scope
_fixed_layer	followed_by	normal
equal	followed_by	to_int32
clip_by_norm	followed_by	Variable
_enas_conv	followed_by	stack
size	followed_by	stack
average_pooling2d	has_arg0	SAME
_model	call	dropout
average_pooling2d	followed_by	_get_C
_build_test	call	reduce_sum
range	followed_by	_get_C
matmul	followed_by	trainable_variables
MomentumOptimizer	has_arg0	0.9
_model	followed_by	sparse_softmax_cross_entropy_with_logits
_enas_conv	call	_get_C
batch_norm	followed_by	zeros
variable_scope	followed_by	_enas_layer
exponential_decay	call	maximum
variable_scope	followed_by	zeros
_build_valid	call	to_int32
zeros	has_dtype	np.int32
_factorized_reduction	call	create_weight
variable_scope	has_arg0	path_conv
_get_HW	followed_by	_get_C
average_pooling2d	has_arg0	VALID
_enas_conv	call	separable_conv2d
batch_norm	followed_by	reshape
average_pooling2d	followed_by	variable_scope
separable_conv2d	followed_by	fused_batch_norm
argmax	has_axis	1
to_int32	followed_by	equal
_maybe_calibrate_size	call	relu
_enas_layer	call	range
_build_train	call	trainable_variables
argmax	followed_by	to_int32
transpose	followed_by	reshape
where	call	greater
_enas_layer	call	relu
get_train_ops	call	IndexedSlices
where	followed_by	to_int32
_fixed_layer	call	zeros
get_train_ops	call	cond
variable_scope	has_arg0	pool_x
batch_norm	followed_by	_enas_conv
batch_norm	followed_by	_get_C
_enas_layer	call	reshape
separable_conv2d	followed_by	batch_norm
separable_conv2d	has_data_format	self.data_format
_fixed_layer	call	_fixed_conv
variable_scope	followed_by	relu
minimum	has_arg0	1.0
variable_scope	followed_by	average_pooling2d
_enas_layer	call	one_hot
create_weight	followed_by	gather
_model	call	global_avg_pool
average_pooling2d	has_data_format	self.actual_data_format
variable_scope	followed_by	_get_HW
batch_norm	followed_by	range
batch_norm	call	variable_scope
_get_C	followed_by	_factorized_reduction
get_train_ops	call	maximum
enas.cifar10.micro_child.MicroChild.connect_controller	call	_build_valid
_build_train	call	argmax
stack	has_axis	0
batch_norm	followed_by	variable_scope
drop_path	call	div
conv2d	followed_by	batch_norm
_enas_conv	call	relu
_enas_conv	call	range
get_train_ops	call	sqrt
_enas_cell	call	relu
variable_scope	has_arg0	calibrate
_enas_layer	call	variable_scope
_enas_layer	call	batch_norm
floor	followed_by	div
_enas_layer	call	create_weight
_enas_layer	call	_enas_cell
conv2d	has_data_format	self.data_format
cond	followed_by	exponential_decay
_build_valid	call	_model
drop_path	call	shape
_enas_conv	call	ones
_model	call	relu
variable_scope	followed_by	global_avg_pool
get_train_ops	call	reduce_sum
avg_pool	has_arg0	VALID
_build_test	call	_model
_model	call	matmul
_fixed_layer	call	create_weight
get_train_ops	call	SyncReplicasOptimizer
_fixed_layer	call	_maybe_calibrate_size
Variable	followed_by	Variable
_build_train	call	count_model_params
_fixed_conv	call	range
_enas_conv	call	create_weight
_fixed_conv	call	separable_conv2d
variable_scope	has_arg0	x_conv
reshape	call	shape
sparse_softmax_cross_entropy_with_logits	followed_by	reduce_mean
max_pooling2d	has_data_format	self.actual_data_format
enas.cifar10.micro_child.MicroChild.connect_controller	call	_build_train
variable_scope	has_arg0	max_pool
_enas_layer	call	size
ones	has_dtype	tf.float32
batch_norm	call	identity
shape	followed_by	transpose
_fixed_layer	call	max_pooling2d
shape	followed_by	random_uniform
concat	followed_by	batch_norm
variable_scope	has_arg0	x
_apply_drop_path	call	drop_path
drop_path	call	floor
avg_pool	has_data_format	self.data_format
_apply_drop_path	followed_by	_fixed_combine
variable_scope	has_arg0	bn
sqrt	followed_by	sqrt
drop_path	call	random_uniform
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	argmax
map_fn	followed_by	_model
variable_scope	has_arg0	stem_conv
variable_scope	has_arg0	fc
_enas_layer	call	relu
variable_scope	has_arg0	pool
variable_scope	followed_by	create_weight
variable_scope	has_arg0	final_conv
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	_model
create_weight	followed_by	relu
variable_scope	has_arg0	path1_conv
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	equal
boolean_mask	followed_by	get_variable
_enas_layer	call	variable_scope
create_weight	followed_by	conv2d
variable_scope	has_arg0	path_conv
concat	followed_by	concat
argmax	followed_by	to_int32
_fixed_layer	call	concat
equal	followed_by	variable_scope
batch_norm	call	variable_scope
boolean_mask	followed_by	reshape
avg_pool	followed_by	variable_scope
create_weight	followed_by	separable_conv2d
_fixed_layer	call	variable_scope
variable_scope	has_arg0	skip
_conv_branch	call	separable_conv2d
separable_conv2d	followed_by	range
_enas_layer	call	logical_or
avg_pool	has_arg0	VALID
conv2d	has_data_format	self.data_format
_conv_branch	call	variable_scope
equal	has_arg0	1
fused_batch_norm	followed_by	boolean_mask
_conv_branch	call	conv2d
relu	call	where
separable_conv2d	has_data_format	self.data_format
logical_or	followed_by	boolean_mask
batch_norm_with_mask	call	where
_factorized_reduction	call	concat
reshape	followed_by	variable_scope
batch_norm	followed_by	create_weight
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	reduce_sum
reshape	followed_by	separable_conv2d
variable_scope	followed_by	get_variable
_conv_branch	followed_by	variable_scope
reshape	followed_by	conv2d
_factorized_reduction	call	conv2d
transpose	followed_by	conv2d
variable_scope	followed_by	_enas_layer
case	followed_by	variable_scope
range	followed_by	range
reshape	followed_by	concat
cond	call	zeros_like
create_weight	call	get_variable
get_variable	has_arg0	offset
_model	call	range
_enas_layer	call	range
scatter_sub	followed_by	scatter_sub
_enas_layer	call	conv2d
batch_norm	call	identity
_enas_layer	call	create_weight
boolean_mask	followed_by	boolean_mask
batch_norm_with_mask	call	to_int32
case	call	constant
variable_scope	has_arg0	conv_1x1
batch_norm	followed_by	_get_strides
shuffle_batch	followed_by	map_fn
_model	call	conv2d
range	followed_by	cond
variable_scope	has_arg0	branch_2
equal	has_arg0	4
_model	call	_factorized_reduction
_model	call	create_weight
_pool_branch	call	average_pooling2d
relu	followed_by	conv2d
conv2d	followed_by	batch_norm
argmax	has_axis	1
batch_norm	call	get_variable
batch_norm_with_mask	call	boolean_mask
_fixed_layer	followed_by	variable_scope
relu	followed_by	variable_scope
conv2d	followed_by	range
_factorized_reduction	call	batch_norm
_model	call	global_avg_pool
equal	has_arg0	5
batch_norm_with_mask	call	reshape
shape	followed_by	reshape
batch_norm	call	fused_batch_norm
control_dependencies	followed_by	identity
identity	followed_by	boolean_mask
cond	followed_by	add_n
device	followed_by	transpose
scatter_sub	followed_by	control_dependencies
variable_scope	followed_by	_pool_branch
batch_norm	call	control_dependencies
_enas_layer	call	shape
_fixed_layer	call	range
range	has_dtype	tf.int32
equal	has_arg0	2
equal	has_arg0	0
create_weight	followed_by	create_weight
range	followed_by	concat
_conv_branch	call	create_weight
_fixed_layer	call	_pool_branch
_factorized_reduction	call	create_weight
_enas_layer	call	_conv_branch
_factorized_reduction	call	_get_strides
_model	call	dropout
where	call	greater
_enas_layer	call	_pool_branch
_enas_layer	call	add_n
range	has_arg0	0
_pool_branch	call	relu
_conv_branch	followed_by	equal
batch_norm	followed_by	variable_scope
get_variable	followed_by	fused_batch_norm
boolean_mask	followed_by	scatter_sub
variable_scope	followed_by	range
variable_scope	followed_by	_get_C
_model	call	matmul
_pool_branch	followed_by	variable_scope
_pool_branch	call	variable_scope
_model	call	batch_norm
_model	call	variable_scope
pad	followed_by	pad
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	map_fn
create_weight	followed_by	transpose
concat	followed_by	batch_norm
_pool_branch	call	batch_norm
_enas_layer	call	cond
variable_scope	has_arg0	branch_4
_enas_layer	followed_by	_fixed_layer
concat	has_axis	1
_conv_branch	call	logical_and
variable_scope	has_arg0	path2_conv
_factorized_reduction	call	pad
reduce_mean	followed_by	reduce_mean
cond	call	equal
transpose	followed_by	reshape
_enas_layer	call	logical_and
conv2d	followed_by	pad
logical_and	followed_by	batch_norm_with_mask
batch_norm_with_mask	call	scatter_sub
_enas_layer	call	concat
_fixed_layer	call	conv2d
batch_norm_with_mask	followed_by	relu
concat	followed_by	relu
get_variable	has_arg0	moving_mean
create_weight	followed_by	concat
_fixed_layer	call	relu
variable_scope	followed_by	_factorized_reduction
dropout	followed_by	variable_scope
global_avg_pool	followed_by	dropout
batch_norm_with_mask	call	variable_scope
concat	followed_by	variable_scope
batch_norm	followed_by	range
max_pooling2d	has_arg0	SAME
_factorized_reduction	call	variable_scope
_factorized_reduction	call	_get_C
_conv_branch	call	relu
to_int32	followed_by	reshape
boolean_mask	followed_by	fused_batch_norm
conv2d	followed_by	concat
avg_pool	has_data_format	self.data_format
add_n	followed_by	batch_norm
_pool_branch	call	conv2d
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	transpose
get_variable	followed_by	get_variable
get_variable	has_arg0	moving_variance
average_pooling2d	has_arg0	SAME
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	to_int32
pad	followed_by	avg_pool
variable_scope	has_arg0	branch_0
equal	followed_by	to_int32
equal	followed_by	zeros_like
_model	followed_by	argmax
fused_batch_norm	followed_by	control_dependencies
global_avg_pool	call	reduce_mean
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	shuffle_batch
identity	followed_by	fused_batch_norm
range	followed_by	logical_and
variable_scope	followed_by	variable_scope
average_pooling2d	followed_by	max_pooling2d
constant	followed_by	range
_model	call	_fixed_layer
_pool_branch	call	create_weight
_factorized_reduction	followed_by	global_avg_pool
concat	has_axis	3
_enas_layer	call	boolean_mask
_conv_branch	call	batch_norm
variable_scope	has_arg0	branch_1
_conv_branch	call	transpose
_enas_layer	call	case
_get_C	followed_by	create_weight
variable_scope	has_arg0	conv_1
_conv_branch	call	range
equal	has_arg0	3
equal	followed_by	case
_conv_branch	call	reshape
logical_and	followed_by	logical_or
separable_conv2d	followed_by	batch_norm
get_variable	has_arg0	scale
conv2d	has_arg0	SAME
get_variable	followed_by	boolean_mask
constant	has_arg0	0
batch_norm_with_mask	followed_by	create_weight
_pool_branch	call	max_pooling2d
greater	has_arg0	0
_fixed_layer	call	batch_norm
_get_strides	followed_by	avg_pool
transpose	followed_by	transpose
_enas_layer	call	equal
create_weight	followed_by	constant
variable_scope	followed_by	_conv_branch
range	followed_by	variable_scope
concat	followed_by	shape
variable_scope	has_arg0	inp_conv_1
batch_norm	followed_by	relu
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	device
_pool_branch	followed_by	equal
_fixed_layer	call	create_weight
create_weight	followed_by	matmul
variable_scope	has_arg0	branch_3
_factorized_reduction	call	avg_pool
transpose	followed_by	shuffle_batch
_conv_branch	call	batch_norm_with_mask
_model	call	_enas_layer
batch_norm_with_mask	call	control_dependencies
variable_scope	followed_by	average_pooling2d
variable_scope	has_arg0	branch_5
batch_norm_with_mask	call	identity
_fixed_layer	call	_conv_branch
batch_norm_with_mask	call	fused_batch_norm
_enas_layer	call	batch_norm
to_int32	followed_by	reduce_sum
_enas_layer	call	constant
to_int32	followed_by	equal
device	has_arg0	/cpu:0
_enas_layer	call	reshape
batch_norm_with_mask	call	get_variable
where	followed_by	to_int32
split	has_axis	1
constant	followed_by	constant
constant	has_name	log_prob
sigmoid	followed_by	sigmoid
_build_sampler	call	matmul
constant	has_name	layer_id
zeros_like	followed_by	matmul
zeros	followed_by	range
get_variable	followed_by	get_variable
_build_sampler	call	constant
range	followed_by	zeros
_create_params	call	range
_build_sampler	call	stack
stack	followed_by	reshape
variable_scope	followed_by	variable_scope
_create_params	call	reshape
range	followed_by	stack_lstm
_create_params	followed_by	_build_sampler
split	followed_by	sigmoid
matmul	followed_by	constant
constant	followed_by	while_loop
variable_scope	has_arg0	emb
TensorArray	followed_by	TensorArray
_build_sampler	call	reduce_sum
_build_sampler	call	range
reduce_sum	followed_by	reduce_sum
range	followed_by	range
get_variable	followed_by	reshape
matmul	followed_by	split
variable_scope	has_arg0	attention
constant	has_name	entropy
_build_sampler	call	reshape
range	followed_by	variable_scope
sigmoid	followed_by	tanh
reshape	followed_by	reduce_sum
lstm	call	sigmoid
get_variable	has_arg0	w_1
_create_params	call	get_variable
_build_sampler	call	while_loop
_create_params	call	variable_scope
tanh	followed_by	tanh
get_variable	has_arg0	v
_build_sampler	followed_by	_build_sampler
stack_lstm	followed_by	zeros_like
TensorArray	followed_by	zeros
variable_scope	followed_by	range
_build_sampler	call	zeros
lstm	call	matmul
while_loop	has_parallel_iterations	1
get_variable	has_arg0	w
enas.cifar10.micro_controller.MicroController.__init__	call	_build_sampler
variable_scope	has_arg0	lstm
split	has_arg0	4
concat	has_axis	1
constant	has_dtype	tf.float32
matmul	call	concat
variable_scope	followed_by	get_variable
constant	followed_by	variable_scope
range	has_arg0	2
_build_sampler	call	zeros_like
_build_sampler	call	TensorArray
_create_params	call	constant
reshape	followed_by	constant
enas.cifar10.micro_controller.MicroController.__init__	call	_create_params
get_variable	followed_by	variable_scope
while_loop	followed_by	stack
get_variable	has_arg0	g_emb
constant	has_arg0	2
get_variable	has_arg0	w_2
get_variable	has_arg0	b
stack_lstm	call	lstm
lstm	call	tanh
_build_sampler	call	stack_lstm
variable_scope	has_arg0	softmax
lstm	call	split
transpose	followed_by	shuffle_batch
to_int32	followed_by	equal
equal	followed_by	to_int32
enas.cifar10.models.Model.build_valid_rl	call	device
enas.cifar10.models.Model.build_valid_rl	call	to_int32
enas.cifar10.models.Model.build_valid_rl	call	_model
enas.cifar10.models.Model.build_valid_rl	call	reduce_sum
enas.cifar10.models.Model.build_valid_rl	call	shuffle_batch
enas.cifar10.models.Model.build_valid_rl	call	equal
device	followed_by	transpose
shuffle_batch	followed_by	map_fn
device	has_arg0	/cpu:0
argmax	has_axis	1
enas.cifar10.models.Model.build_valid_rl	call	transpose
to_int32	followed_by	reduce_sum
map_fn	followed_by	_model
enas.cifar10.models.Model.build_valid_rl	call	map_fn
enas.cifar10.models.Model.build_valid_rl	call	argmax
argmax	followed_by	to_int32
_model	followed_by	argmax
_factorized_reduction	followed_by	_enas_layer
_enas_cell	call	_enas_conv
variable_scope	has_arg0	x_conv
_model	call	average_pooling2d
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	device
_enas_layer	call	stack
_model	call	_get_HW
get_variable	followed_by	get_variable
relu	followed_by	_factorized_reduction
variable_scope	has_arg0	path2_conv
variable_scope	followed_by	_enas_layer
avg_pool	has_arg0	VALID
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	to_int32
variable_scope	has_arg0	x
gather	has_axis	0
reshape	followed_by	create_weight
_model	call	_factorized_reduction
average_pooling2d	has_data_format	self.actual_data_format
_get_C	followed_by	variable_scope
_model	call	_fixed_layer
conv2d	followed_by	batch_norm
Reduction	followed_by	variable_scope
_fixed_conv	followed_by	_get_C
where	call	greater
relu	followed_by	separable_conv2d
variable_scope	followed_by	zeros
_enas_cell	call	_get_C
add_n	followed_by	where
_fixed_layer	call	_get_C
variable_scope	followed_by	relu
variable_scope	followed_by	average_pooling2d
_enas_conv	call	reshape
reshape	followed_by	size
variable_scope	has_arg0	pool_x
conv2d	has_arg0	VALID
one_hot	followed_by	add_n
_enas_layer	call	add_n
_fixed_combine	call	variable_scope
device	has_arg0	/cpu:0
to_int32	followed_by	reshape
_maybe_calibrate_size	followed_by	variable_scope
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	_model
shuffle_batch	followed_by	map_fn
ones	has_dtype	tf.float32
floor	followed_by	div
_enas_conv	call	ones
batch_norm	followed_by	reshape
average_pooling2d	followed_by	max_pooling2d
_enas_conv	call	create_weight
range	followed_by	stack
create_weight	followed_by	reshape
to_int32	followed_by	reduce_sum
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	transpose
drop_path	call	floor
variable_scope	has_arg0	y
max_pooling2d	followed_by	_get_C
variable_scope	followed_by	range
variable_scope	followed_by	variable_scope
gather	followed_by	reshape
control_dependencies	followed_by	identity
drop_path	call	random_uniform
_enas_layer	call	size
_fixed_conv	call	variable_scope
equal	followed_by	to_int32
_get_C	followed_by	average_pooling2d
global_avg_pool	followed_by	create_weight
stack	has_axis	0
_maybe_calibrate_size	call	relu
concat	followed_by	batch_norm
_model	call	matmul
get_variable	has_arg0	offset
global_avg_pool	call	reduce_mean
_maybe_calibrate_size	followed_by	range
relu	followed_by	conv2d
count_model_params	followed_by	relu
batch_norm	followed_by	range
_model	call	conv2d
variable_scope	has_arg0	bn
to_float	followed_by	to_float
average_pooling2d	has_arg0	SAME
_enas_cell	call	average_pooling2d
transpose	followed_by	shuffle_batch
_fixed_layer	followed_by	Reduction
max_pooling2d	followed_by	create_weight
_get_C	followed_by	_factorized_reduction
_apply_drop_path	call	drop_path
reshape	followed_by	relu
_fixed_combine	call	min
batch_norm	call	control_dependencies
ones	followed_by	create_weight
variable_scope	has_arg0	stem_conv
_enas_conv	followed_by	stack
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	equal
_enas_cell	call	create_weight
_enas_conv	call	range
variable_scope	has_arg0	proj
_model	call	range
relu	followed_by	variable_scope
relu	followed_by	global_avg_pool
_model	call	variable_scope
_maybe_calibrate_size	call	_get_HW
_factorized_reduction	call	avg_pool
max_pooling2d	has_arg0	SAME
create_weight	followed_by	conv2d
transpose	followed_by	reshape
_enas_layer	call	relu
_fixed_combine	call	concat
create_weight	followed_by	matmul
variable_scope	followed_by	_fixed_conv
_fixed_conv	call	range
_model	call	count_model_params
relu	call	where
_enas_cell	call	stack
variable_scope	has_arg0	calibrate
create_weight	followed_by	gather
_enas_conv	call	separable_conv2d
_fixed_conv	call	separable_conv2d
create_weight	followed_by	create_weight
one_hot	has_dtype	tf.int32
variable_scope	followed_by	_enas_cell
_apply_drop_path	call	to_float
batch_norm	followed_by	variable_scope
_factorized_reduction	call	pad
gather	followed_by	shape
variable_scope	followed_by	_factorized_reduction
conv2d	followed_by	concat
fused_batch_norm	followed_by	control_dependencies
concat	followed_by	concat
_model	call	create_weight
variable_scope	has_arg0	conv
variable_scope	has_arg0	avg_pool
dropout	followed_by	variable_scope
_fixed_combine	call	_get_HW
_enas_cell	call	variable_scope
get_variable	has_arg0	scale
min	followed_by	variable_scope
batch_norm	call	get_variable
create_weight	call	get_variable
identity	followed_by	fused_batch_norm
variable_scope	followed_by	global_avg_pool
argmax	followed_by	to_int32
_maybe_calibrate_size	call	batch_norm
relu	followed_by	average_pooling2d
greater	has_arg0	0
variable_scope	has_arg0	path1_conv
_enas_cell	call	conv2d
trainable_variables	followed_by	count_model_params
_factorized_reduction	followed_by	concat
_enas_layer	call	shape
_fixed_layer	call	average_pooling2d
_factorized_reduction	call	_get_C
_enas_layer	followed_by	_fixed_layer
_get_C	followed_by	_get_strides
_fixed_conv	call	create_weight
_enas_layer	call	reshape
range	followed_by	variable_scope
_enas_cell	call	relu
map_fn	followed_by	_model
_fixed_layer	call	range
variable_scope	followed_by	create_weight
conv2d	followed_by	pad
random_uniform	followed_by	floor
shape	followed_by	shape
variable_scope	has_arg0	final_combine
_fixed_layer	call	_maybe_calibrate_size
batch_norm	call	variable_scope
_enas_conv	call	variable_scope
_apply_drop_path	call	minimum
_apply_drop_path	followed_by	variable_scope
_maybe_calibrate_size	call	_factorized_reduction
drop_path	call	div
_model	call	global_avg_pool
_fixed_layer	call	_factorized_reduction
_get_C	followed_by	_get_HW
batch_norm	followed_by	_apply_drop_path
_model	call	trainable_variables
_get_strides	followed_by	variable_scope
variable_scope	has_arg0	path_conv
avg_pool	followed_by	variable_scope
_model	followed_by	argmax
_maybe_calibrate_size	call	_get_C
variable_scope	has_arg0	y_conv
where	followed_by	to_int32
variable_scope	followed_by	get_variable
reshape	call	shape
range	followed_by	_get_C
max_pooling2d	has_data_format	self.actual_data_format
_model	call	normal
equal	has_arg0	0
_get_strides	followed_by	avg_pool
_fixed_layer	call	_fixed_conv
batch_norm	followed_by	_enas_conv
_enas_layer	call	_enas_cell
_model	call	Reduction
size	followed_by	stack
_get_C	followed_by	create_weight
_fixed_conv	call	_get_strides
get_variable	has_arg0	moving_mean
conv2d	has_arg0	SAME
reshape	followed_by	variable_scope
_enas_layer	call	transpose
variable_scope	followed_by	_get_HW
_factorized_reduction	call	concat
shape	followed_by	transpose
_fixed_conv	call	relu
zeros	has_dtype	np.int32
_fixed_layer	call	conv2d
variable_scope	has_arg0	layer_base
minimum	followed_by	drop_path
_maybe_calibrate_size	call	conv2d
_model	call	_enas_layer
_apply_drop_path	followed_by	_fixed_combine
one_hot	followed_by	variable_scope
_fixed_layer	call	relu
variable_scope	followed_by	_get_C
_enas_cell	call	batch_norm
to_int32	followed_by	equal
variable_scope	has_arg0	fc
normal	followed_by	_factorized_reduction
_enas_layer	call	one_hot
_fixed_layer	call	_apply_drop_path
_fixed_layer	call	zeros
variable_scope	has_arg0	max_pool
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	reduce_sum
device	followed_by	transpose
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	shuffle_batch
_fixed_layer	call	max_pooling2d
_fixed_layer	call	variable_scope
to_float	followed_by	minimum
_factorized_reduction	followed_by	create_weight
_maybe_calibrate_size	call	variable_scope
_enas_layer	call	to_int32
stack	followed_by	variable_scope
batch_norm	call	identity
_get_HW	followed_by	variable_scope
batch_norm	followed_by	zeros
average_pooling2d	followed_by	variable_scope
separable_conv2d	has_data_format	self.data_format
_model	call	relu
shape	followed_by	random_uniform
stack	followed_by	gather
random_uniform	has_dtype	tf.float32
_fixed_conv	call	_get_C
_enas_cell	followed_by	one_hot
_model	call	dropout
_enas_layer	call	create_weight
_factorized_reduction	call	variable_scope
_model	call	_get_C
batch_norm	followed_by	_get_C
_enas_layer	call	where
zeros	has_dtype	tf.float32
variable_scope	has_arg0	final_conv
average_pooling2d	has_arg0	VALID
where	call	equal
_enas_layer	call	conv2d
_enas_layer	call	batch_norm
_enas_conv	call	fused_batch_norm
_enas_cell	call	reshape
batch_norm	call	fused_batch_norm
matmul	followed_by	trainable_variables
get_variable	has_arg0	moving_variance
separable_conv2d	followed_by	batch_norm
variable_scope	has_arg0	aux_head
variable_scope	followed_by	max_pooling2d
get_variable	followed_by	fused_batch_norm
_get_HW	followed_by	_get_C
_enas_conv	call	_get_C
_factorized_reduction	call	_get_strides
reshape	followed_by	shape
_factorized_reduction	call	conv2d
avg_pool	has_data_format	self.data_format
_enas_layer	call	gather
_enas_layer	call	_maybe_calibrate_size
create_weight	followed_by	relu
_factorized_reduction	call	batch_norm
argmax	has_axis	1
_fixed_conv	call	batch_norm
conv2d	has_data_format	self.data_format
_factorized_reduction	call	create_weight
batch_norm	followed_by	relu
_enas_layer	call	variable_scope
average_pooling2d	followed_by	_get_C
minimum	has_arg0	1.0
_enas_layer	call	range
batch_norm	followed_by	_get_strides
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	map_fn
_enas_conv	call	zeros
zeros	followed_by	range
concat	has_axis	3
zeros	followed_by	ones
_enas_cell	call	max_pooling2d
_enas_conv	call	relu
_fixed_layer	call	batch_norm
_fixed_combine	call	_factorized_reduction
_get_HW	followed_by	create_weight
_enas_conv	followed_by	_enas_conv
variable_scope	has_arg0	pool_y
drop_path	call	shape
_maybe_calibrate_size	call	create_weight
_fixed_layer	followed_by	normal
_fixed_layer	call	_fixed_combine
pad	followed_by	avg_pool
global_avg_pool	followed_by	dropout
concat	has_axis	1
pad	followed_by	pad
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	argmax
separable_conv2d	followed_by	fused_batch_norm
_factorized_reduction	followed_by	variable_scope
_fixed_layer	call	create_weight
reduce_mean	followed_by	reduce_mean
_model	call	batch_norm
to_int32	followed_by	reduce_sum
enas.cifar10.models.Model._build_test	call	to_int32
_model	followed_by	argmax
equal	followed_by	to_int32
enas.cifar10.models.Model._build_test	call	reduce_sum
enas.cifar10.models.Model._build_test	call	argmax
argmax	has_axis	1
enas.cifar10.models.Model._build_test	call	_model
enas.cifar10.models.Model._build_test	call	equal
argmax	followed_by	to_int32
to_int32	followed_by	equal
split	followed_by	split
.process	call	main
split	followed_by	size
size	followed_by	size
main	call	size
main	call	split
split	has_arg0	 
enas.cifar10.models.Model._build_valid	call	to_int32
argmax	has_axis	1
enas.cifar10.models.Model._build_valid	call	argmax
_model	followed_by	argmax
to_int32	followed_by	equal
enas.cifar10.models.Model._build_valid	call	reduce_sum
enas.cifar10.models.Model._build_valid	call	equal
equal	followed_by	to_int32
enas.cifar10.models.Model._build_valid	call	_model
argmax	followed_by	to_int32
to_int32	followed_by	reduce_sum
variable_scope	followed_by	create_weight
enas.cifar10.image_ops.fully_connected	call	variable_scope
create_weight	call	get_variable
enas.cifar10.image_ops.fully_connected	call	create_weight
enas.cifar10.image_ops.fully_connected	call	matmul
create_weight	followed_by	matmul
