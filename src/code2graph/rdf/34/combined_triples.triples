.enas.ptb.main	call	DEFINE_string
DEFINE_integer	followed_by	DEFINE_integer
DEFINE_integer	followed_by	DEFINE_boolean
DEFINE_integer	call	DEFINE_integer
DEFINE_float	followed_by	DEFINE_boolean
DEFINE_boolean	followed_by	DEFINE_string
DEFINE_string	followed_by	DEFINE_integer
.enas.ptb.main	call	run
DEFINE_boolean	followed_by	DEFINE_boolean
DEFINE_boolean	call	DEFINE_boolean
DEFINE_float	call	DEFINE_float
DEFINE_integer	followed_by	DEFINE_string
DEFINE_float	followed_by	DEFINE_integer
DEFINE_boolean	followed_by	DEFINE_float
DEFINE_boolean	followed_by	DEFINE_integer
DEFINE_string	call	DEFINE_string
DEFINE_integer	followed_by	DEFINE_float
.enas.ptb.main	call	DEFINE_integer
DEFINE_float	followed_by	DEFINE_float
DEFINE_string	followed_by	DEFINE_boolean
.enas.ptb.main	call	DEFINE_float
DEFINE_string	followed_by	DEFINE_string
DEFINE_integer	followed_by	run
.enas.ptb.main	call	DEFINE_boolean
.enas.cifar10.main	call	DEFINE_string
DEFINE_boolean	followed_by	DEFINE_string
DEFINE_boolean	followed_by	DEFINE_boolean
DEFINE_boolean	followed_by	DEFINE_float
DEFINE_string	call	DEFINE_string
DEFINE_integer	followed_by	DEFINE_float
DEFINE_boolean	call	DEFINE_boolean
DEFINE_float	followed_by	DEFINE_integer
DEFINE_integer	call	DEFINE_integer
DEFINE_string	followed_by	DEFINE_boolean
.enas.cifar10.main	call	run
DEFINE_string	followed_by	DEFINE_integer
DEFINE_boolean	followed_by	DEFINE_integer
.enas.cifar10.main	call	DEFINE_integer
DEFINE_string	followed_by	DEFINE_string
.enas.cifar10.main	call	DEFINE_boolean
DEFINE_float	call	DEFINE_float
DEFINE_integer	followed_by	DEFINE_boolean
DEFINE_integer	followed_by	DEFINE_integer
DEFINE_float	followed_by	DEFINE_string
DEFINE_float	followed_by	DEFINE_float
.enas.cifar10.main	call	DEFINE_float
DEFINE_integer	followed_by	run
Variable	has_name	last_reset
variable_scope	has_arg0	proj
average_pooling2d	has_data_format	self.actual_data_format
concat	has_axis	3
_fixed_conv	call	range
variable_scope	has_arg0	path2_conv
_fixed_conv	call	separable_conv2d
batch_norm	followed_by	range
count_model_params	followed_by	relu
variable_scope	has_arg0	avg_pool
_build_train	followed_by	_build_valid
drop_path	call	random_uniform
Variable	followed_by	Variable
reshape	followed_by	variable_scope
global_avg_pool	followed_by	dropout
_enas_conv	call	reshape
exponential_decay	call	maximum
conv2d	followed_by	batch_norm
_build_valid	call	argmax
_fixed_combine	call	_factorized_reduction
sqrt	followed_by	clip_by_global_norm
_apply_drop_path	call	drop_path
_factorized_reduction	call	variable_scope
one_hot	followed_by	variable_scope
Variable	followed_by	cond
_model	call	trainable_variables
relu	followed_by	average_pooling2d
drop_path	call	div
_build_train	call	reduce_mean
_get_strides	followed_by	avg_pool
_maybe_calibrate_size	call	batch_norm
cond	followed_by	MomentumOptimizer
create_weight	followed_by	conv2d
relu	followed_by	conv2d
create_weight	followed_by	relu
_model	call	_enas_layer
_enas_layer	call	shape
sparse_softmax_cross_entropy_with_logits	has_labels	self.y_train
batch_norm	followed_by	_apply_drop_path
_get_C	followed_by	variable_scope
variable_scope	followed_by	average_pooling2d
relu	followed_by	variable_scope
variable_scope	followed_by	_get_HW
_enas_layer	call	size
_apply_drop_path	call	to_float
_enas_layer	call	add_n
get_variable	has_arg0	scale
batch_norm	followed_by	variable_scope
_model	call	_fixed_layer
floor	followed_by	div
average_pooling2d	has_arg0	VALID
Variable	has_arg0	0
reduce_sum	followed_by	trainable_variables
_enas_layer	call	_enas_cell
add_n	followed_by	gradients
_fixed_layer	call	range
batch_norm	followed_by	_enas_conv
reshape	call	shape
_fixed_conv	call	variable_scope
minimum	followed_by	drop_path
variable_scope	followed_by	relu
cond	call	greater_equal
_enas_cell	call	conv2d
zeros	has_dtype	np.int32
min	followed_by	variable_scope
get_train_ops	call	sqrt
global_avg_pool	call	reduce_mean
_factorized_reduction	call	pad
batch_norm	call	identity
_fixed_layer	call	_fixed_conv
create_weight	call	get_variable
_model	call	normal
pad	followed_by	pad
dropout	followed_by	variable_scope
_enas_cell	call	average_pooling2d
ones	has_dtype	tf.float32
_maybe_calibrate_size	call	_factorized_reduction
_get_C	followed_by	_factorized_reduction
batch_norm	call	variable_scope
get_train_ops	call	SyncReplicasOptimizer
cond	followed_by	exponential_decay
maximum	has_arg0	0
_fixed_layer	call	_get_C
separable_conv2d	followed_by	batch_norm
_model	followed_by	sparse_softmax_cross_entropy_with_logits
batch_norm	followed_by	zeros
_build_valid	followed_by	_build_test
_fixed_layer	call	max_pooling2d
reduce_mean	followed_by	reduce_mean
_model	call	_factorized_reduction
concat	followed_by	concat
_factorized_reduction	call	concat
batch_norm	call	fused_batch_norm
_build_train	call	argmax
to_float	followed_by	minimum
argmax	followed_by	to_int32
_fixed_combine	call	variable_scope
_enas_cell	call	_get_C
cond	call	less
random_uniform	has_dtype	tf.float32
variable_scope	followed_by	_get_C
_model	call	average_pooling2d
avg_pool	followed_by	variable_scope
enas.cifar10.micro_child.MicroChild.connect_controller	call	_build_train
_build_train	call	_model
sqrt	call	reduce_sum
_build_train	call	trainable_variables
_build_valid	call	equal
get_train_ops	call	Variable
get_train_ops	call	exponential_decay
relu	followed_by	global_avg_pool
where	followed_by	to_int32
reshape	followed_by	size
argmax	has_axis	1
_enas_layer	call	one_hot
conv2d	followed_by	pad
_model	call	conv2d
_enas_layer	call	variable_scope
_maybe_calibrate_size	call	variable_scope
_fixed_layer	call	relu
add_n	followed_by	where
equal	has_arg0	0
_fixed_layer	call	variable_scope
_fixed_conv	followed_by	_get_C
_get_HW	followed_by	_get_C
greater	has_arg0	0
_enas_layer	call	conv2d
variable_scope	followed_by	range
get_train_ops	call	maximum
_model	call	variable_scope
_enas_conv	call	create_weight
zeros	followed_by	range
variable_scope	has_arg0	y_conv
_enas_cell	call	variable_scope
_enas_conv	call	fused_batch_norm
_model	call	matmul
exponential_decay	followed_by	maximum
_enas_conv	call	ones
_model	call	Reduction
_maybe_calibrate_size	call	relu
range	followed_by	variable_scope
IndexedSlices	followed_by	clip_by_norm
batch_norm	call	control_dependencies
_build_test	call	_model
variable_scope	has_arg0	path1_conv
average_pooling2d	has_arg0	SAME
shape	followed_by	random_uniform
zeros	has_dtype	tf.float32
_factorized_reduction	followed_by	create_weight
get_variable	has_arg0	moving_variance
_enas_conv	call	variable_scope
drop_path	call	floor
average_pooling2d	followed_by	_get_C
_factorized_reduction	call	_get_C
_fixed_layer	followed_by	Reduction
_factorized_reduction	call	_get_strides
relu	call	where
_fixed_combine	call	concat
gather	followed_by	reshape
variable_scope	has_arg0	stem_conv
_maybe_calibrate_size	call	_get_C
relu	followed_by	separable_conv2d
concat	has_axis	1
variable_scope	followed_by	_factorized_reduction
_maybe_calibrate_size	followed_by	variable_scope
_fixed_layer	call	_factorized_reduction
Reduction	followed_by	variable_scope
_fixed_conv	call	create_weight
_model	call	_get_C
_enas_layer	followed_by	_fixed_layer
get_variable	followed_by	fused_batch_norm
_model	call	count_model_params
_enas_conv	call	zeros
get_train_ops	call	GradientDescentOptimizer
create_weight	followed_by	reshape
variable_scope	followed_by	global_avg_pool
concat	followed_by	batch_norm
_factorized_reduction	call	avg_pool
_fixed_combine	call	min
_model	followed_by	argmax
global_norm	followed_by	sqrt
where	call	equal
minimum	has_arg0	1.0
matmul	followed_by	trainable_variables
shape	followed_by	shape
_get_strides	followed_by	variable_scope
batch_norm	followed_by	_get_C
MomentumOptimizer	has_arg0	0.9
_enas_layer	call	_maybe_calibrate_size
_factorized_reduction	followed_by	concat
ones	followed_by	create_weight
shape	followed_by	transpose
create_weight	followed_by	matmul
_build_train	call	sparse_softmax_cross_entropy_with_logits
variable_scope	has_arg0	conv
_enas_layer	call	reshape
variable_scope	followed_by	get_variable
_enas_cell	call	batch_norm
trainable_variables	followed_by	count_model_params
average_pooling2d	followed_by	variable_scope
pad	followed_by	avg_pool
max_pooling2d	has_data_format	self.actual_data_format
_model	call	batch_norm
stack	followed_by	gather
reshape	followed_by	create_weight
_model	call	range
enas.cifar10.micro_child.MicroChild.connect_controller	call	_build_test
reshape	followed_by	relu
GradientDescentOptimizer	followed_by	AdamOptimizer
max_pooling2d	followed_by	_get_C
_enas_cell	call	reshape
drop_path	call	shape
get_variable	has_arg0	offset
get_train_ops	call	add_n
_fixed_layer	followed_by	normal
_factorized_reduction	followed_by	_enas_layer
max_pooling2d	followed_by	create_weight
reduce_sum	followed_by	add_n
_enas_layer	call	to_int32
_build_valid	call	_model
equal	followed_by	to_int32
variable_scope	has_arg0	max_pool
variable_scope	followed_by	zeros
one_hot	followed_by	add_n
gradients	followed_by	global_norm
_get_C	followed_by	_get_strides
variable_scope	has_arg0	bn
to_int32	followed_by	reduce_sum
separable_conv2d	followed_by	fused_batch_norm
range	followed_by	stack
fused_batch_norm	followed_by	control_dependencies
get_variable	followed_by	get_variable
variable_scope	has_arg0	layer_base
_get_C	followed_by	_get_HW
identity	followed_by	fused_batch_norm
get_train_ops	call	reduce_sum
_build_test	call	equal
_enas_conv	followed_by	stack
_fixed_conv	call	relu
_enas_conv	followed_by	_enas_conv
stack	has_axis	0
_get_C	followed_by	average_pooling2d
variable_scope	followed_by	variable_scope
_fixed_layer	call	batch_norm
size	followed_by	stack
batch_norm	followed_by	relu
to_int32	followed_by	equal
_get_C	followed_by	create_weight
_enas_layer	call	transpose
_enas_layer	call	batch_norm
AdamOptimizer	followed_by	SyncReplicasOptimizer
_fixed_layer	call	_apply_drop_path
_enas_conv	call	separable_conv2d
_fixed_combine	call	_get_HW
variable_scope	has_arg0	pool_y
_enas_layer	call	range
gather	has_axis	0
variable_scope	has_arg0	x_conv
_build_test	call	reduce_sum
_build_train	call	equal
get_train_ops	call	clip_by_norm
_factorized_reduction	followed_by	variable_scope
_fixed_layer	call	_maybe_calibrate_size
_enas_conv	call	range
_build_test	call	to_int32
enas.cifar10.micro_child.MicroChild.connect_controller	call	_build_valid
_apply_drop_path	followed_by	_fixed_combine
normal	followed_by	_factorized_reduction
_fixed_conv	call	batch_norm
_maybe_calibrate_size	call	create_weight
_build_valid	call	to_int32
_get_HW	followed_by	variable_scope
get_train_ops	call	global_norm
_build_valid	call	reduce_sum
get_train_ops	call	IndexedSlices
_enas_layer	call	create_weight
_factorized_reduction	call	create_weight
where	call	greater
global_avg_pool	followed_by	create_weight
get_train_ops	call	cond
variable_scope	has_arg0	fc
_maybe_calibrate_size	call	_get_HW
avg_pool	has_arg0	VALID
create_weight	followed_by	create_weight
variable_scope	followed_by	_enas_layer
_build_train	call	reduce_sum
relu	followed_by	_factorized_reduction
conv2d	followed_by	concat
_fixed_conv	call	_get_strides
_model	call	global_avg_pool
_model	call	create_weight
MomentumOptimizer	followed_by	GradientDescentOptimizer
variable_scope	has_arg0	calibrate
_fixed_layer	call	zeros
_fixed_conv	call	_get_C
control_dependencies	followed_by	identity
variable_scope	followed_by	max_pooling2d
batch_norm	followed_by	_get_strides
variable_scope	has_arg0	x
gather	followed_by	shape
reshape	followed_by	shape
get_train_ops	call	MomentumOptimizer
_enas_cell	followed_by	one_hot
_build_train	call	get_train_ops
range	followed_by	_get_C
_maybe_calibrate_size	followed_by	range
zeros	followed_by	ones
_model	call	_get_HW
clip_by_global_norm	followed_by	clip_by_norm
sparse_softmax_cross_entropy_with_logits	followed_by	reduce_mean
_enas_conv	call	_get_C
_factorized_reduction	call	conv2d
get_train_ops	call	AdamOptimizer
conv2d	has_data_format	self.data_format
get_variable	has_arg0	moving_mean
one_hot	has_dtype	tf.int32
avg_pool	has_data_format	self.data_format
variable_scope	followed_by	_fixed_conv
get_train_ops	call	gradients
batch_norm	followed_by	reshape
get_train_ops	call	clip_by_global_norm
variable_scope	has_arg0	pool_x
reduce_mean	followed_by	sparse_softmax_cross_entropy_with_logits
_enas_cell	call	max_pooling2d
maximum	followed_by	cond
reduce_mean	followed_by	argmax
variable_scope	followed_by	_enas_cell
_model	call	relu
_maybe_calibrate_size	call	conv2d
variable_scope	has_arg0	y
variable_scope	has_arg0	final_conv
sqrt	followed_by	sqrt
Variable	has_name	T_i
_build_train	call	count_model_params
conv2d	has_arg0	SAME
_enas_layer	call	relu
batch_norm	call	get_variable
variable_scope	has_arg0	path_conv
_build_train	call	to_int32
to_int32	followed_by	reshape
random_uniform	followed_by	floor
_apply_drop_path	followed_by	variable_scope
_fixed_layer	call	conv2d
_get_HW	followed_by	create_weight
variable_scope	has_arg0	aux_head
count_model_params	followed_by	get_train_ops
_enas_cell	call	_enas_conv
clip_by_norm	followed_by	IndexedSlices
_model	call	dropout
_enas_cell	call	stack
_enas_cell	call	create_weight
_fixed_layer	call	average_pooling2d
average_pooling2d	followed_by	max_pooling2d
_enas_conv	call	relu
_enas_layer	call	where
max_pooling2d	has_arg0	SAME
_enas_cell	call	relu
_build_test	call	argmax
_apply_drop_path	call	minimum
conv2d	has_arg0	VALID
variable_scope	followed_by	create_weight
stack	followed_by	variable_scope
transpose	followed_by	reshape
_fixed_layer	call	_fixed_combine
_fixed_layer	call	create_weight
separable_conv2d	has_data_format	self.data_format
clip_by_norm	followed_by	Variable
_enas_layer	call	gather
variable_scope	has_arg0	final_combine
to_float	followed_by	to_float
_enas_layer	call	stack
create_weight	followed_by	gather
_factorized_reduction	call	batch_norm
argmax	has_axis	1
enas.cifar10.models.Model._build_valid	call	argmax
argmax	followed_by	to_int32
enas.cifar10.models.Model._build_valid	call	equal
to_int32	followed_by	equal
equal	followed_by	to_int32
enas.cifar10.models.Model._build_valid	call	to_int32
enas.cifar10.models.Model._build_valid	call	reduce_sum
to_int32	followed_by	reduce_sum
_model	followed_by	argmax
enas.cifar10.models.Model._build_valid	call	_model
get_train_ops	call	clip_by_global_norm
multinomial	has_arg0	1
build_trainer	followed_by	connect_controller
_model	call	zeros_like
_build_valid	followed_by	_build_valid_rl
exponential_decay	call	maximum
_model	call	constant
variable_scope	has_arg0	rnn
Variable	followed_by	Variable
count_model_params	followed_by	reduce_sum
cond	followed_by	MomentumOptimizer
concat	followed_by	concat
_gen_mask	followed_by	constant
variable_scope	followed_by	variable_scope
_model	call	assign
random_uniform	followed_by	floor
multinomial	followed_by	to_int32
tanh	followed_by	to_float
_create_params	call	get_variable
_build_valid_rl	call	reduce_mean
_build_valid	call	_get_log_probs
concat	followed_by	reduce_sum
embedding_lookup	followed_by	_gen_mask
get_variable	has_arg0	w_prev
maximum	followed_by	cond
_model	call	where
_build_sampler	call	stop_gradient
stack	followed_by	reduce_sum
reduce_sum	followed_by	to_float
ptb_input_producer	call	reshape
run	followed_by	run
_build_test	call	_get_log_probs
_build_params	followed_by	_build_train
variable_scope	followed_by	get_variable
train	call	Saver
_model	call	transpose
ptb_input_producer	followed_by	reshape
_get_log_probs	followed_by	reduce_sum
_build_sampler	call	embedding_lookup
_gen_mask	call	floor
print_user_flags	followed_by	train
_build_sampler	call	tanh
range	followed_by	logical_and
random_uniform	has_dtype	tf.int32
_build_sampler	call	stack_lstm
cond	followed_by	exponential_decay
_build_train	call	_model
_create_params	followed_by	_build_sampler
build_trainer	call	to_float
connect_controller	followed_by	build_trainer
_build_sampler	call	matmul
to_float	followed_by	stop_gradient
get_ops	call	PTBEnasController.__init__
train	call	load
sparse_softmax_cross_entropy_with_logits	followed_by	exp
_model	call	while_loop
identity	followed_by	Variable
split	followed_by	sigmoid
ptb_input_producer	call	reduce_sum
stack_lstm	followed_by	matmul
Variable	has_name	last_reset
split	has_arg0	4
_create_params	call	variable_scope
zeros	followed_by	range
_gen_mask	followed_by	zeros_like
Variable	has_name	train_step
CheckpointSaverHook	followed_by	SingularMonitoredSession
to_float	call	range
sigmoid	followed_by	tanh
variable_scope	has_arg0	softmax
ptb_input_producer	call	size
convert_to_tensor	followed_by	size
reshape	followed_by	sparse_softmax_cross_entropy_with_logits
assign	followed_by	assign
_build_valid_rl	call	stop_gradient
Saver	followed_by	CheckpointSaverHook
GradientDescentOptimizer	followed_by	AdamOptimizer
variable_scope	followed_by	range
range	has_arg0	0
reshape	followed_by	ptb_input_producer
_build_params	call	zeros
Variable	followed_by	trainable_variables
Saver	has_max_to_keep	10
constant	has_arg0	0
MomentumOptimizer	has_arg0	0.9
reshape	followed_by	tanh
_build_valid_rl	call	_get_log_probs
build_trainer	call	reduce_sum
to_int32	followed_by	reshape
exp	followed_by	trainable_variables
concat	followed_by	tanh
reshape	followed_by	range
build_trainer	call	assign_sub
get_ops	call	PTBEnasChild.__init__
global_norm	followed_by	sqrt
Graph	followed_by	get_ops
get_variable	has_arg0	w_1
strided_slice	followed_by	strided_slice
concat	followed_by	where
cond	call	greater_equal
get_train_ops	call	reduce_sum
range	followed_by	_gen_mask
transpose	followed_by	reshape
train	call	Graph
zeros_like	followed_by	ones_like
IndexedSlices	followed_by	clip_by_norm
load	followed_by	size
PTBEnasController.__init__	call	_build_sampler
sqrt	call	reduce_sum
_model	call	range
enas.ptb.main.main	call	Logger.__init__
clip_by_global_norm	followed_by	clip_by_norm
while_loop	followed_by	stack
reduce_mean	followed_by	stop_gradient
range	followed_by	Variable
_get_log_probs	call	sparse_softmax_cross_entropy_with_logits
PTBEnasController.__init__	call	_create_params
_model	call	concat
reshape	followed_by	assign
device	followed_by	identity
get_train_ops	call	maximum
_build_train	call	exp
_gen_mask	followed_by	_gen_mask
_build_sampler	call	to_int32
_build_valid	call	reduce_sum
_build_params	call	variable_scope
connect_controller	call	_build_valid
_build_sampler	call	concat
logical_and	followed_by	reduce_any
PTBEnasChild.__init__	call	reshape
tanh	followed_by	matmul
variable_scope	followed_by	zeros
ptb_input_producer	call	identity
add_n	followed_by	gradients
get_train_ops	call	GradientDescentOptimizer
_build_train	followed_by	_build_valid
connect_controller	call	_build_test
Variable	has_arg0	0.0
_model	call	stack
build_trainer	call	get_train_ops
equal	has_arg0	0
SingularMonitoredSession	has_checkpoint_dir	FLAGS.output_dir
matmul	followed_by	tanh
concat	followed_by	exp
exponential_decay	followed_by	maximum
PTBEnasChild.__init__	followed_by	SGD
logical_and	call	equal
where	call	zeros_like
PTBEnasChild.__init__	call	ptb_input_producer
range	has_arg0	1
zeros	has_dtype	np.float32
assign	call	stop_gradient
maximum	has_arg0	0
clip_by_norm	followed_by	Variable
concat	has_axis	0
reduce_sum	followed_by	strided_slice
build_trainer	call	control_dependencies
zeros	has_dtype	tf.float32
get_variable	followed_by	get_variable
connect_controller	call	_build_params
to_float	followed_by	stack_lstm
Variable	has_name	global_step
get_train_ops	call	clip_by_norm
size	followed_by	reshape
train	call	size
_build_valid_rl	call	_model
SingularMonitoredSession	followed_by	run
exp	followed_by	concat
ptb_input_producer	call	strided_slice
_build_train	call	trainable_variables
reduce_sum	followed_by	Variable
build_trainer	call	identity
sqrt	followed_by	clip_by_global_norm
_model	call	TensorArray
convert_to_tensor	has_dtype	tf.int32
_model	call	reduce_any
_build_test	call	_model
_build_sampler	call	to_float
identity	has_name	epoch_size
train	followed_by	train
variable_scope	has_arg0	embedding
ptb_input_producer	call	convert_to_tensor
_gen_mask	call	random_uniform
_build_params	call	range
_build_valid	call	stop_gradient
clip_by_norm	followed_by	IndexedSlices
reshape	followed_by	device
ones_like	followed_by	constant
to_float	followed_by	Variable
get_ops	call	connect_controller
get_ops	call	SGD
_build_sampler	call	multinomial
to_float	followed_by	reshape
size	followed_by	size
get_ops	call	Adam
build_trainer	call	Variable
get_variable	followed_by	range
_get_log_probs	followed_by	reduce_mean
get_train_ops	call	exponential_decay
stop_gradient	followed_by	exp
get_train_ops	call	add_n
_build_sampler	call	reshape
size	followed_by	convert_to_tensor
identity	followed_by	random_uniform
get_variable	has_arg0	w
reduce_sum	followed_by	add_n
sqrt	followed_by	sqrt
assign_sub	followed_by	control_dependencies
_build_train	call	_get_log_probs
_model	call	reshape
get_variable	has_arg0	g_emb
_model	call	control_dependencies
_model	call	_gen_mask
cond	call	less
SGD	followed_by	PTBEnasController.__init__
zeros	followed_by	stack_lstm
constant	has_dtype	tf.bool
tanh	followed_by	tanh
assign	followed_by	control_dependencies
equal	followed_by	equal
stack_lstm	call	lstm
_model	call	embedding_lookup
constant	followed_by	while_loop
_model	call	ones_like
matmul	followed_by	sparse_softmax_cross_entropy_with_logits
_build_train	call	Variable
get_train_ops	call	IndexedSlices
_build_test	call	stop_gradient
matmul	followed_by	concat
embedding_lookup	call	concat
embedding_lookup	followed_by	to_float
matmul	followed_by	matmul
reshape	followed_by	multinomial
connect_controller	call	_build_train
Variable	followed_by	assign_sub
get_variable	followed_by	variable_scope
get_train_ops	call	AdamOptimizer
trainable_variables	followed_by	count_model_params
_build_sampler	call	reduce_sum
get_train_ops	call	MomentumOptimizer
Variable	has_name	T_i
PTBEnasController.__init__	followed_by	Adam
AdamOptimizer	followed_by	SyncReplicasOptimizer
_get_log_probs	call	matmul
enas.ptb.main.main	call	print_user_flags
variable_scope	has_arg0	attention
train	call	get_ops
Adam	followed_by	connect_controller
tanh	followed_by	multinomial
lstm	call	sigmoid
gradients	followed_by	global_norm
train	call	CheckpointSaverHook
Logger.__init__	followed_by	print_user_flags
MomentumOptimizer	followed_by	GradientDescentOptimizer
build_trainer	call	stop_gradient
_build_params	call	get_variable
variable_scope	has_arg0	lstm
exp	followed_by	stop_gradient
TensorArray	followed_by	embedding_lookup
to_float	followed_by	to_float
concat	has_axis	1
get_ops	followed_by	Saver
stop_gradient	followed_by	embedding_lookup
build_trainer	call	trainable_variables
zeros	followed_by	zeros
random_uniform	followed_by	reduce_sum
_build_sampler	call	zeros
ptb_input_producer	call	random_uniform
exp	followed_by	reduce_sum
_build_sampler	call	exp
reduce_any	followed_by	concat
split	has_axis	1
get_train_ops	call	cond
matmul	call	concat
embedding_lookup	followed_by	concat
trainable_variables	followed_by	get_train_ops
random_uniform	has_dtype	tf.float32
_build_test	call	reduce_sum
to_float	followed_by	exp
sigmoid	followed_by	sigmoid
constant	followed_by	range
enas.ptb.main.main	call	train
matmul	followed_by	split
where	followed_by	reshape
_model	followed_by	stop_gradient
_build_valid	call	_model
assign	call	zeros_like
_model	call	logical_and
stop_gradient	followed_by	_get_log_probs
Variable	followed_by	get_train_ops
Variable	followed_by	cond
matmul	followed_by	reshape
get_variable	has_arg0	v
lstm	call	split
_build_train	call	get_train_ops
Variable	has_arg0	0
_build_valid_rl	followed_by	_build_test
exp	call	reduce_mean
_build_train	call	reduce_sum
build_trainer	call	exp
lstm	call	tanh
variable_scope	has_arg0	starting_states
_model	call	identity
size	followed_by	Graph
get_train_ops	call	SyncReplicasOptimizer
get_variable	has_arg0	w_2
_model	call	reduce_sum
reduce_sum	followed_by	transpose
get_ops	call	build_trainer
connect_controller	call	_build_valid_rl
_build_sampler	call	sparse_softmax_cross_entropy_with_logits
control_dependencies	followed_by	identity
ptb_input_producer	call	device
get_train_ops	call	sqrt
get_train_ops	call	global_norm
_build_params	call	Variable
get_train_ops	call	gradients
_model	followed_by	_get_log_probs
_build_train	call	to_float
lstm	call	matmul
device	has_arg0	/cpu:0
_build_train	call	count_model_params
get_train_ops	call	Variable
train	call	SingularMonitoredSession
constant	has_dtype	tf.int32
train	call	run
range	followed_by	variable_scope
MomentumOptimizer	has_arg0	0.9
to_int32	followed_by	equal
reduce_sum	followed_by	add_n
clip_by_norm	followed_by	IndexedSlices
IndexedSlices	followed_by	clip_by_norm
get_train_ops	call	SyncReplicasOptimizer
maximum	followed_by	cond
get_train_ops	call	IndexedSlices
trainable_variables	followed_by	count_model_params
get_train_ops	call	AdamOptimizer
sparse_softmax_cross_entropy_with_logits	followed_by	reduce_mean
AdamOptimizer	followed_by	SyncReplicasOptimizer
get_train_ops	call	cond
sqrt	followed_by	sqrt
get_train_ops	call	reduce_sum
enas.cifar10.models.Model._build_train	call	equal
get_train_ops	call	gradients
enas.cifar10.models.Model._build_train	call	reduce_sum
sqrt	followed_by	clip_by_global_norm
clip_by_norm	followed_by	Variable
cond	followed_by	exponential_decay
get_train_ops	call	GradientDescentOptimizer
enas.cifar10.models.Model._build_train	call	get_train_ops
get_train_ops	call	exponential_decay
Variable	has_name	T_i
GradientDescentOptimizer	followed_by	AdamOptimizer
Variable	followed_by	Variable
exponential_decay	followed_by	maximum
argmax	has_axis	1
get_train_ops	call	sqrt
enas.cifar10.models.Model._build_train	call	sparse_softmax_cross_entropy_with_logits
sqrt	call	reduce_sum
cond	followed_by	MomentumOptimizer
enas.cifar10.models.Model._build_train	call	count_model_params
maximum	has_arg0	0
to_int32	followed_by	reduce_sum
global_norm	followed_by	sqrt
get_train_ops	call	maximum
get_train_ops	call	clip_by_norm
reduce_sum	followed_by	trainable_variables
enas.cifar10.models.Model._build_train	call	to_int32
Variable	has_arg0	0
Variable	followed_by	cond
reduce_mean	followed_by	argmax
exponential_decay	call	maximum
get_train_ops	call	MomentumOptimizer
count_model_params	followed_by	Variable
enas.cifar10.models.Model._build_train	call	argmax
enas.cifar10.models.Model._build_train	call	trainable_variables
enas.cifar10.models.Model._build_train	call	_model
_model	followed_by	sparse_softmax_cross_entropy_with_logits
Variable	followed_by	get_train_ops
add_n	followed_by	gradients
equal	followed_by	to_int32
argmax	followed_by	to_int32
Variable	has_name	last_reset
Variable	has_name	global_step
clip_by_global_norm	followed_by	clip_by_norm
get_train_ops	call	Variable
cond	call	less
sparse_softmax_cross_entropy_with_logits	has_labels	self.y_train
get_train_ops	call	clip_by_global_norm
enas.cifar10.models.Model._build_train	call	Variable
MomentumOptimizer	followed_by	GradientDescentOptimizer
get_train_ops	call	global_norm
cond	call	greater_equal
enas.cifar10.models.Model._build_train	call	reduce_mean
get_train_ops	call	add_n
gradients	followed_by	global_norm
Saver	has_max_to_keep	2
variable_scope	has_arg0	inp_conv_1
pad	followed_by	pad
_read_data	followed_by	mean
_pool_branch	call	conv2d
pad	followed_by	avg_pool
_enas_layer	call	cond
avg_pool	followed_by	variable_scope
_conv_branch	call	separable_conv2d
batch_norm_with_mask	call	scatter_sub
_pool_branch	call	batch_norm
get_ops	call	connect_controller
variable_scope	has_arg0	branch_2
_build_test	call	reduce_sum
argmax	followed_by	to_int32
_read_data	followed_by	_read_data
logical_or	followed_by	boolean_mask
variable_scope	has_arg0	stem_conv
_build_valid	call	_model
conv2d	followed_by	range
_fixed_layer	call	conv2d
variable_scope	has_arg0	fc
variable_scope	has_arg0	conv_1x1
to_float	followed_by	reduce_sum
variable_scope	has_arg0	branch_0
Variable	has_name	last_reset
build_trainer	call	control_dependencies
concat	followed_by	batch_norm
_fixed_layer	call	batch_norm
train	call	Graph
_build_train	call	to_int32
_pool_branch	followed_by	equal
fused_batch_norm	followed_by	control_dependencies
_model	call	range
CheckpointSaverHook	followed_by	ConfigProto
_conv_branch	call	logical_and
_fixed_layer	call	relu
create_weight	followed_by	constant
reduce_sum	followed_by	trainable_variables
get_variable	followed_by	fused_batch_norm
build_trainer	call	reduce_sum
_model	call	variable_scope
batch_norm	followed_by	variable_scope
variable_scope	has_arg0	path_conv
relu	followed_by	conv2d
build_trainer	call	to_float
variable_scope	has_arg0	conv_1
_fixed_layer	call	_conv_branch
conv2d	has_arg0	SAME
equal	followed_by	variable_scope
batch_norm	followed_by	range
exponential_decay	call	maximum
exponential_decay	followed_by	maximum
cond	call	zeros_like
clip_by_global_norm	followed_by	clip_by_norm
_build_train	call	sparse_softmax_cross_entropy_with_logits
MomentumOptimizer	followed_by	GradientDescentOptimizer
_conv_branch	followed_by	equal
batch_norm	call	identity
batch_norm_with_mask	followed_by	relu
boolean_mask	followed_by	boolean_mask
_factorized_reduction	call	pad
batch_norm	followed_by	create_weight
batch_norm_with_mask	call	get_variable
_build_train	call	Variable
sqrt	call	reduce_sum
add_n	followed_by	batch_norm
create_weight	followed_by	concat
logical_and	followed_by	batch_norm_with_mask
batch_norm_with_mask	call	variable_scope
build_trainer	call	trainable_variables
variable_scope	has_arg0	pool
_factorized_reduction	call	batch_norm
range	followed_by	range
build_trainer	followed_by	connect_controller
GradientDescentOptimizer	followed_by	AdamOptimizer
Logger.__init__	followed_by	print_user_flags
Variable	has_arg0	0
Variable	has_name	train_step
_conv_branch	call	reshape
create_weight	followed_by	matmul
_conv_branch	call	batch_norm_with_mask
connect_controller	followed_by	build_trainer
read_data	followed_by	Graph
connect_controller	call	_build_valid
_build_valid	followed_by	_build_test
_enas_layer	call	add_n
separable_conv2d	followed_by	range
_enas_layer	call	shape
equal	has_arg0	2
equal	has_arg0	3
clip_by_norm	followed_by	IndexedSlices
batch_norm_with_mask	call	to_int32
batch_norm	call	variable_scope
_factorized_reduction	call	conv2d
Variable	followed_by	trainable_variables
maximum	followed_by	cond
variable_scope	followed_by	_get_C
batch_norm	call	control_dependencies
where	call	greater
_build_train	call	equal
variable_scope	has_arg0	path2_conv
build_trainer	call	Variable
get_train_ops	call	sqrt
variable_scope	has_arg0	branch_3
cond	followed_by	exponential_decay
SingularMonitoredSession	has_checkpoint_dir	FLAGS.output_dir
variable_scope	followed_by	_enas_layer
avg_pool	has_arg0	VALID
Graph	followed_by	get_ops
Variable	has_name	global_step
global_avg_pool	call	reduce_mean
reshape	followed_by	separable_conv2d
build_trainer	call	assign_sub
dropout	followed_by	variable_scope
_enas_layer	call	variable_scope
_build_valid	call	equal
_model	call	_enas_layer
_enas_layer	call	boolean_mask
constant	followed_by	range
build_trainer	call	get_train_ops
_conv_branch	call	relu
_build_test	call	to_int32
scatter_sub	followed_by	control_dependencies
_enas_layer	call	case
_build_valid	call	argmax
train	call	ConfigProto
_fixed_layer	call	create_weight
_conv_branch	followed_by	variable_scope
_enas_layer	call	logical_or
where	followed_by	to_int32
train	call	Saver
_get_C	followed_by	create_weight
_factorized_reduction	call	avg_pool
_conv_branch	call	transpose
_factorized_reduction	followed_by	global_avg_pool
build_trainer	call	identity
reduce_sum	followed_by	Variable
variable_scope	followed_by	get_variable
train	call	read_data
max_pooling2d	has_arg0	SAME
Variable	followed_by	assign_sub
_model	call	dropout
get_train_ops	call	cond
reshape	followed_by	variable_scope
_enas_layer	call	constant
_model	call	matmul
_conv_branch	call	conv2d
concat	has_axis	3
_fixed_layer	call	_pool_branch
_pool_branch	call	relu
_conv_branch	call	batch_norm
concat	followed_by	concat
greater	has_arg0	0
get_train_ops	call	gradients
batch_norm	followed_by	relu
gradients	followed_by	global_norm
equal	has_arg0	1
batch_norm_with_mask	call	boolean_mask
get_ops	call	norm
create_weight	followed_by	conv2d
avg_pool	has_data_format	self.data_format
_enas_layer	call	reshape
trainable_variables	followed_by	count_model_params
variable_scope	followed_by	_conv_branch
assign_sub	followed_by	control_dependencies
_build_train	call	count_model_params
relu	call	where
_conv_branch	call	variable_scope
variable_scope	followed_by	create_weight
get_train_ops	call	IndexedSlices
reshape	followed_by	conv2d
concat	has_axis	1
get_ops	followed_by	Saver
_enas_layer	call	relu
get_train_ops	call	MomentumOptimizer
_enas_layer	call	batch_norm
_enas_layer	call	equal
clip_by_norm	followed_by	Variable
batch_norm_with_mask	call	where
_enas_layer	call	conv2d
batch_norm_with_mask	call	identity
control_dependencies	followed_by	identity
separable_conv2d	followed_by	batch_norm
variable_scope	has_arg0	branch_4
Adam	followed_by	connect_controller
connect_controller	call	_build_train
shape	followed_by	reshape
Variable	followed_by	cond
variable_scope	followed_by	range
get_train_ops	call	Variable
batch_norm	call	fused_batch_norm
AdamOptimizer	followed_by	SyncReplicasOptimizer
create_weight	followed_by	create_weight
_conv_branch	call	range
add_n	followed_by	gradients
_build_valid	call	reduce_sum
get_train_ops	call	AdamOptimizer
reshape	followed_by	reshape
scatter_sub	followed_by	scatter_sub
_fixed_layer	call	concat
range	has_arg0	0
_build_test	call	_model
_conv_branch	call	create_weight
reshape	followed_by	transpose
_enas_layer	call	_pool_branch
boolean_mask	followed_by	reshape
get_train_ops	call	clip_by_norm
mean	followed_by	reshape
get_train_ops	call	reduce_sum
print_user_flags	followed_by	train
to_float	followed_by	to_float
equal	followed_by	zeros_like
variable_scope	has_arg0	final_conv
cond	call	greater_equal
case	call	constant
_model	call	create_weight
variable_scope	has_arg0	path1_conv
case	followed_by	variable_scope
identity	followed_by	boolean_mask
get_ops	call	build_trainer
_build_train	call	get_train_ops
range	followed_by	variable_scope
equal	has_arg0	0
_build_train	call	argmax
_model	call	conv2d
range	has_dtype	tf.int32
fused_batch_norm	followed_by	boolean_mask
_model	call	batch_norm
get_train_ops	call	clip_by_global_norm
IndexedSlices	followed_by	clip_by_norm
_pool_branch	call	create_weight
get_train_ops	call	SyncReplicasOptimizer
to_int32	followed_by	equal
_model	followed_by	argmax
variable_scope	followed_by	_pool_branch
_model	followed_by	sparse_softmax_cross_entropy_with_logits
_factorized_reduction	call	variable_scope
_model	call	_fixed_layer
_fixed_layer	call	variable_scope
reduce_mean	followed_by	argmax
to_int32	followed_by	reshape
ConfigProto	followed_by	SingularMonitoredSession
train	call	SingularMonitoredSession
get_variable	followed_by	get_variable
enas.cifar10.main.main	call	Logger.__init__
global_norm	followed_by	sqrt
variable_scope	followed_by	_factorized_reduction
_read_data	call	load
_enas_layer	call	range
_build_train	followed_by	_build_valid
batch_norm_with_mask	call	reshape
_get_strides	followed_by	avg_pool
create_weight	followed_by	transpose
Saver	followed_by	CheckpointSaverHook
_read_data	call	reshape
batch_norm_with_mask	call	control_dependencies
_factorized_reduction	call	_get_strides
equal	followed_by	case
range	followed_by	cond
_enas_layer	call	create_weight
variable_scope	followed_by	average_pooling2d
constant	has_arg0	0
train	call	get_ops
reduce_mean	followed_by	reduce_mean
relu	followed_by	variable_scope
get_train_ops	call	maximum
concat	followed_by	variable_scope
_build_test	call	equal
equal	followed_by	to_int32
create_weight	followed_by	relu
Variable	followed_by	Variable
get_train_ops	call	exponential_decay
enas.cifar10.main.main	call	print_user_flags
_factorized_reduction	call	concat
get_variable	followed_by	boolean_mask
boolean_mask	followed_by	fused_batch_norm
_pool_branch	call	variable_scope
average_pooling2d	followed_by	max_pooling2d
_build_train	call	reduce_mean
range	followed_by	logical_and
trainable_variables	followed_by	get_train_ops
get_train_ops	call	GradientDescentOptimizer
_fixed_layer	call	range
_build_test	call	argmax
equal	has_arg0	4
identity	followed_by	fused_batch_norm
_model	call	global_avg_pool
identity	followed_by	Variable
get_train_ops	call	add_n
train	call	CheckpointSaverHook
get_variable	has_arg0	moving_mean
conv2d	followed_by	batch_norm
Variable	has_name	T_i
_build_train	call	reduce_sum
sparse_softmax_cross_entropy_with_logits	followed_by	reduce_mean
variable_scope	has_arg0	branch_5
logical_and	followed_by	logical_or
to_int32	followed_by	reduce_sum
conv2d	has_data_format	self.data_format
cond	followed_by	add_n
_build_valid	call	to_int32
read_data	call	mean
_pool_branch	followed_by	variable_scope
load	followed_by	reshape
average_pooling2d	has_arg0	SAME
_read_data	call	transpose
concat	followed_by	shape
argmax	has_axis	1
variable_scope	has_arg0	skip
get_train_ops	call	global_norm
separable_conv2d	has_data_format	self.data_format
_pool_branch	call	max_pooling2d
concat	followed_by	relu
transpose	followed_by	reshape
_fixed_layer	followed_by	variable_scope
Variable	followed_by	get_train_ops
get_variable	has_arg0	offset
Variable	has_arg0	0.0
variable_scope	has_arg0	branch_1
sqrt	followed_by	clip_by_global_norm
variable_scope	followed_by	variable_scope
boolean_mask	followed_by	get_variable
_model	call	_factorized_reduction
create_weight	call	get_variable
read_data	call	_read_data
read_data	call	reshape
conv2d	followed_by	concat
norm	followed_by	Adam
boolean_mask	followed_by	scatter_sub
_enas_layer	call	concat
equal	has_arg0	5
_pool_branch	call	average_pooling2d
reduce_sum	followed_by	add_n
_build_train	call	trainable_variables
batch_norm	followed_by	_get_strides
create_weight	followed_by	separable_conv2d
cond	call	less
count_model_params	followed_by	Variable
get_variable	has_arg0	moving_variance
transpose	followed_by	transpose
batch_norm_with_mask	followed_by	create_weight
cond	followed_by	MomentumOptimizer
_enas_layer	call	_conv_branch
_enas_layer	call	logical_and
get_ops	call	Adam
batch_norm_with_mask	call	fused_batch_norm
range	followed_by	concat
_factorized_reduction	call	_get_C
enas.cifar10.main.main	call	train
batch_norm	call	get_variable
_factorized_reduction	call	create_weight
sparse_softmax_cross_entropy_with_logits	has_labels	self.y_train
_build_train	call	_model
reshape	followed_by	concat
get_variable	has_arg0	scale
MomentumOptimizer	has_arg0	0.9
connect_controller	call	_build_test
global_avg_pool	followed_by	dropout
conv2d	followed_by	pad
_enas_layer	followed_by	_fixed_layer
maximum	has_arg0	0
read_data	followed_by	read_data
cond	call	equal
sqrt	followed_by	sqrt
transpose	followed_by	conv2d
_build_sampler	call	zeros
_build_sampler	call	TensorArray
_build_sampler	call	stack
_create_params	call	variable_scope
get_variable	followed_by	reshape
get_variable	has_arg0	g_emb
range	followed_by	range
stack_lstm	call	lstm
range	followed_by	variable_scope
concat	has_axis	1
_build_sampler	call	reshape
_build_sampler	call	range
reshape	followed_by	reduce_sum
constant	followed_by	while_loop
range	has_arg0	2
lstm	call	tanh
constant	followed_by	constant
stack	followed_by	reshape
reshape	followed_by	constant
get_variable	has_arg0	w_2
get_variable	has_arg0	b
matmul	followed_by	constant
variable_scope	followed_by	range
lstm	call	split
zeros_like	followed_by	matmul
constant	has_name	layer_id
while_loop	has_parallel_iterations	1
sigmoid	followed_by	sigmoid
_create_params	call	get_variable
constant	has_dtype	tf.float32
get_variable	has_arg0	w_1
matmul	call	concat
_create_params	call	constant
constant	followed_by	variable_scope
enas.cifar10.micro_controller.MicroController.__init__	call	_build_sampler
split	has_axis	1
enas.cifar10.micro_controller.MicroController.__init__	call	_create_params
_create_params	call	reshape
_build_sampler	call	reduce_sum
_create_params	call	range
sigmoid	followed_by	tanh
split	has_arg0	4
split	followed_by	sigmoid
variable_scope	followed_by	get_variable
lstm	call	sigmoid
constant	has_name	entropy
get_variable	followed_by	get_variable
TensorArray	followed_by	TensorArray
range	followed_by	zeros
matmul	followed_by	split
stack_lstm	followed_by	zeros_like
_build_sampler	call	while_loop
range	followed_by	stack_lstm
variable_scope	has_arg0	lstm
get_variable	followed_by	variable_scope
TensorArray	followed_by	zeros
_build_sampler	followed_by	_build_sampler
zeros	followed_by	range
_build_sampler	call	matmul
get_variable	has_arg0	w
variable_scope	followed_by	variable_scope
reduce_sum	followed_by	reduce_sum
get_variable	has_arg0	v
_create_params	followed_by	_build_sampler
constant	has_name	log_prob
constant	has_arg0	2
_build_sampler	call	stack_lstm
variable_scope	has_arg0	emb
variable_scope	has_arg0	softmax
_build_sampler	call	constant
while_loop	followed_by	stack
variable_scope	has_arg0	attention
lstm	call	matmul
tanh	followed_by	tanh
_build_sampler	call	zeros_like
_enas_conv	call	range
range	followed_by	variable_scope
_fixed_layer	call	max_pooling2d
_fixed_layer	call	average_pooling2d
control_dependencies	followed_by	identity
to_float	followed_by	minimum
_fixed_layer	followed_by	normal
separable_conv2d	has_data_format	self.data_format
_enas_conv	call	reshape
variable_scope	followed_by	zeros
_maybe_calibrate_size	call	variable_scope
avg_pool	has_arg0	VALID
_model	call	batch_norm
drop_path	call	div
trainable_variables	followed_by	count_model_params
_model	call	dropout
concat	followed_by	concat
_fixed_combine	call	variable_scope
to_int32	followed_by	reduce_sum
reshape	followed_by	variable_scope
variable_scope	followed_by	variable_scope
_get_HW	followed_by	variable_scope
_get_C	followed_by	variable_scope
_model	followed_by	argmax
dropout	followed_by	variable_scope
_factorized_reduction	call	_get_C
_model	call	range
create_weight	followed_by	conv2d
pad	followed_by	pad
_enas_layer	call	transpose
relu	followed_by	conv2d
separable_conv2d	followed_by	batch_norm
average_pooling2d	has_arg0	VALID
where	call	greater
_enas_layer	call	one_hot
transpose	followed_by	reshape
batch_norm	followed_by	zeros
_fixed_conv	call	_get_strides
argmax	has_axis	1
_maybe_calibrate_size	call	_get_C
avg_pool	followed_by	variable_scope
_factorized_reduction	call	avg_pool
_get_C	followed_by	_get_strides
_enas_conv	call	separable_conv2d
_fixed_combine	call	concat
Reduction	followed_by	variable_scope
_fixed_layer	call	_get_C
max_pooling2d	followed_by	_get_C
_get_C	followed_by	_get_HW
_get_HW	followed_by	_get_C
variable_scope	has_arg0	final_conv
_enas_layer	call	size
_enas_cell	call	_enas_conv
_fixed_layer	call	relu
max_pooling2d	has_data_format	self.actual_data_format
_fixed_conv	call	variable_scope
average_pooling2d	followed_by	variable_scope
_enas_layer	call	variable_scope
variable_scope	has_arg0	pool_y
average_pooling2d	has_arg0	SAME
_fixed_conv	call	range
batch_norm	followed_by	relu
variable_scope	followed_by	_fixed_conv
_model	call	create_weight
variable_scope	followed_by	_get_C
_model	call	_factorized_reduction
_enas_cell	call	reshape
_apply_drop_path	call	drop_path
_maybe_calibrate_size	followed_by	range
_model	call	_get_HW
_enas_conv	call	fused_batch_norm
zeros	has_dtype	tf.float32
variable_scope	followed_by	_enas_layer
_apply_drop_path	call	to_float
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	shuffle_batch
equal	has_arg0	0
conv2d	has_arg0	SAME
range	followed_by	stack
identity	followed_by	fused_batch_norm
_enas_cell	call	variable_scope
concat	has_axis	3
_factorized_reduction	call	conv2d
_factorized_reduction	followed_by	variable_scope
average_pooling2d	followed_by	max_pooling2d
_enas_layer	call	reshape
_enas_layer	call	shape
_factorized_reduction	call	pad
get_variable	followed_by	get_variable
to_int32	followed_by	reshape
_enas_cell	call	batch_norm
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	equal
drop_path	call	floor
variable_scope	followed_by	max_pooling2d
_model	call	variable_scope
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	reduce_sum
gather	has_axis	0
min	followed_by	variable_scope
floor	followed_by	div
pad	followed_by	avg_pool
_fixed_layer	call	_factorized_reduction
reshape	call	shape
variable_scope	has_arg0	avg_pool
size	followed_by	stack
_fixed_layer	call	_apply_drop_path
average_pooling2d	has_data_format	self.actual_data_format
minimum	has_arg0	1.0
_model	call	_fixed_layer
_model	call	relu
_maybe_calibrate_size	call	conv2d
variable_scope	has_arg0	stem_conv
batch_norm	followed_by	_get_C
_fixed_layer	call	create_weight
get_variable	has_arg0	moving_variance
batch_norm	followed_by	_enas_conv
max_pooling2d	followed_by	create_weight
_model	call	_enas_layer
_maybe_calibrate_size	call	create_weight
zeros	followed_by	ones
shape	followed_by	random_uniform
_fixed_layer	call	range
_maybe_calibrate_size	call	_get_HW
relu	followed_by	average_pooling2d
_enas_layer	call	where
reshape	followed_by	shape
_enas_layer	call	add_n
batch_norm	call	identity
device	followed_by	transpose
where	followed_by	to_int32
variable_scope	has_arg0	layer_base
create_weight	followed_by	relu
relu	call	where
gather	followed_by	reshape
variable_scope	followed_by	range
_get_C	followed_by	average_pooling2d
separable_conv2d	followed_by	fused_batch_norm
_fixed_combine	call	_factorized_reduction
_enas_cell	call	relu
_factorized_reduction	followed_by	_enas_layer
variable_scope	has_arg0	x_conv
_model	call	_get_C
batch_norm	followed_by	variable_scope
variable_scope	followed_by	create_weight
_fixed_conv	followed_by	_get_C
_enas_layer	call	batch_norm
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	_model
batch_norm	call	get_variable
_factorized_reduction	call	concat
_fixed_combine	call	_get_HW
variable_scope	followed_by	_get_HW
_fixed_layer	call	variable_scope
global_avg_pool	call	reduce_mean
_enas_layer	call	_enas_cell
_enas_conv	call	create_weight
batch_norm	followed_by	reshape
conv2d	followed_by	batch_norm
variable_scope	has_arg0	proj
_model	call	conv2d
device	has_arg0	/cpu:0
reshape	followed_by	size
_enas_layer	call	relu
normal	followed_by	_factorized_reduction
_fixed_combine	call	min
_fixed_conv	call	_get_C
variable_scope	has_arg0	pool_x
batch_norm	call	variable_scope
conv2d	followed_by	pad
batch_norm	followed_by	range
_fixed_conv	call	relu
minimum	followed_by	drop_path
variable_scope	followed_by	average_pooling2d
_fixed_layer	call	_maybe_calibrate_size
_apply_drop_path	call	minimum
shuffle_batch	followed_by	map_fn
_factorized_reduction	call	batch_norm
variable_scope	followed_by	get_variable
get_variable	has_arg0	scale
gather	followed_by	shape
map_fn	followed_by	_model
range	followed_by	_get_C
ones	has_dtype	tf.float32
_fixed_conv	call	batch_norm
_fixed_layer	call	_fixed_combine
reduce_mean	followed_by	reduce_mean
batch_norm	followed_by	_apply_drop_path
create_weight	followed_by	gather
variable_scope	followed_by	relu
_enas_cell	call	create_weight
random_uniform	has_dtype	tf.float32
create_weight	followed_by	matmul
to_int32	followed_by	equal
batch_norm	call	control_dependencies
create_weight	followed_by	reshape
_enas_conv	call	zeros
relu	followed_by	_factorized_reduction
_enas_cell	followed_by	one_hot
variable_scope	followed_by	_enas_cell
variable_scope	has_arg0	path_conv
variable_scope	has_arg0	calibrate
_enas_cell	call	max_pooling2d
_fixed_layer	call	batch_norm
_factorized_reduction	call	_get_strides
_enas_layer	call	_maybe_calibrate_size
one_hot	followed_by	variable_scope
add_n	followed_by	where
_model	call	matmul
_maybe_calibrate_size	followed_by	variable_scope
_enas_cell	call	stack
_get_strides	followed_by	avg_pool
reshape	followed_by	relu
drop_path	call	shape
variable_scope	followed_by	global_avg_pool
count_model_params	followed_by	relu
stack	followed_by	variable_scope
variable_scope	has_arg0	x
get_variable	followed_by	fused_batch_norm
_factorized_reduction	followed_by	create_weight
variable_scope	has_arg0	bn
zeros	followed_by	range
_maybe_calibrate_size	call	relu
equal	followed_by	to_int32
stack	has_axis	0
variable_scope	has_arg0	max_pool
argmax	followed_by	to_int32
get_variable	has_arg0	moving_mean
_enas_conv	call	ones
_factorized_reduction	call	variable_scope
zeros	has_dtype	np.int32
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	argmax
drop_path	call	random_uniform
_apply_drop_path	followed_by	_fixed_combine
_enas_layer	call	stack
_model	call	count_model_params
variable_scope	has_arg0	path1_conv
random_uniform	followed_by	floor
_model	call	Reduction
get_variable	has_arg0	offset
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	device
_maybe_calibrate_size	call	batch_norm
_enas_conv	followed_by	_enas_conv
avg_pool	has_data_format	self.data_format
concat	followed_by	batch_norm
variable_scope	has_arg0	fc
_enas_conv	call	_get_C
where	call	equal
_enas_cell	call	average_pooling2d
matmul	followed_by	trainable_variables
_fixed_layer	call	zeros
concat	has_axis	1
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	map_fn
variable_scope	has_arg0	y
conv2d	has_data_format	self.data_format
variable_scope	has_arg0	path2_conv
max_pooling2d	has_arg0	SAME
_fixed_conv	call	create_weight
_enas_layer	call	create_weight
_enas_layer	followed_by	_fixed_layer
_model	call	global_avg_pool
relu	followed_by	variable_scope
_enas_cell	call	conv2d
_factorized_reduction	followed_by	concat
_enas_conv	followed_by	stack
_factorized_reduction	call	create_weight
relu	followed_by	global_avg_pool
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	to_int32
_enas_layer	call	range
_enas_conv	call	relu
_model	call	trainable_variables
shape	followed_by	transpose
_enas_layer	call	gather
_model	call	average_pooling2d
relu	followed_by	separable_conv2d
global_avg_pool	followed_by	create_weight
stack	followed_by	gather
_fixed_layer	call	_fixed_conv
variable_scope	has_arg0	y_conv
fused_batch_norm	followed_by	control_dependencies
conv2d	followed_by	concat
transpose	followed_by	shuffle_batch
_apply_drop_path	followed_by	variable_scope
average_pooling2d	followed_by	_get_C
_enas_layer	call	to_int32
batch_norm	call	fused_batch_norm
_enas_layer	call	conv2d
_enas_cell	call	_get_C
enas.cifar10.micro_child.MicroChild.build_valid_rl	call	transpose
greater	has_arg0	0
_get_strides	followed_by	variable_scope
to_float	followed_by	to_float
shape	followed_by	shape
create_weight	followed_by	create_weight
ones	followed_by	create_weight
create_weight	call	get_variable
one_hot	has_dtype	tf.int32
batch_norm	followed_by	_get_strides
_fixed_layer	call	conv2d
one_hot	followed_by	add_n
_get_HW	followed_by	create_weight
_enas_conv	call	variable_scope
_get_C	followed_by	_factorized_reduction
global_avg_pool	followed_by	dropout
reshape	followed_by	create_weight
variable_scope	has_arg0	conv
_get_C	followed_by	create_weight
_fixed_conv	call	separable_conv2d
variable_scope	has_arg0	aux_head
_maybe_calibrate_size	call	_factorized_reduction
_model	call	normal
conv2d	has_arg0	VALID
_fixed_layer	followed_by	Reduction
variable_scope	followed_by	_factorized_reduction
variable_scope	has_arg0	final_combine
enas.cifar10.image_ops.fully_connected	call	matmul
create_weight	call	get_variable
variable_scope	followed_by	create_weight
create_weight	followed_by	matmul
enas.cifar10.image_ops.fully_connected	call	variable_scope
enas.cifar10.image_ops.fully_connected	call	create_weight
lstm	call	split
sparse_softmax_cross_entropy_with_logits	followed_by	embedding_lookup
zeros	followed_by	zeros
_build_sampler	call	concat
multinomial	followed_by	to_int32
split	followed_by	sigmoid
zeros	followed_by	stack_lstm
get_variable	followed_by	variable_scope
matmul	followed_by	tanh
variable_scope	has_arg0	embedding
variable_scope	has_arg0	softmax
_build_sampler	call	exp
_build_sampler	call	tanh
variable_scope	followed_by	get_variable
_create_params	followed_by	_build_sampler
tanh	followed_by	tanh
to_int32	followed_by	reshape
exp	call	to_float
_build_sampler	call	stack_lstm
_build_sampler	call	stop_gradient
_build_sampler	call	sparse_softmax_cross_entropy_with_logits
zeros	has_dtype	tf.float32
exp	call	reduce_sum
_build_sampler	call	zeros
stop_gradient	followed_by	matmul
reshape	followed_by	sparse_softmax_cross_entropy_with_logits
matmul	followed_by	split
_build_sampler	call	to_int32
multinomial	has_arg0	1
enas.cifar10.controller.ConvController.__init__	call	_build_sampler
split	has_axis	1
concat	has_axis	0
concat	followed_by	concat
_create_params	call	variable_scope
get_variable	has_arg0	w
_build_sampler	call	embedding_lookup
get_variable	has_arg0	g_emb
lstm	call	tanh
_build_sampler	call	reshape
tanh	followed_by	multinomial
split	has_arg0	4
stack_lstm	call	lstm
variable_scope	has_arg0	critic
embedding_lookup	followed_by	concat
reduce_sum	followed_by	to_float
_create_params	call	get_variable
matmul	call	concat
enas.cifar10.controller.ConvController.__init__	call	_create_params
lstm	call	sigmoid
sigmoid	followed_by	tanh
get_variable	followed_by	get_variable
stack_lstm	followed_by	stop_gradient
_build_sampler	call	multinomial
_build_sampler	call	matmul
concat	followed_by	exp
variable_scope	has_arg0	lstm
variable_scope	followed_by	variable_scope
concat	has_axis	1
lstm	call	matmul
sigmoid	followed_by	sigmoid
get_train_ops	call	SyncReplicasOptimizer
Variable	followed_by	assign_sub
Variable	has_name	critic_train_step
get_train_ops	call	AdamOptimizer
reduce_mean	followed_by	Variable
get_train_ops	call	global_norm
sqrt	followed_by	clip_by_global_norm
enas.cifar10.controller.ConvController.build_trainer	call	assign_sub
get_train_ops	call	reduce_sum
sqrt	call	reduce_sum
get_train_ops	call	IndexedSlices
global_norm	followed_by	sqrt
Variable	followed_by	Variable
enas.cifar10.controller.ConvController.build_trainer	call	identity
enas.cifar10.controller.ConvController.build_trainer	call	reduce_mean
exponential_decay	call	maximum
enas.cifar10.controller.ConvController.build_trainer	call	Adam
enas.cifar10.controller.ConvController.build_trainer	call	group
reduce_sum	followed_by	add_n
get_train_ops	call	clip_by_global_norm
get_train_ops	call	MomentumOptimizer
clip_by_norm	followed_by	Variable
get_train_ops	call	GradientDescentOptimizer
clip_by_norm	followed_by	IndexedSlices
reduce_sum	followed_by	reduce_mean
cond	call	greater_equal
IndexedSlices	followed_by	clip_by_norm
cond	followed_by	MomentumOptimizer
get_train_ops	call	Variable
get_train_ops	call	clip_by_norm
Variable	followed_by	get_train_ops
trainable_variables	followed_by	get_train_ops
add_n	followed_by	gradients
get_train_ops	call	cond
identity	followed_by	Variable
to_float	followed_by	concat
Variable	has_name	T_i
to_float	followed_by	to_float
GradientDescentOptimizer	followed_by	AdamOptimizer
AdamOptimizer	followed_by	SyncReplicasOptimizer
concat	has_axis	0
gradients	followed_by	global_norm
control_dependencies	followed_by	identity
MomentumOptimizer	followed_by	GradientDescentOptimizer
clip_by_global_norm	followed_by	clip_by_norm
Adam	followed_by	reduce_sum
MomentumOptimizer	has_arg0	0.9
get_train_ops	call	sqrt
enas.cifar10.controller.ConvController.build_trainer	call	get_train_ops
maximum	has_arg0	0
sqrt	followed_by	sqrt
cond	followed_by	exponential_decay
enas.cifar10.controller.ConvController.build_trainer	call	reduce_sum
get_train_ops	call	maximum
get_train_ops	followed_by	Adam
matmul	followed_by	reduce_sum
assign_sub	followed_by	control_dependencies
get_train_ops	followed_by	group
Variable	has_name	last_reset
enas.cifar10.controller.ConvController.build_trainer	call	Variable
reduce_mean	followed_by	reduce_mean
get_train_ops	call	add_n
enas.cifar10.controller.ConvController.build_trainer	call	concat
Variable	has_arg0	0
Variable	has_arg0	0.0
Variable	has_name	train_step
maximum	followed_by	cond
reduce_sum	followed_by	Variable
enas.cifar10.controller.ConvController.build_trainer	call	matmul
enas.cifar10.controller.ConvController.build_trainer	call	to_float
Variable	followed_by	cond
enas.cifar10.controller.ConvController.build_trainer	call	control_dependencies
get_train_ops	call	exponential_decay
get_train_ops	call	gradients
enas.cifar10.controller.ConvController.build_trainer	call	trainable_variables
Variable	followed_by	trainable_variables
exponential_decay	followed_by	maximum
concat	followed_by	matmul
cond	call	less
device	has_arg0	/cpu:0
enas.cifar10.models.Model.build_valid_rl	call	to_int32
equal	followed_by	to_int32
transpose	followed_by	shuffle_batch
enas.cifar10.models.Model.build_valid_rl	call	device
enas.cifar10.models.Model.build_valid_rl	call	equal
_model	followed_by	argmax
enas.cifar10.models.Model.build_valid_rl	call	reduce_sum
to_int32	followed_by	equal
shuffle_batch	followed_by	map_fn
enas.cifar10.models.Model.build_valid_rl	call	map_fn
map_fn	followed_by	_model
enas.cifar10.models.Model.build_valid_rl	call	shuffle_batch
device	followed_by	transpose
to_int32	followed_by	reduce_sum
enas.cifar10.models.Model.build_valid_rl	call	_model
argmax	followed_by	to_int32
enas.cifar10.models.Model.build_valid_rl	call	argmax
argmax	has_axis	1
enas.cifar10.models.Model.build_valid_rl	call	transpose
constant	has_dtype	tf.float32
constant	has_arg0	0.0
Variable	followed_by	trainable_variables
enas.cifar10.micro_controller.MicroController.build_trainer	call	identity
get_train_ops	call	maximum
get_train_ops	call	add_n
trainable_variables	followed_by	get_train_ops
sqrt	followed_by	clip_by_global_norm
sqrt	call	reduce_sum
GradientDescentOptimizer	followed_by	AdamOptimizer
get_train_ops	call	IndexedSlices
clip_by_norm	followed_by	Variable
Variable	has_arg0	0.0
to_float	followed_by	reduce_sum
cond	followed_by	exponential_decay
enas.cifar10.micro_controller.MicroController.build_trainer	call	Variable
Variable	has_name	last_reset
AdamOptimizer	followed_by	SyncReplicasOptimizer
exponential_decay	followed_by	maximum
sqrt	followed_by	sqrt
gradients	followed_by	global_norm
reduce_sum	followed_by	Variable
get_train_ops	call	clip_by_norm
enas.cifar10.micro_controller.MicroController.build_trainer	call	assign_sub
exponential_decay	call	maximum
get_train_ops	call	SyncReplicasOptimizer
enas.cifar10.micro_controller.MicroController.build_trainer	call	constant
Variable	has_arg0	0
get_train_ops	call	global_norm
get_train_ops	followed_by	constant
enas.cifar10.micro_controller.MicroController.build_trainer	call	reduce_sum
MomentumOptimizer	followed_by	GradientDescentOptimizer
Variable	followed_by	assign_sub
get_train_ops	call	MomentumOptimizer
reduce_sum	followed_by	add_n
Variable	followed_by	cond
get_train_ops	call	cond
get_train_ops	call	exponential_decay
to_float	followed_by	to_float
Variable	followed_by	Variable
get_train_ops	call	sqrt
maximum	has_arg0	0
enas.cifar10.micro_controller.MicroController.build_trainer	call	get_train_ops
assign_sub	followed_by	control_dependencies
clip_by_norm	followed_by	IndexedSlices
control_dependencies	followed_by	identity
add_n	followed_by	gradients
maximum	followed_by	cond
get_train_ops	call	Variable
cond	call	less
enas.cifar10.micro_controller.MicroController.build_trainer	call	control_dependencies
get_train_ops	call	gradients
get_train_ops	call	AdamOptimizer
enas.cifar10.micro_controller.MicroController.build_trainer	call	trainable_variables
IndexedSlices	followed_by	clip_by_norm
identity	followed_by	Variable
MomentumOptimizer	has_arg0	0.9
cond	followed_by	MomentumOptimizer
global_norm	followed_by	sqrt
get_train_ops	call	reduce_sum
cond	call	greater_equal
enas.cifar10.micro_controller.MicroController.build_trainer	call	to_float
get_train_ops	call	clip_by_global_norm
Variable	has_name	T_i
clip_by_global_norm	followed_by	clip_by_norm
get_train_ops	call	GradientDescentOptimizer
Variable	has_name	train_step
equal	followed_by	to_int32
to_int32	followed_by	reduce_sum
enas.cifar10.models.Model._build_test	call	equal
enas.cifar10.models.Model._build_test	call	argmax
argmax	followed_by	to_int32
argmax	has_axis	1
to_int32	followed_by	equal
_model	followed_by	argmax
enas.cifar10.models.Model._build_test	call	to_int32
enas.cifar10.models.Model._build_test	call	_model
enas.cifar10.models.Model._build_test	call	reduce_sum
_enas_layer	call	conv2d
variable_scope	has_arg0	inp_conv_1
range	has_dtype	tf.int32
_enas_layer	call	cond
_conv_branch	call	transpose
variable_scope	has_arg0	conv_1x1
equal	has_arg0	2
variable_scope	has_arg0	branch_3
variable_scope	has_arg0	path2_conv
create_weight	followed_by	matmul
_fixed_layer	call	_conv_branch
concat	followed_by	variable_scope
logical_and	followed_by	logical_or
batch_norm_with_mask	call	identity
create_weight	followed_by	transpose
_fixed_layer	call	batch_norm
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	map_fn
dropout	followed_by	variable_scope
_fixed_layer	call	range
pad	followed_by	pad
_enas_layer	call	create_weight
variable_scope	has_arg0	branch_5
where	call	greater
variable_scope	followed_by	_enas_layer
conv2d	followed_by	concat
get_variable	followed_by	boolean_mask
reshape	followed_by	separable_conv2d
_model	call	create_weight
equal	has_arg0	0
variable_scope	has_arg0	conv_1
_pool_branch	call	create_weight
add_n	followed_by	batch_norm
case	call	constant
_enas_layer	call	range
separable_conv2d	followed_by	range
batch_norm_with_mask	call	variable_scope
variable_scope	has_arg0	branch_1
batch_norm_with_mask	call	fused_batch_norm
device	has_arg0	/cpu:0
_pool_branch	call	average_pooling2d
to_int32	followed_by	equal
batch_norm	call	control_dependencies
create_weight	call	get_variable
variable_scope	followed_by	_get_C
variable_scope	has_arg0	path1_conv
conv2d	followed_by	pad
pad	followed_by	avg_pool
_conv_branch	call	separable_conv2d
create_weight	followed_by	concat
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	reduce_sum
transpose	followed_by	conv2d
equal	followed_by	zeros_like
_enas_layer	call	equal
greater	has_arg0	0
average_pooling2d	followed_by	max_pooling2d
variable_scope	has_arg0	skip
range	followed_by	logical_and
transpose	followed_by	shuffle_batch
average_pooling2d	has_arg0	SAME
batch_norm	call	identity
_enas_layer	call	boolean_mask
avg_pool	has_data_format	self.data_format
logical_and	followed_by	batch_norm_with_mask
reduce_mean	followed_by	reduce_mean
variable_scope	followed_by	_factorized_reduction
_enas_layer	followed_by	_fixed_layer
batch_norm_with_mask	call	get_variable
_fixed_layer	call	conv2d
global_avg_pool	call	reduce_mean
concat	followed_by	shape
variable_scope	has_arg0	fc
batch_norm	followed_by	range
equal	followed_by	to_int32
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	shuffle_batch
equal	has_arg0	1
range	followed_by	concat
variable_scope	has_arg0	path_conv
create_weight	followed_by	conv2d
conv2d	followed_by	range
_model	call	global_avg_pool
_enas_layer	call	_pool_branch
conv2d	followed_by	batch_norm
variable_scope	followed_by	_conv_branch
to_int32	followed_by	reduce_sum
get_variable	has_arg0	moving_variance
equal	has_arg0	3
_fixed_layer	call	_pool_branch
constant	has_arg0	0
identity	followed_by	boolean_mask
variable_scope	followed_by	variable_scope
_model	call	variable_scope
variable_scope	has_arg0	stem_conv
_fixed_layer	call	concat
variable_scope	followed_by	get_variable
range	followed_by	variable_scope
_conv_branch	call	batch_norm_with_mask
_factorized_reduction	call	conv2d
batch_norm	call	variable_scope
_conv_branch	call	batch_norm
_model	call	dropout
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	argmax
batch_norm	call	get_variable
logical_or	followed_by	boolean_mask
_conv_branch	call	range
_factorized_reduction	call	create_weight
boolean_mask	followed_by	get_variable
_conv_branch	followed_by	equal
equal	has_arg0	5
separable_conv2d	has_data_format	self.data_format
case	followed_by	variable_scope
_enas_layer	call	constant
equal	has_arg0	4
cond	call	equal
transpose	followed_by	transpose
_fixed_layer	call	relu
_enas_layer	call	reshape
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	transpose
constant	followed_by	range
variable_scope	has_arg0	branch_2
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	_model
_enas_layer	call	shape
_conv_branch	call	reshape
concat	followed_by	relu
argmax	followed_by	to_int32
_model	followed_by	argmax
boolean_mask	followed_by	scatter_sub
_factorized_reduction	call	concat
fused_batch_norm	followed_by	boolean_mask
create_weight	followed_by	constant
batch_norm	call	fused_batch_norm
max_pooling2d	has_arg0	SAME
cond	followed_by	add_n
_conv_branch	followed_by	variable_scope
_fixed_layer	followed_by	variable_scope
to_int32	followed_by	reshape
conv2d	has_data_format	self.data_format
get_variable	has_arg0	offset
boolean_mask	followed_by	fused_batch_norm
_pool_branch	followed_by	variable_scope
batch_norm	followed_by	_get_strides
map_fn	followed_by	_model
_get_C	followed_by	create_weight
separable_conv2d	followed_by	batch_norm
variable_scope	followed_by	average_pooling2d
batch_norm_with_mask	call	where
range	followed_by	cond
_pool_branch	followed_by	equal
batch_norm	followed_by	relu
_model	call	batch_norm
identity	followed_by	fused_batch_norm
boolean_mask	followed_by	boolean_mask
batch_norm_with_mask	followed_by	create_weight
_enas_layer	call	case
variable_scope	followed_by	create_weight
_conv_branch	call	variable_scope
_model	call	range
batch_norm_with_mask	call	reshape
create_weight	followed_by	separable_conv2d
scatter_sub	followed_by	control_dependencies
_enas_layer	call	batch_norm
control_dependencies	followed_by	identity
_pool_branch	call	max_pooling2d
shuffle_batch	followed_by	map_fn
batch_norm_with_mask	call	scatter_sub
range	followed_by	range
concat	has_axis	1
variable_scope	followed_by	range
_model	call	_enas_layer
_factorized_reduction	call	variable_scope
batch_norm_with_mask	call	control_dependencies
_fixed_layer	call	create_weight
where	followed_by	to_int32
scatter_sub	followed_by	scatter_sub
concat	followed_by	concat
_get_strides	followed_by	avg_pool
_factorized_reduction	call	pad
_pool_branch	call	batch_norm
_conv_branch	call	logical_and
concat	has_axis	3
batch_norm_with_mask	followed_by	relu
cond	call	zeros_like
_conv_branch	call	relu
_enas_layer	call	logical_and
get_variable	has_arg0	scale
batch_norm_with_mask	call	to_int32
avg_pool	has_arg0	VALID
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	equal
batch_norm	followed_by	variable_scope
range	has_arg0	0
equal	followed_by	variable_scope
_enas_layer	call	relu
global_avg_pool	followed_by	dropout
_enas_layer	call	_conv_branch
_model	call	conv2d
create_weight	followed_by	create_weight
boolean_mask	followed_by	reshape
get_variable	followed_by	get_variable
_pool_branch	call	relu
_enas_layer	call	logical_or
_model	call	_factorized_reduction
_enas_layer	call	variable_scope
fused_batch_norm	followed_by	control_dependencies
_conv_branch	call	create_weight
relu	followed_by	conv2d
_factorized_reduction	call	_get_strides
transpose	followed_by	reshape
_factorized_reduction	call	avg_pool
reshape	followed_by	concat
variable_scope	has_arg0	branch_4
_model	call	matmul
relu	followed_by	variable_scope
_enas_layer	call	add_n
equal	followed_by	case
concat	followed_by	batch_norm
_enas_layer	call	concat
device	followed_by	transpose
_fixed_layer	call	variable_scope
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	device
_pool_branch	call	variable_scope
enas.cifar10.general_child.GeneralChild.build_valid_rl	call	to_int32
variable_scope	followed_by	_pool_branch
avg_pool	followed_by	variable_scope
_factorized_reduction	followed_by	global_avg_pool
get_variable	followed_by	fused_batch_norm
shape	followed_by	reshape
conv2d	has_arg0	SAME
variable_scope	has_arg0	final_conv
reshape	followed_by	conv2d
batch_norm_with_mask	call	boolean_mask
argmax	has_axis	1
relu	call	where
batch_norm	followed_by	create_weight
get_variable	has_arg0	moving_mean
_pool_branch	call	conv2d
_factorized_reduction	call	batch_norm
variable_scope	has_arg0	branch_0
_conv_branch	call	conv2d
create_weight	followed_by	relu
variable_scope	has_arg0	pool
_factorized_reduction	call	_get_C
reshape	followed_by	variable_scope
_model	call	_fixed_layer
split	has_arg0	 
main	call	size
size	followed_by	size
split	followed_by	size
main	call	split
split	followed_by	split
.process	call	main
__init__	call	shuffle_batch
__init__	call	map_fn
shape	followed_by	batch
device	followed_by	shape
Variable	has_arg0	0
__init__	followed_by	Variable
transpose	followed_by	shape
__init__	call	transpose
__init__	call	batch
batch	followed_by	transpose
__init__	call	shape
__init__	call	device
Variable	has_name	global_step
enas.cifar10.micro_child.MicroChild.__init__	call	Variable
device	has_arg0	/cpu:0
map_fn	followed_by	transpose
shape	followed_by	shuffle_batch
enas.cifar10.micro_child.MicroChild.__init__	call	__init__
shuffle_batch	followed_by	map_fn
