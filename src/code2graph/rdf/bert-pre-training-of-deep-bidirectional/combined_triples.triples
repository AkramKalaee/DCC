bert-master.tokenization_test.TokenizationTest.test_full_tokenizer	call	tokenize
FullTokenizer.__init__	call	WordpieceTokenizer.__init__
tokenize	followed_by	tokenize
tokenize	followed_by	convert_tokens_to_ids
FullTokenizer.__init__	call	load_vocab
convert_tokens_to_ids	call	convert_by_vocab
tokenize	call	tokenize
FullTokenizer.__init__	followed_by	tokenize
BasicTokenizer.__init__	followed_by	WordpieceTokenizer.__init__
load_vocab	followed_by	BasicTokenizer.__init__
bert-master.tokenization_test.TokenizationTest.test_full_tokenizer	call	FullTokenizer.__init__
bert-master.tokenization_test.TokenizationTest.test_full_tokenizer	call	convert_tokens_to_ids
FullTokenizer.__init__	call	BasicTokenizer.__init__
bert-master.optimization_test.OptimizationTest.test_adam	call	range
bert-master.optimization_test.OptimizationTest.test_adam	call	AdamWeightDecayOptimizer.__init__
range	followed_by	run
AdamWeightDecayOptimizer.__init__	followed_by	apply_gradients
assign	followed_by	assign
bert-master.optimization_test.OptimizationTest.test_adam	call	get_variable
group	has_arg0	1
apply_gradients	call	get_variable
apply_gradients	call	multiply
bert-master.optimization_test.OptimizationTest.test_adam	call	get_or_create_global_step
range	has_arg0	100
apply_gradients	call	group
_get_variable_name	followed_by	get_variable
sqrt	followed_by	_do_use_weight_decay
run	followed_by	range
bert-master.optimization_test.OptimizationTest.test_adam	call	gradients
run	followed_by	run
apply_gradients	call	sqrt
_get_variable_name	call	group
multiply	followed_by	sqrt
reduce_mean	call	square
get_variable	followed_by	constant
multiply	followed_by	multiply
bert-master.optimization_test.OptimizationTest.test_adam	call	group
constant	followed_by	reduce_mean
gradients	followed_by	get_or_create_global_step
bert-master.optimization_test.OptimizationTest.test_adam	call	reduce_mean
reduce_mean	followed_by	trainable_variables
bert-master.optimization_test.OptimizationTest.test_adam	call	run
apply_gradients	call	assign
get_variable	followed_by	get_variable
get_variable	followed_by	multiply
trainable_variables	followed_by	gradients
assign	followed_by	group
bert-master.optimization_test.OptimizationTest.test_adam	call	constant
bert-master.optimization_test.OptimizationTest.test_adam	call	apply_gradients
get_or_create_global_step	followed_by	AdamWeightDecayOptimizer.__init__
apply_gradients	followed_by	group
group	followed_by	run
_do_use_weight_decay	followed_by	assign
get_variable	has_arg0	w
apply_gradients	call	_do_use_weight_decay
multiply	call	square
bert-master.optimization_test.OptimizationTest.test_adam	call	trainable_variables
apply_gradients	call	_get_variable_name
tokenize	call	_clean_text
_clean_text	call	_is_whitespace
BasicTokenizer.__init__	followed_by	tokenize
bert-master.tokenization_test.TokenizationTest.test_basic_tokenizer_lower	call	tokenize
tokenize	call	whitespace_tokenize
tokenize	call	_run_strip_accents
tokenize	followed_by	tokenize
bert-master.tokenization_test.TokenizationTest.test_basic_tokenizer_lower	call	BasicTokenizer.__init__
whitespace_tokenize	followed_by	_run_strip_accents
tokenize	call	convert_to_unicode
_tokenize_chinese_chars	followed_by	whitespace_tokenize
convert_to_unicode	followed_by	_clean_text
_clean_text	followed_by	_tokenize_chinese_chars
_tokenize_chinese_chars	call	_is_chinese_char
_run_split_on_punc	followed_by	whitespace_tokenize
tokenize	call	_run_split_on_punc
tokenize	call	_tokenize_chinese_chars
whitespace_tokenize	call	split
_run_split_on_punc	call	List
_run_strip_accents	followed_by	_run_split_on_punc
bert-master.run_classifier.MrpcProcessor.get_train_examples	call	_create_examples
_create_examples	call	InputExample.__init__
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	convert_to_unicode
convert_to_unicode	followed_by	InputExample.__init__
_create_examples	followed_by	_read_tsv
bert-master.run_classifier.MrpcProcessor.get_train_examples	call	_read_tsv
from_json_file	followed_by	FullTokenizer.__init__
file_based_input_fn_builder	followed_by	get_test_examples
convert_examples_to_features	followed_by	close
_compute_softmax	call	exp
shuffle	followed_by	model_fn_builder
main	call	create_training_instances
create_training_instances	call	List
get_dev_examples	followed_by	convert_examples_to_features
main	call	get_dev_examples
read_squad_examples	call	load
write_predictions	call	_get_best_indexes
.bert-master.optimization_test	call	main
is_whitespace	followed_by	whitespace_tokenize
main	call	validate_case_matches_checkpoint
file_based_convert_examples_to_features	call	close
convert_single_example	call	InputFeatures.__init__
run	followed_by	FullTokenizer.__init__
whitespace_tokenize	followed_by	_run_strip_accents
main	call	convert_examples_to_features
input_fn_builder	followed_by	write_predictions
convert_single_example	call	_truncate_seq_pair
get_test_examples	followed_by	file_based_convert_examples_to_features
FullTokenizer.__init__	call	load_vocab
write_instance_to_example_files	call	Example
InputFeatures.__init__	followed_by	_truncate_seq_pair
round	has_arg0	6
create_training_instances	call	range
model_fn_builder	followed_by	FeatureWriter.__init__
create_tokenizer_from_hub_module	call	run
write_instance_to_example_files	call	create_float_feature
input_fn_builder	followed_by	round
create_float_feature	call	Feature
Session	followed_by	run
write_predictions	call	_compute_softmax
convert_examples_to_features	call	range
create_float_feature	followed_by	create_int_feature
get_final_text	call	BasicTokenizer.__init__
tokenize	call	_tokenize_chinese_chars
from_dict	call	BertConfig.__init__
Example	call	Features
split	followed_by	get_final_text
create_int_feature	followed_by	Example
from_json_file	followed_by	validate_flags_or_throw
create_tokenizer_from_hub_module	call	FullTokenizer.__init__
FloatList	call	List
_tokenize_chinese_chars	call	_is_chinese_char
main	call	round
tokenize	followed_by	_strip_spaces
main	call	write_instance_to_example_files
main	call	validate_flags_or_throw
BasicTokenizer.__init__	followed_by	WordpieceTokenizer.__init__
main	call	write_predictions
create_int_feature	followed_by	create_float_feature
main	followed_by	main
file_based_convert_examples_to_features	call	Example
get_train_examples	followed_by	model_fn_builder
from_json_file	followed_by	model_fn_builder
printable_text	followed_by	printable_text
main	call	get_train_examples
FullTokenizer.__init__	followed_by	get_train_examples
get_dev_examples	followed_by	file_based_convert_examples_to_features
main	call	create_tokenizer_from_hub_module
convert_examples_to_features	call	convert_single_example
main	call	from_json_file
model_fn_builder	followed_by	input_fn_builder
create_tokenizer_from_hub_module	followed_by	get_train_examples
load_vocab	followed_by	BasicTokenizer.__init__
_get_best_indexes	followed_by	split
from_json_file	call	from_dict
get_labels	followed_by	create_tokenizer_from_hub_module
Example	followed_by	close
input_fn_builder	followed_by	get_dev_examples
validate_flags_or_throw	call	validate_case_matches_checkpoint
main	call	close
range	followed_by	range
create_training_instances	call	shuffle
write_instance_to_example_files	call	close
get_final_text	followed_by	_compute_softmax
main	call	shuffle
FullTokenizer.__init__	call	WordpieceTokenizer.__init__
convert_to_unicode	followed_by	_clean_text
create_instances_from_document	followed_by	shuffle
tokenize	call	_clean_text
_check_is_max_context	call	min
tokenize	call	whitespace_tokenize
create_training_instances	call	create_instances_from_document
read_squad_examples	call	whitespace_tokenize
_run_split_on_punc	followed_by	whitespace_tokenize
List	followed_by	List
main	call	file_based_convert_examples_to_features
tokenize	call	_run_strip_accents
convert_examples_to_features	followed_by	input_fn_builder
printable_text	followed_by	InputFeatures.__init__
_improve_answer_span	followed_by	range
convert_examples_to_features	followed_by	model_fn_builder
read_squad_examples	call	is_whitespace
read_squad_examples	followed_by	FeatureWriter.__init__
convert_single_example	followed_by	create_int_feature
_improve_answer_span	call	range
read_squad_examples	followed_by	shuffle
FullTokenizer.__init__	followed_by	read_squad_examples
main	call	FullTokenizer.__init__
create_training_instances	followed_by	write_instance_to_example_files
convert_examples_to_features	call	_improve_answer_span
_clean_text	followed_by	_tokenize_chinese_chars
write_predictions	call	split
whitespace_tokenize	followed_by	SquadExample.__init__
_check_is_max_context	followed_by	printable_text
input_fn_builder	followed_by	input_fn_builder
Int64List	call	List
FullTokenizer.__init__	followed_by	create_training_instances
BasicTokenizer.__init__	followed_by	tokenize
group	has_arg0	1
range	followed_by	create_instances_from_document
file_based_convert_examples_to_features	call	create_int_feature
main	call	read_examples
FullTokenizer.__init__	followed_by	read_examples
input_fn_builder	followed_by	get_test_examples
validate_case_matches_checkpoint	call	group
_tokenize_chinese_chars	followed_by	whitespace_tokenize
file_based_input_fn_builder	followed_by	get_dev_examples
get_final_text	call	tokenize
get_labels	followed_by	FullTokenizer.__init__
create_int_feature	followed_by	create_int_feature
model_fn_builder	followed_by	convert_examples_to_features
main	call	FeatureWriter.__init__
file_based_convert_examples_to_features	call	convert_single_example
_clean_text	call	_is_whitespace
_run_strip_accents	followed_by	_run_split_on_punc
create_int_feature	call	Feature
main	call	read_squad_examples
input_fn_builder	followed_by	read_squad_examples
List	followed_by	create_int_feature
main	call	file_based_input_fn_builder
tokenize	call	_run_split_on_punc
read_examples	followed_by	convert_examples_to_features
FeatureWriter.__init__	followed_by	convert_examples_to_features
main	call	input_fn_builder
_truncate_seq_pair	followed_by	printable_text
write_instance_to_example_files	call	create_int_feature
range	followed_by	_check_is_max_context
read_squad_examples	call	SquadExample.__init__
Example	followed_by	printable_text
printable_text	followed_by	close
whitespace_tokenize	call	split
close	followed_by	input_fn_builder
main	call	model_fn_builder
shuffle	followed_by	List
write_instance_to_example_files	call	printable_text
main	call	get_test_examples
main	call	get_labels
from_json_file	followed_by	get_labels
_run_split_on_punc	call	List
model_fn_builder	followed_by	file_based_convert_examples_to_features
List	followed_by	range
close	call	close
tokenize	call	convert_to_unicode
write_predictions	call	get_final_text
convert_examples_to_features	call	_truncate_seq_pair
get_final_text	call	_strip_spaces
write_instance_to_example_files	call	List
convert_examples_to_features	call	printable_text
file_based_convert_examples_to_features	followed_by	file_based_input_fn_builder
FullTokenizer.__init__	call	BasicTokenizer.__init__
validate_flags_or_throw	followed_by	FullTokenizer.__init__
Feature	call	Int64List
Feature	call	FloatList
convert_single_example	call	printable_text
_get_best_indexes	followed_by	_get_best_indexes
_strip_spaces	followed_by	_strip_spaces
validate_case_matches_checkpoint	followed_by	from_json_file
load	followed_by	is_whitespace
_get_best_indexes	call	range
create_tokenizer_from_hub_module	call	Session
convert_examples_to_features	call	_check_is_max_context
convert_examples_to_features	call	InputFeatures.__init__
_clean_text	followed_by	_tokenize_chinese_chars
group	has_arg0	1
convert_examples_to_features	call	_truncate_seq_pair
create_float_feature	call	Feature
write_instance_to_example_files	call	close
Feature	call	FloatList
close	followed_by	input_fn_builder
main	call	round
main	call	write_predictions
from_json_file	call	from_dict
convert_examples_to_features	followed_by	input_fn_builder
model_fn_builder	followed_by	FeatureWriter.__init__
main	call	get_test_examples
printable_text	followed_by	printable_text
get_final_text	call	_strip_spaces
whitespace_tokenize	followed_by	_run_strip_accents
main	call	model_fn_builder
main	call	validate_case_matches_checkpoint
get_dev_examples	followed_by	file_based_convert_examples_to_features
main	followed_by	main
create_training_instances	call	range
convert_examples_to_features	call	_check_is_max_context
main	call	shuffle
BasicTokenizer.__init__	followed_by	tokenize
Int64List	call	List
range	followed_by	range
FullTokenizer.__init__	followed_by	get_train_examples
_get_best_indexes	followed_by	split
file_based_convert_examples_to_features	call	convert_single_example
create_tokenizer_from_hub_module	call	run
model_fn_builder	followed_by	input_fn_builder
main	call	from_json_file
BasicTokenizer.__init__	followed_by	WordpieceTokenizer.__init__
main	call	create_training_instances
main	call	create_tokenizer_from_hub_module
input_fn_builder	followed_by	input_fn_builder
FullTokenizer.__init__	call	WordpieceTokenizer.__init__
create_int_feature	followed_by	create_int_feature
main	call	read_squad_examples
printable_text	followed_by	InputFeatures.__init__
convert_examples_to_features	followed_by	close
round	has_arg0	6
convert_single_example	call	InputFeatures.__init__
Example	call	Features
write_predictions	call	get_final_text
validate_flags_or_throw	followed_by	FullTokenizer.__init__
main	call	FullTokenizer.__init__
load	followed_by	is_whitespace
shuffle	followed_by	List
create_instances_from_document	followed_by	shuffle
input_fn_builder	followed_by	read_squad_examples
get_final_text	call	BasicTokenizer.__init__
main	call	close
_check_is_max_context	call	min
create_float_feature	followed_by	create_int_feature
file_based_input_fn_builder	followed_by	get_dev_examples
convert_to_unicode	followed_by	_clean_text
_compute_softmax	call	exp
_run_split_on_punc	followed_by	whitespace_tokenize
main	call	get_labels
FullTokenizer.__init__	call	BasicTokenizer.__init__
convert_single_example	followed_by	create_int_feature
write_instance_to_example_files	call	Example
validate_case_matches_checkpoint	call	group
FullTokenizer.__init__	followed_by	read_examples
convert_examples_to_features	call	printable_text
create_training_instances	call	shuffle
input_fn_builder	followed_by	get_dev_examples
from_json_file	followed_by	validate_flags_or_throw
create_int_feature	followed_by	Example
create_training_instances	followed_by	write_instance_to_example_files
from_json_file	followed_by	model_fn_builder
file_based_convert_examples_to_features	call	create_int_feature
_truncate_seq_pair	followed_by	printable_text
_clean_text	call	_is_whitespace
main	call	input_fn_builder
whitespace_tokenize	call	split
write_predictions	call	_get_best_indexes
validate_case_matches_checkpoint	followed_by	from_json_file
shuffle	followed_by	model_fn_builder
FullTokenizer.__init__	followed_by	create_training_instances
FloatList	call	List
write_instance_to_example_files	call	printable_text
get_labels	followed_by	create_tokenizer_from_hub_module
read_squad_examples	call	is_whitespace
tokenize	call	_clean_text
load_vocab	followed_by	BasicTokenizer.__init__
close	call	close
tokenize	call	whitespace_tokenize
_get_best_indexes	followed_by	_get_best_indexes
List	followed_by	List
printable_text	followed_by	close
get_train_examples	followed_by	model_fn_builder
read_squad_examples	call	SquadExample.__init__
_tokenize_chinese_chars	call	_is_chinese_char
convert_single_example	call	printable_text
read_squad_examples	call	whitespace_tokenize
range	followed_by	_check_is_max_context
model_fn_builder	followed_by	convert_examples_to_features
from_json_file	followed_by	FullTokenizer.__init__
main	call	read_examples
tokenize	call	_run_strip_accents
InputFeatures.__init__	followed_by	_truncate_seq_pair
convert_examples_to_features	followed_by	model_fn_builder
run	followed_by	FullTokenizer.__init__
create_tokenizer_from_hub_module	call	Session
from_json_file	followed_by	get_labels
_get_best_indexes	call	range
input_fn_builder	followed_by	round
create_int_feature	call	Feature
main	call	validate_flags_or_throw
_check_is_max_context	followed_by	printable_text
main	call	file_based_convert_examples_to_features
_run_strip_accents	followed_by	_run_split_on_punc
file_based_convert_examples_to_features	call	Example
_improve_answer_span	call	range
validate_flags_or_throw	call	validate_case_matches_checkpoint
List	followed_by	create_int_feature
read_squad_examples	followed_by	shuffle
Feature	call	Int64List
get_test_examples	followed_by	file_based_convert_examples_to_features
get_final_text	call	tokenize
tokenize	followed_by	_strip_spaces
Session	followed_by	run
create_tokenizer_from_hub_module	call	FullTokenizer.__init__
write_instance_to_example_files	call	create_int_feature
file_based_convert_examples_to_features	followed_by	file_based_input_fn_builder
input_fn_builder	followed_by	write_predictions
convert_examples_to_features	call	range
read_examples	followed_by	convert_examples_to_features
create_training_instances	call	List
FullTokenizer.__init__	call	load_vocab
main	call	file_based_input_fn_builder
write_instance_to_example_files	call	List
input_fn_builder	followed_by	get_test_examples
FeatureWriter.__init__	followed_by	convert_examples_to_features
write_instance_to_example_files	call	create_float_feature
get_labels	followed_by	FullTokenizer.__init__
convert_single_example	call	_truncate_seq_pair
model_fn_builder	followed_by	file_based_convert_examples_to_features
Example	followed_by	close
main	call	FeatureWriter.__init__
main	call	write_instance_to_example_files
tokenize	call	_run_split_on_punc
FullTokenizer.__init__	followed_by	read_squad_examples
.bert-master.modeling_test	call	main
file_based_input_fn_builder	followed_by	get_test_examples
from_dict	call	BertConfig.__init__
create_int_feature	followed_by	create_float_feature
List	followed_by	range
whitespace_tokenize	followed_by	SquadExample.__init__
read_squad_examples	followed_by	FeatureWriter.__init__
write_predictions	call	_compute_softmax
get_dev_examples	followed_by	convert_examples_to_features
split	followed_by	get_final_text
main	call	get_dev_examples
read_squad_examples	call	load
main	call	get_train_examples
is_whitespace	followed_by	whitespace_tokenize
_strip_spaces	followed_by	_strip_spaces
tokenize	call	_tokenize_chinese_chars
_improve_answer_span	followed_by	range
main	call	convert_examples_to_features
file_based_convert_examples_to_features	call	close
_run_split_on_punc	call	List
range	followed_by	create_instances_from_document
Example	followed_by	printable_text
convert_examples_to_features	call	convert_single_example
convert_examples_to_features	call	InputFeatures.__init__
create_tokenizer_from_hub_module	followed_by	get_train_examples
_tokenize_chinese_chars	followed_by	whitespace_tokenize
convert_examples_to_features	call	_improve_answer_span
tokenize	call	convert_to_unicode
get_final_text	followed_by	_compute_softmax
create_training_instances	call	create_instances_from_document
write_predictions	call	split
transformer_model	call	reshape_to_matrix
get_sequence_output	followed_by	get_pooled_output
embedding_postprocessor	followed_by	variable_scope
attention_layer	followed_by	concat
layer_norm	followed_by	reshape_from_matrix
create_attention_mask_from_input_mask	call	ones
squeeze	has_axis	1
embedding_postprocessor	call	matmul
embedding_lookup	call	get_variable
softmax	followed_by	dropout
expand_dims	followed_by	cast
assert_rank	call	get_variable_scope
ids_tensor	followed_by	BertConfig.__init__
attention_layer	call	expand_dims
get_shape_list	followed_by	ones
embedding_postprocessor	call	get_shape_list
BertModel.__init__	call	dense
dropout	followed_by	layer_norm
matmul	followed_by	transpose
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	get_pooled_output
embedding_postprocessor	call	control_dependencies
create_attention_mask_from_input_mask	followed_by	transformer_model
embedding_lookup	call	one_hot
reshape_to_matrix	followed_by	dense
variable_scope	followed_by	embedding_lookup
variable_scope	has_arg0	intermediate
embedding_postprocessor	call	slice
transformer_model	call	range
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	BertModel.__init__
transformer_model	call	reshape_from_matrix
cast	followed_by	ones
concat	followed_by	variable_scope
dense	followed_by	dropout
transpose	followed_by	reshape
multiply	followed_by	expand_dims
transpose	followed_by	matmul
zeros	has_dtype	tf.int32
one_hot	followed_by	matmul
attention_layer	call	reshape_to_matrix
BertModel.__init__	call	variable_scope
variable_scope	has_arg0	encoder
embedding_postprocessor	call	assert_less_equal
variable_scope	has_arg0	pooler
BertModel.__init__	call	get_shape_list
transpose_for_scores	followed_by	matmul
transpose_for_scores	call	transpose
range	followed_by	variable_scope
cast	call	reshape
variable_scope	has_arg0	output
get_activation	followed_by	variable_scope
reshape_from_matrix	call	reshape
BertModel.__init__	call	zeros
reshape	followed_by	reshape
attention_layer	call	transpose
variable_scope	followed_by	dense
dense	followed_by	variable_scope
get_variable	followed_by	reshape
reshape_to_matrix	call	reshape
embedding_postprocessor	call	reshape
layer_norm_and_dropout	call	layer_norm
embedding_postprocessor	call	layer_norm_and_dropout
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	get_all_encoder_layers
ids_tensor	followed_by	ids_tensor
ids_tensor	call	range
get_shape_list	followed_by	reshape_to_matrix
get_variable	followed_by	slice
variable_scope	followed_by	create_attention_mask_from_input_mask
embedding_postprocessor	call	get_variable
get_shape_list	call	assert_rank
reshape	followed_by	one_hot
get_shape_list	followed_by	get_variable
variable_scope	followed_by	squeeze
attention_layer	call	get_shape_list
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	get_embedding_output
layer_norm	call	layer_norm
matmul	followed_by	gather
transformer_model	followed_by	get_activation
reshape_to_matrix	followed_by	range
squeeze	followed_by	dense
embedding_lookup	call	reshape
create_attention_mask_from_input_mask	call	get_shape_list
get_pooled_output	followed_by	get_all_encoder_layers
range	followed_by	reshape
transpose_for_scores	followed_by	transpose_for_scores
layer_norm	followed_by	dropout
cast	followed_by	softmax
embedding_lookup	call	expand_dims
variable_scope	has_default_name	bert
BertModel.__init__	call	ones
transformer_model	call	attention_layer
BertModel.__init__	call	create_attention_mask_from_input_mask
ones	has_dtype	tf.int32
get_shape_list	followed_by	get_shape_list
reshape	followed_by	transpose
create_attention_mask_from_input_mask	call	cast
dropout	call	dropout
attention_layer	call	matmul
multiply	call	sqrt
dropout	followed_by	reshape
get_shape_list	call	shape
matmul	followed_by	multiply
attention_layer	call	reshape
ones	followed_by	zeros
attention_layer	call	dense
transformer_model	call	variable_scope
variable_scope	has_arg0	attention
get_shape_list	followed_by	cast
BertModel.__init__	call	get_activation
embedding_lookup	call	matmul
variable_scope	has_arg0	self
embedding_lookup	call	gather
gather	followed_by	get_shape_list
get_shape_list	followed_by	reshape
BertModel.__init__	call	embedding_postprocessor
transformer_model	call	get_shape_list
attention_layer	call	cast
range	followed_by	constant
transformer_model	call	dropout
embedding_postprocessor	call	range
assert_rank	followed_by	shape
embedding_lookup	followed_by	embedding_postprocessor
dense	followed_by	transpose_for_scores
reshape_to_matrix	followed_by	reshape_to_matrix
embedding_postprocessor	call	one_hot
BertConfig.__init__	followed_by	BertModel.__init__
attention_layer	call	multiply
variable_scope	followed_by	attention_layer
reshape	followed_by	assert_less_equal
attention_layer	call	softmax
control_dependencies	followed_by	get_variable
slice	followed_by	range
transformer_model	call	layer_norm
expand_dims	followed_by	get_variable
transpose_for_scores	call	reshape
BertModel.__init__	call	squeeze
layer_norm	followed_by	variable_scope
matmul	followed_by	reshape
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	BertConfig.__init__
assert_less_equal	followed_by	control_dependencies
ones	has_dtype	tf.float32
layer_norm_and_dropout	call	dropout
zeros	followed_by	variable_scope
BertModel.__init__	call	embedding_lookup
variable_scope	followed_by	variable_scope
dense	followed_by	dense
reshape_from_matrix	followed_by	reshape_from_matrix
ids_tensor	call	constant
BertModel.__init__	call	transformer_model
attention_layer	call	transpose_for_scores
reshape_from_matrix	call	get_shape_list
BertModel.__init__	followed_by	get_embedding_output
transformer_model	call	dense
variable_scope	has_arg0	embeddings
transformer_model	call	concat
reshape	followed_by	layer_norm_and_dropout
get_embedding_output	followed_by	get_sequence_output
embedding_lookup	call	get_shape_list
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	ids_tensor
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	get_sequence_output
attention_layer	call	dropout
bert-master.tokenization_test.TokenizationTest.test_wordpiece_tokenizer	call	tokenize
WordpieceTokenizer.__init__	followed_by	tokenize
whitespace_tokenize	followed_by	List
convert_to_unicode	followed_by	whitespace_tokenize
bert-master.tokenization_test.TokenizationTest.test_wordpiece_tokenizer	call	WordpieceTokenizer.__init__
tokenize	call	whitespace_tokenize
tokenize	followed_by	tokenize
whitespace_tokenize	call	split
tokenize	call	convert_to_unicode
tokenize	call	List
bert-master.tokenization.FullTokenizer.convert_ids_to_tokens	call	convert_by_vocab
bert-master.modeling_test.BertModelTest.test_config_to_json_string	call	BertConfig.__init__
bert-master.modeling_test.BertModelTest.test_config_to_json_string	call	to_json_string
BertConfig.__init__	followed_by	to_json_string
to_json_string	call	to_dict
bert-master.run_classifier.XnliProcessor.get_dev_examples	call	convert_to_unicode
convert_to_unicode	followed_by	convert_to_unicode
bert-master.run_classifier.XnliProcessor.get_dev_examples	call	InputExample.__init__
convert_to_unicode	followed_by	InputExample.__init__
bert-master.run_classifier.XnliProcessor.get_dev_examples	call	_read_tsv
_read_tsv	followed_by	convert_to_unicode
whitespace_tokenize	followed_by	_run_strip_accents
_clean_text	call	_is_whitespace
bert-master.tokenization_test.TokenizationTest.test_basic_tokenizer_no_lower	call	tokenize
_run_split_on_punc	followed_by	whitespace_tokenize
tokenize	call	convert_to_unicode
_tokenize_chinese_chars	followed_by	whitespace_tokenize
convert_to_unicode	followed_by	_clean_text
tokenize	call	_run_strip_accents
whitespace_tokenize	call	split
tokenize	call	whitespace_tokenize
_clean_text	followed_by	_tokenize_chinese_chars
_run_strip_accents	followed_by	_run_split_on_punc
_tokenize_chinese_chars	call	_is_chinese_char
tokenize	call	_tokenize_chinese_chars
bert-master.tokenization_test.TokenizationTest.test_basic_tokenizer_no_lower	call	BasicTokenizer.__init__
tokenize	call	_clean_text
_run_split_on_punc	call	List
BasicTokenizer.__init__	followed_by	tokenize
tokenize	call	_run_split_on_punc
convert_to_unicode	followed_by	InputExample.__init__
_create_examples	call	InputExample.__init__
bert-master.run_classifier.ColaProcessor.get_test_examples	call	_read_tsv
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	convert_to_unicode
_create_examples	followed_by	_read_tsv
bert-master.run_classifier.ColaProcessor.get_test_examples	call	_create_examples
__repr__	call	printable_text
printable_text	followed_by	printable_text
bert-master.run_squad.SquadExample.__str__	call	__repr__
create_training_instances	call	List
model_fn_builder	followed_by	convert_examples_to_features
model_fn_builder	followed_by	file_based_convert_examples_to_features
main	call	get_test_examples
main	call	get_dev_examples
Feature	call	FloatList
tokenize	call	convert_to_unicode
main	call	shuffle
_run_strip_accents	followed_by	_run_split_on_punc
shuffle	followed_by	List
from_json_file	call	from_dict
convert_examples_to_features	call	_improve_answer_span
write_predictions	call	split
write_instance_to_example_files	call	close
from_json_file	followed_by	get_labels
_truncate_seq_pair	followed_by	printable_text
FullTokenizer.__init__	followed_by	get_train_examples
FullTokenizer.__init__	followed_by	read_examples
_strip_spaces	followed_by	_strip_spaces
split	followed_by	get_final_text
validate_flags_or_throw	call	validate_case_matches_checkpoint
range	followed_by	_check_is_max_context
load	followed_by	is_whitespace
input_fn_builder	followed_by	round
_tokenize_chinese_chars	call	_is_chinese_char
FullTokenizer.__init__	followed_by	create_training_instances
_get_best_indexes	followed_by	_get_best_indexes
file_based_convert_examples_to_features	call	close
input_fn_builder	followed_by	input_fn_builder
get_test_examples	followed_by	file_based_convert_examples_to_features
Example	call	Features
main	call	FullTokenizer.__init__
create_tokenizer_from_hub_module	call	run
convert_examples_to_features	call	printable_text
whitespace_tokenize	followed_by	_run_strip_accents
create_int_feature	followed_by	create_float_feature
InputFeatures.__init__	followed_by	_truncate_seq_pair
_improve_answer_span	call	range
validate_case_matches_checkpoint	followed_by	from_json_file
Session	followed_by	run
write_instance_to_example_files	call	printable_text
whitespace_tokenize	followed_by	SquadExample.__init__
get_labels	followed_by	create_tokenizer_from_hub_module
load_vocab	followed_by	BasicTokenizer.__init__
FullTokenizer.__init__	call	BasicTokenizer.__init__
write_predictions	call	_get_best_indexes
main	call	create_tokenizer_from_hub_module
write_predictions	call	get_final_text
main	followed_by	main
create_instances_from_document	followed_by	shuffle
tokenize	call	_clean_text
create_int_feature	followed_by	create_int_feature
convert_single_example	call	InputFeatures.__init__
main	call	write_predictions
printable_text	followed_by	InputFeatures.__init__
tokenize	call	_run_strip_accents
close	call	close
_clean_text	call	_is_whitespace
file_based_input_fn_builder	followed_by	get_dev_examples
main	call	close
create_float_feature	followed_by	create_int_feature
from_json_file	followed_by	validate_flags_or_throw
get_dev_examples	followed_by	file_based_convert_examples_to_features
convert_examples_to_features	followed_by	input_fn_builder
main	call	model_fn_builder
_get_best_indexes	followed_by	split
FloatList	call	List
main	call	file_based_convert_examples_to_features
group	has_arg0	1
write_predictions	call	_compute_softmax
get_final_text	call	BasicTokenizer.__init__
FullTokenizer.__init__	call	load_vocab
input_fn_builder	followed_by	read_squad_examples
main	call	validate_flags_or_throw
close	followed_by	input_fn_builder
convert_single_example	call	_truncate_seq_pair
tokenize	followed_by	_strip_spaces
convert_examples_to_features	followed_by	close
convert_single_example	followed_by	create_int_feature
write_instance_to_example_files	call	create_float_feature
printable_text	followed_by	printable_text
read_examples	followed_by	convert_examples_to_features
convert_examples_to_features	call	_check_is_max_context
_run_split_on_punc	followed_by	whitespace_tokenize
write_instance_to_example_files	call	List
_clean_text	followed_by	_tokenize_chinese_chars
get_labels	followed_by	FullTokenizer.__init__
Int64List	call	List
main	call	read_examples
is_whitespace	followed_by	whitespace_tokenize
create_float_feature	call	Feature
create_tokenizer_from_hub_module	followed_by	get_train_examples
create_int_feature	followed_by	Example
from_dict	call	BertConfig.__init__
validate_case_matches_checkpoint	call	group
file_based_convert_examples_to_features	call	convert_single_example
get_train_examples	followed_by	model_fn_builder
read_squad_examples	call	whitespace_tokenize
create_int_feature	call	Feature
main	call	read_squad_examples
Feature	call	Int64List
main	call	convert_examples_to_features
file_based_convert_examples_to_features	call	Example
get_dev_examples	followed_by	convert_examples_to_features
FeatureWriter.__init__	followed_by	convert_examples_to_features
main	call	get_labels
convert_single_example	call	printable_text
.bert-master.tokenization_test	call	main
create_training_instances	followed_by	write_instance_to_example_files
List	followed_by	List
main	call	from_json_file
printable_text	followed_by	close
from_json_file	followed_by	FullTokenizer.__init__
main	call	validate_case_matches_checkpoint
create_tokenizer_from_hub_module	call	Session
range	followed_by	range
BasicTokenizer.__init__	followed_by	tokenize
read_squad_examples	call	SquadExample.__init__
input_fn_builder	followed_by	write_predictions
convert_examples_to_features	call	convert_single_example
main	call	create_training_instances
round	has_arg0	6
get_final_text	followed_by	_compute_softmax
create_tokenizer_from_hub_module	call	FullTokenizer.__init__
shuffle	followed_by	model_fn_builder
List	followed_by	range
main	call	file_based_input_fn_builder
run	followed_by	FullTokenizer.__init__
main	call	input_fn_builder
write_instance_to_example_files	call	Example
main	call	get_train_examples
create_training_instances	call	create_instances_from_document
convert_examples_to_features	followed_by	model_fn_builder
get_final_text	call	tokenize
read_squad_examples	call	is_whitespace
main	call	FeatureWriter.__init__
file_based_input_fn_builder	followed_by	get_test_examples
create_training_instances	call	range
FullTokenizer.__init__	call	WordpieceTokenizer.__init__
_tokenize_chinese_chars	followed_by	whitespace_tokenize
model_fn_builder	followed_by	FeatureWriter.__init__
model_fn_builder	followed_by	input_fn_builder
range	followed_by	create_instances_from_document
tokenize	call	whitespace_tokenize
write_instance_to_example_files	call	create_int_feature
Example	followed_by	close
convert_examples_to_features	call	_truncate_seq_pair
from_json_file	followed_by	model_fn_builder
input_fn_builder	followed_by	get_dev_examples
Example	followed_by	printable_text
BasicTokenizer.__init__	followed_by	WordpieceTokenizer.__init__
tokenize	call	_tokenize_chinese_chars
_run_split_on_punc	call	List
create_training_instances	call	shuffle
read_squad_examples	followed_by	shuffle
_improve_answer_span	followed_by	range
file_based_convert_examples_to_features	followed_by	file_based_input_fn_builder
_compute_softmax	call	exp
whitespace_tokenize	call	split
main	call	round
_get_best_indexes	call	range
get_final_text	call	_strip_spaces
file_based_convert_examples_to_features	call	create_int_feature
main	call	write_instance_to_example_files
validate_flags_or_throw	followed_by	FullTokenizer.__init__
input_fn_builder	followed_by	get_test_examples
_check_is_max_context	call	min
convert_examples_to_features	call	InputFeatures.__init__
read_squad_examples	followed_by	FeatureWriter.__init__
List	followed_by	create_int_feature
FullTokenizer.__init__	followed_by	read_squad_examples
tokenize	call	_run_split_on_punc
_check_is_max_context	followed_by	printable_text
convert_to_unicode	followed_by	_clean_text
read_squad_examples	call	load
convert_examples_to_features	call	range
convert_tokens_to_ids	call	convert_by_vocab
bert-master.tokenization_test.TokenizationTest.test_convert_tokens_to_ids	call	convert_tokens_to_ids
bert-master.tokenization_test.TokenizationTest.test_is_whitespace	call	_is_whitespace
_is_whitespace	followed_by	_is_whitespace
_is_punctuation	followed_by	_is_punctuation
bert-master.tokenization_test.TokenizationTest.test_is_punctuation	call	_is_punctuation
_create_examples	followed_by	_read_tsv
bert-master.run_classifier.MrpcProcessor.get_dev_examples	call	_create_examples
convert_to_unicode	followed_by	InputExample.__init__
_create_examples	call	InputExample.__init__
bert-master.run_classifier.MrpcProcessor.get_dev_examples	call	_read_tsv
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	convert_to_unicode
bert-master.tokenization.convert_ids_to_tokens	call	convert_by_vocab
get_unreachable_ops	call	flatten_recursive
assert_all_tensors_reachable	call	get_unreachable_ops
flatten_recursive	call	List
bert-master.modeling_test.BertModelTest.test_default	call	BertModelTester.__init__
run_tester	call	run
run_tester	followed_by	BertModelTester.__init__
List	followed_by	flatten_recursive
bert-master.modeling_test.BertModelTest.test_default	call	run_tester
run	followed_by	assert_all_tensors_reachable
run_tester	call	group
group	followed_by	run
run	followed_by	run
flatten_recursive	call	flatten_recursive
run_tester	call	assert_all_tensors_reachable
printable_text	followed_by	printable_text
__str__	call	printable_text
bert-master.create_pretraining_data.TrainingInstance.__repr__	call	__str__
convert_to_unicode	followed_by	convert_to_unicode
_create_examples	followed_by	_read_tsv
_create_examples	call	convert_to_unicode
_create_examples	call	InputExample.__init__
bert-master.run_classifier.ColaProcessor.get_dev_examples	call	_create_examples
convert_to_unicode	followed_by	InputExample.__init__
bert-master.run_classifier.ColaProcessor.get_dev_examples	call	_read_tsv
bert-master.run_classifier.XnliProcessor.get_train_examples	call	convert_to_unicode
bert-master.run_classifier.XnliProcessor.get_train_examples	call	InputExample.__init__
convert_to_unicode	followed_by	convert_to_unicode
_read_tsv	followed_by	convert_to_unicode
convert_to_unicode	followed_by	InputExample.__init__
bert-master.run_classifier.XnliProcessor.get_train_examples	call	_read_tsv
convert_to_unicode	followed_by	_clean_text
_tokenize_chinese_chars	call	_is_chinese_char
whitespace_tokenize	call	split
BasicTokenizer.__init__	followed_by	tokenize
bert-master.tokenization_test.TokenizationTest.test_chinese	call	tokenize
tokenize	call	whitespace_tokenize
tokenize	call	_run_split_on_punc
tokenize	call	convert_to_unicode
tokenize	call	_clean_text
_run_split_on_punc	followed_by	whitespace_tokenize
bert-master.tokenization_test.TokenizationTest.test_chinese	call	BasicTokenizer.__init__
_run_strip_accents	followed_by	_run_split_on_punc
_clean_text	followed_by	_tokenize_chinese_chars
whitespace_tokenize	followed_by	_run_strip_accents
_tokenize_chinese_chars	followed_by	whitespace_tokenize
_run_split_on_punc	call	List
_clean_text	call	_is_whitespace
tokenize	call	_tokenize_chinese_chars
tokenize	call	_run_strip_accents
_create_examples	call	InputExample.__init__
convert_to_unicode	followed_by	InputExample.__init__
convert_to_unicode	followed_by	convert_to_unicode
bert-master.run_classifier.MrpcProcessor.get_test_examples	call	_create_examples
bert-master.run_classifier.MrpcProcessor.get_test_examples	call	_read_tsv
_create_examples	followed_by	_read_tsv
_create_examples	call	convert_to_unicode
_create_examples	call	InputExample.__init__
bert-master.run_classifier.MnliProcessor.get_dev_examples	call	_read_tsv
bert-master.run_classifier.MnliProcessor.get_dev_examples	call	_create_examples
convert_to_unicode	followed_by	convert_to_unicode
_create_examples	followed_by	_read_tsv
convert_to_unicode	followed_by	InputExample.__init__
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	convert_to_unicode
bert-master.run_classifier.ColaProcessor.get_train_examples	call	_read_tsv
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	InputExample.__init__
_create_examples	call	InputExample.__init__
bert-master.run_classifier.ColaProcessor.get_train_examples	call	_create_examples
_create_examples	followed_by	_read_tsv
_create_examples	call	InputExample.__init__
convert_to_unicode	followed_by	convert_to_unicode
_create_examples	followed_by	_read_tsv
bert-master.run_classifier.MnliProcessor.get_train_examples	call	_read_tsv
bert-master.run_classifier.MnliProcessor.get_train_examples	call	_create_examples
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	InputExample.__init__
_create_examples	followed_by	_read_tsv
convert_to_unicode	followed_by	convert_to_unicode
convert_to_unicode	followed_by	InputExample.__init__
_create_examples	call	InputExample.__init__
_create_examples	call	convert_to_unicode
bert-master.run_classifier.MnliProcessor.get_test_examples	call	_create_examples
bert-master.run_classifier.MnliProcessor.get_test_examples	call	_read_tsv
bert-master.tokenization_test.TokenizationTest.test_is_control	call	_is_control
_is_control	followed_by	_is_control
