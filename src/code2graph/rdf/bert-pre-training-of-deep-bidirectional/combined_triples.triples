WordpieceTokenizer.__init__	followed_by	tokenize
convert_to_unicode	followed_by	whitespace_tokenize
bert-master.tokenization_test.TokenizationTest.test_wordpiece_tokenizer	call	tokenize
whitespace_tokenize	followed_by	List
tokenize	call	convert_to_unicode
whitespace_tokenize	call	split
tokenize	call	whitespace_tokenize
tokenize	followed_by	tokenize
tokenize	call	List
bert-master.tokenization_test.TokenizationTest.test_wordpiece_tokenizer	call	WordpieceTokenizer.__init__
_read_tsv	followed_by	convert_to_unicode
convert_to_unicode	followed_by	convert_to_unicode
bert-master.run_classifier.XnliProcessor.get_train_examples	call	InputExample.__init__
convert_to_unicode	followed_by	InputExample.__init__
bert-master.run_classifier.XnliProcessor.get_train_examples	call	convert_to_unicode
bert-master.run_classifier.XnliProcessor.get_train_examples	call	_read_tsv
convert_to_unicode	followed_by	InputExample.__init__
convert_to_unicode	followed_by	convert_to_unicode
bert-master.run_classifier.MnliProcessor.get_dev_examples	call	_read_tsv
_create_examples	call	convert_to_unicode
_create_examples	followed_by	_read_tsv
_create_examples	call	InputExample.__init__
bert-master.run_classifier.MnliProcessor.get_dev_examples	call	_create_examples
_clean_text	followed_by	_tokenize_chinese_chars
_clean_text	call	_is_whitespace
_tokenize_chinese_chars	followed_by	whitespace_tokenize
convert_to_unicode	followed_by	_clean_text
tokenize	call	_run_split_on_punc
_run_split_on_punc	call	List
tokenize	call	_run_strip_accents
whitespace_tokenize	followed_by	_run_strip_accents
tokenize	call	_tokenize_chinese_chars
_run_strip_accents	followed_by	_run_split_on_punc
tokenize	call	convert_to_unicode
BasicTokenizer.__init__	followed_by	tokenize
bert-master.tokenization_test.TokenizationTest.test_chinese	call	BasicTokenizer.__init__
whitespace_tokenize	call	split
bert-master.tokenization_test.TokenizationTest.test_chinese	call	tokenize
_tokenize_chinese_chars	call	_is_chinese_char
tokenize	call	whitespace_tokenize
tokenize	call	_clean_text
_run_split_on_punc	followed_by	whitespace_tokenize
get_shape_list	followed_by	cast
transpose_for_scores	call	reshape
ids_tensor	call	constant
control_dependencies	followed_by	get_variable
expand_dims	followed_by	cast
embedding_postprocessor	call	range
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	get_embedding_output
cast	call	reshape
variable_scope	has_arg0	intermediate
embedding_postprocessor	call	reshape
reshape	followed_by	layer_norm_and_dropout
transformer_model	call	reshape_from_matrix
ids_tensor	followed_by	ids_tensor
variable_scope	has_arg0	self
transformer_model	followed_by	get_activation
softmax	followed_by	dropout
embedding_postprocessor	call	matmul
BertModel.__init__	call	zeros
attention_layer	call	softmax
matmul	followed_by	transpose
transformer_model	call	dropout
range	followed_by	constant
cast	followed_by	softmax
BertModel.__init__	call	ones
variable_scope	has_arg0	encoder
transformer_model	call	variable_scope
assert_rank	call	get_variable_scope
transformer_model	call	layer_norm
transformer_model	call	range
get_shape_list	call	shape
embedding_postprocessor	call	one_hot
attention_layer	call	reshape
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	get_pooled_output
layer_norm	call	layer_norm
get_variable	followed_by	reshape
attention_layer	call	matmul
BertConfig.__init__	followed_by	BertModel.__init__
transformer_model	call	attention_layer
reshape	followed_by	transpose
embedding_lookup	call	expand_dims
variable_scope	followed_by	dense
one_hot	followed_by	matmul
embedding_postprocessor	call	get_shape_list
embedding_postprocessor	call	assert_less_equal
create_attention_mask_from_input_mask	call	get_shape_list
get_shape_list	followed_by	get_shape_list
embedding_lookup	call	get_shape_list
create_attention_mask_from_input_mask	call	cast
transpose_for_scores	followed_by	transpose_for_scores
dropout	followed_by	layer_norm
matmul	followed_by	gather
reshape_to_matrix	followed_by	range
BertModel.__init__	call	get_activation
dense	followed_by	transpose_for_scores
get_pooled_output	followed_by	get_all_encoder_layers
BertModel.__init__	call	squeeze
reshape_to_matrix	call	reshape
ones	followed_by	zeros
assert_rank	followed_by	shape
variable_scope	has_arg0	pooler
reshape	followed_by	one_hot
variable_scope	has_arg0	attention
assert_less_equal	followed_by	control_dependencies
multiply	call	sqrt
get_shape_list	call	assert_rank
zeros	has_dtype	tf.int32
attention_layer	call	dropout
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	get_sequence_output
embedding_lookup	call	get_variable
embedding_lookup	call	gather
embedding_postprocessor	followed_by	variable_scope
dropout	followed_by	reshape
BertModel.__init__	call	embedding_lookup
transformer_model	call	get_shape_list
multiply	followed_by	expand_dims
get_shape_list	followed_by	reshape
embedding_postprocessor	call	get_variable
BertModel.__init__	call	variable_scope
dense	followed_by	dense
variable_scope	followed_by	create_attention_mask_from_input_mask
get_shape_list	followed_by	reshape_to_matrix
transpose	followed_by	reshape
ids_tensor	followed_by	BertConfig.__init__
reshape_from_matrix	call	get_shape_list
reshape_to_matrix	followed_by	dense
matmul	followed_by	multiply
transformer_model	call	concat
transpose	followed_by	matmul
variable_scope	has_arg0	embeddings
BertModel.__init__	followed_by	get_embedding_output
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	BertConfig.__init__
zeros	followed_by	variable_scope
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	BertModel.__init__
embedding_postprocessor	call	layer_norm_and_dropout
ones	has_dtype	tf.int32
attention_layer	call	dense
attention_layer	call	reshape_to_matrix
variable_scope	has_arg0	output
get_shape_list	followed_by	ones
range	followed_by	variable_scope
attention_layer	call	multiply
get_activation	followed_by	variable_scope
embedding_lookup	followed_by	embedding_postprocessor
attention_layer	call	cast
squeeze	followed_by	dense
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	get_all_encoder_layers
attention_layer	call	transpose_for_scores
dense	followed_by	dropout
variable_scope	has_default_name	bert
cast	followed_by	ones
concat	followed_by	variable_scope
dense	followed_by	variable_scope
attention_layer	call	transpose
BertModel.__init__	call	dense
slice	followed_by	range
layer_norm_and_dropout	call	layer_norm
get_sequence_output	followed_by	get_pooled_output
reshape_to_matrix	followed_by	reshape_to_matrix
embedding_lookup	call	reshape
get_embedding_output	followed_by	get_sequence_output
BertModel.__init__	call	embedding_postprocessor
reshape	followed_by	reshape
dropout	call	dropout
create_attention_mask_from_input_mask	call	ones
reshape_from_matrix	call	reshape
transpose_for_scores	followed_by	matmul
embedding_lookup	call	matmul
layer_norm	followed_by	reshape_from_matrix
embedding_postprocessor	call	slice
variable_scope	followed_by	attention_layer
reshape_from_matrix	followed_by	reshape_from_matrix
BertModel.__init__	call	transformer_model
BertModel.__init__	call	get_shape_list
attention_layer	call	expand_dims
ids_tensor	call	range
layer_norm	followed_by	dropout
transformer_model	call	dense
variable_scope	followed_by	squeeze
create_attention_mask_from_input_mask	followed_by	transformer_model
attention_layer	followed_by	concat
gather	followed_by	get_shape_list
layer_norm	followed_by	variable_scope
embedding_postprocessor	call	control_dependencies
bert-master.modeling_test.BertModelTest.BertModelTester.create_model	call	ids_tensor
transpose_for_scores	call	transpose
reshape	followed_by	assert_less_equal
range	followed_by	reshape
attention_layer	call	get_shape_list
transformer_model	call	reshape_to_matrix
squeeze	has_axis	1
get_shape_list	followed_by	get_variable
expand_dims	followed_by	get_variable
variable_scope	followed_by	embedding_lookup
get_variable	followed_by	slice
ones	has_dtype	tf.float32
matmul	followed_by	reshape
layer_norm_and_dropout	call	dropout
variable_scope	followed_by	variable_scope
embedding_lookup	call	one_hot
BertModel.__init__	call	create_attention_mask_from_input_mask
file_based_convert_examples_to_features	call	create_int_feature
create_training_instances	call	range
file_based_convert_examples_to_features	call	close
main	call	read_examples
tokenize	followed_by	_strip_spaces
group	has_arg0	1
main	call	convert_examples_to_features
close	followed_by	input_fn_builder
range	followed_by	_check_is_max_context
convert_examples_to_features	call	_improve_answer_span
close	call	close
create_training_instances	followed_by	write_instance_to_example_files
main	call	get_labels
load	followed_by	is_whitespace
read_squad_examples	call	SquadExample.__init__
_tokenize_chinese_chars	call	_is_chinese_char
range	followed_by	create_instances_from_document
get_train_examples	followed_by	model_fn_builder
_improve_answer_span	followed_by	range
input_fn_builder	followed_by	get_test_examples
from_json_file	followed_by	get_labels
create_int_feature	call	Feature
_clean_text	followed_by	_tokenize_chinese_chars
shuffle	followed_by	List
create_instances_from_document	followed_by	shuffle
validate_case_matches_checkpoint	followed_by	from_json_file
FeatureWriter.__init__	followed_by	convert_examples_to_features
_run_strip_accents	followed_by	_run_split_on_punc
_tokenize_chinese_chars	followed_by	whitespace_tokenize
from_json_file	followed_by	validate_flags_or_throw
convert_examples_to_features	call	InputFeatures.__init__
convert_to_unicode	followed_by	_clean_text
convert_examples_to_features	call	_check_is_max_context
main	call	get_train_examples
get_final_text	call	tokenize
main	call	write_predictions
tokenize	call	_run_strip_accents
tokenize	call	whitespace_tokenize
convert_examples_to_features	call	printable_text
_check_is_max_context	followed_by	printable_text
main	call	write_instance_to_example_files
FullTokenizer.__init__	followed_by	create_training_instances
main	call	get_dev_examples
from_dict	call	BertConfig.__init__
write_predictions	call	split
create_tokenizer_from_hub_module	call	FullTokenizer.__init__
FullTokenizer.__init__	followed_by	read_examples
get_final_text	call	BasicTokenizer.__init__
tokenize	call	convert_to_unicode
main	call	FullTokenizer.__init__
FullTokenizer.__init__	followed_by	read_squad_examples
List	followed_by	range
file_based_convert_examples_to_features	call	Example
load_vocab	followed_by	BasicTokenizer.__init__
input_fn_builder	followed_by	input_fn_builder
FullTokenizer.__init__	call	BasicTokenizer.__init__
create_training_instances	call	create_instances_from_document
get_labels	followed_by	FullTokenizer.__init__
split	followed_by	get_final_text
write_instance_to_example_files	call	create_float_feature
_get_best_indexes	followed_by	split
Int64List	call	List
Example	followed_by	printable_text
get_final_text	followed_by	_compute_softmax
_strip_spaces	followed_by	_strip_spaces
.bert-master.tokenization_test	call	main
BasicTokenizer.__init__	followed_by	WordpieceTokenizer.__init__
_get_best_indexes	followed_by	_get_best_indexes
tokenize	call	_clean_text
List	followed_by	create_int_feature
write_instance_to_example_files	call	close
validate_flags_or_throw	call	validate_case_matches_checkpoint
main	call	validate_flags_or_throw
InputFeatures.__init__	followed_by	_truncate_seq_pair
get_dev_examples	followed_by	convert_examples_to_features
convert_examples_to_features	followed_by	input_fn_builder
convert_single_example	followed_by	create_int_feature
create_int_feature	followed_by	Example
from_json_file	call	from_dict
write_predictions	call	get_final_text
_get_best_indexes	call	range
main	call	read_squad_examples
from_json_file	followed_by	model_fn_builder
whitespace_tokenize	call	split
FloatList	call	List
is_whitespace	followed_by	whitespace_tokenize
file_based_input_fn_builder	followed_by	get_test_examples
_run_split_on_punc	followed_by	whitespace_tokenize
BasicTokenizer.__init__	followed_by	tokenize
convert_examples_to_features	call	_truncate_seq_pair
main	call	create_training_instances
file_based_input_fn_builder	followed_by	get_dev_examples
validate_flags_or_throw	followed_by	FullTokenizer.__init__
convert_examples_to_features	followed_by	close
write_predictions	call	_get_best_indexes
write_instance_to_example_files	call	create_int_feature
convert_single_example	call	printable_text
Feature	call	Int64List
main	followed_by	main
FullTokenizer.__init__	followed_by	get_train_examples
convert_single_example	call	InputFeatures.__init__
_clean_text	call	_is_whitespace
get_dev_examples	followed_by	file_based_convert_examples_to_features
input_fn_builder	followed_by	round
main	call	input_fn_builder
write_predictions	call	_compute_softmax
read_squad_examples	call	load
convert_examples_to_features	call	convert_single_example
model_fn_builder	followed_by	file_based_convert_examples_to_features
whitespace_tokenize	followed_by	SquadExample.__init__
printable_text	followed_by	close
create_training_instances	call	List
read_squad_examples	call	whitespace_tokenize
create_tokenizer_from_hub_module	followed_by	get_train_examples
validate_case_matches_checkpoint	call	group
range	followed_by	range
main	call	close
read_examples	followed_by	convert_examples_to_features
model_fn_builder	followed_by	FeatureWriter.__init__
Feature	call	FloatList
get_labels	followed_by	create_tokenizer_from_hub_module
get_final_text	call	_strip_spaces
printable_text	followed_by	printable_text
read_squad_examples	call	is_whitespace
main	call	model_fn_builder
run	followed_by	FullTokenizer.__init__
main	call	validate_case_matches_checkpoint
create_tokenizer_from_hub_module	call	run
convert_single_example	call	_truncate_seq_pair
printable_text	followed_by	InputFeatures.__init__
FullTokenizer.__init__	call	load_vocab
model_fn_builder	followed_by	input_fn_builder
input_fn_builder	followed_by	read_squad_examples
write_instance_to_example_files	call	List
main	call	file_based_convert_examples_to_features
convert_examples_to_features	call	range
round	has_arg0	6
create_tokenizer_from_hub_module	call	Session
List	followed_by	List
file_based_convert_examples_to_features	followed_by	file_based_input_fn_builder
_truncate_seq_pair	followed_by	printable_text
main	call	from_json_file
main	call	file_based_input_fn_builder
main	call	get_test_examples
create_float_feature	followed_by	create_int_feature
main	call	FeatureWriter.__init__
file_based_convert_examples_to_features	call	convert_single_example
tokenize	call	_run_split_on_punc
read_squad_examples	followed_by	FeatureWriter.__init__
whitespace_tokenize	followed_by	_run_strip_accents
main	call	round
convert_examples_to_features	followed_by	model_fn_builder
write_instance_to_example_files	call	Example
FullTokenizer.__init__	call	WordpieceTokenizer.__init__
Example	followed_by	close
create_int_feature	followed_by	create_float_feature
Example	call	Features
_check_is_max_context	call	min
write_instance_to_example_files	call	printable_text
create_int_feature	followed_by	create_int_feature
_improve_answer_span	call	range
shuffle	followed_by	model_fn_builder
main	call	create_tokenizer_from_hub_module
_compute_softmax	call	exp
input_fn_builder	followed_by	write_predictions
create_training_instances	call	shuffle
Session	followed_by	run
from_json_file	followed_by	FullTokenizer.__init__
model_fn_builder	followed_by	convert_examples_to_features
input_fn_builder	followed_by	get_dev_examples
read_squad_examples	followed_by	shuffle
_run_split_on_punc	call	List
main	call	shuffle
tokenize	call	_tokenize_chinese_chars
create_float_feature	call	Feature
get_test_examples	followed_by	file_based_convert_examples_to_features
whitespace_tokenize	followed_by	SquadExample.__init__
create_training_instances	call	shuffle
printable_text	followed_by	printable_text
InputFeatures.__init__	followed_by	_truncate_seq_pair
validate_flags_or_throw	followed_by	FullTokenizer.__init__
read_squad_examples	call	load
FullTokenizer.__init__	call	WordpieceTokenizer.__init__
main	call	from_json_file
write_instance_to_example_files	call	close
from_dict	call	BertConfig.__init__
split	followed_by	get_final_text
input_fn_builder	followed_by	read_squad_examples
from_json_file	followed_by	FullTokenizer.__init__
main	call	write_instance_to_example_files
get_final_text	followed_by	_compute_softmax
get_final_text	call	tokenize
main	call	create_training_instances
create_instances_from_document	followed_by	shuffle
_get_best_indexes	call	range
shuffle	followed_by	List
_clean_text	call	_is_whitespace
create_tokenizer_from_hub_module	followed_by	get_train_examples
get_test_examples	followed_by	file_based_convert_examples_to_features
write_instance_to_example_files	call	create_float_feature
write_instance_to_example_files	call	Example
FullTokenizer.__init__	followed_by	create_training_instances
_check_is_max_context	followed_by	printable_text
create_tokenizer_from_hub_module	call	FullTokenizer.__init__
read_squad_examples	followed_by	FeatureWriter.__init__
model_fn_builder	followed_by	file_based_convert_examples_to_features
_run_split_on_punc	followed_by	whitespace_tokenize
main	call	convert_examples_to_features
List	followed_by	range
read_examples	followed_by	convert_examples_to_features
Example	call	Features
input_fn_builder	followed_by	input_fn_builder
file_based_convert_examples_to_features	call	create_int_feature
write_instance_to_example_files	call	create_int_feature
convert_examples_to_features	followed_by	close
write_predictions	call	_get_best_indexes
load_vocab	followed_by	BasicTokenizer.__init__
convert_examples_to_features	call	_check_is_max_context
_get_best_indexes	followed_by	_get_best_indexes
create_training_instances	call	create_instances_from_document
Feature	call	Int64List
get_labels	followed_by	FullTokenizer.__init__
main	call	round
_improve_answer_span	call	range
create_training_instances	followed_by	write_instance_to_example_files
get_dev_examples	followed_by	convert_examples_to_features
main	call	write_predictions
convert_examples_to_features	call	InputFeatures.__init__
convert_examples_to_features	call	range
create_tokenizer_from_hub_module	call	run
tokenize	call	_run_split_on_punc
main	call	get_test_examples
main	call	FullTokenizer.__init__
write_predictions	call	_compute_softmax
List	followed_by	create_int_feature
input_fn_builder	followed_by	get_dev_examples
read_squad_examples	call	SquadExample.__init__
read_squad_examples	call	is_whitespace
_tokenize_chinese_chars	followed_by	whitespace_tokenize
file_based_input_fn_builder	followed_by	get_dev_examples
convert_single_example	call	InputFeatures.__init__
main	call	file_based_convert_examples_to_features
validate_case_matches_checkpoint	call	group
convert_examples_to_features	followed_by	input_fn_builder
main	call	FeatureWriter.__init__
convert_single_example	call	printable_text
tokenize	call	convert_to_unicode
whitespace_tokenize	call	split
convert_single_example	followed_by	create_int_feature
_improve_answer_span	followed_by	range
create_tokenizer_from_hub_module	call	Session
create_int_feature	followed_by	create_float_feature
input_fn_builder	followed_by	write_predictions
FullTokenizer.__init__	call	load_vocab
FloatList	call	List
create_float_feature	call	Feature
_run_strip_accents	followed_by	_run_split_on_punc
BasicTokenizer.__init__	followed_by	tokenize
create_float_feature	followed_by	create_int_feature
List	followed_by	List
create_int_feature	followed_by	Example
_strip_spaces	followed_by	_strip_spaces
main	call	create_tokenizer_from_hub_module
tokenize	call	whitespace_tokenize
write_instance_to_example_files	call	printable_text
convert_to_unicode	followed_by	_clean_text
main	call	file_based_input_fn_builder
model_fn_builder	followed_by	convert_examples_to_features
file_based_convert_examples_to_features	call	close
Feature	call	FloatList
main	call	close
_run_split_on_punc	call	List
_compute_softmax	call	exp
file_based_convert_examples_to_features	call	Example
main	call	validate_flags_or_throw
file_based_convert_examples_to_features	call	convert_single_example
Example	followed_by	printable_text
read_squad_examples	followed_by	shuffle
create_training_instances	call	List
convert_single_example	call	_truncate_seq_pair
convert_examples_to_features	followed_by	model_fn_builder
convert_examples_to_features	call	_truncate_seq_pair
Int64List	call	List
write_instance_to_example_files	call	List
model_fn_builder	followed_by	FeatureWriter.__init__
_get_best_indexes	followed_by	split
tokenize	call	_clean_text
validate_flags_or_throw	call	validate_case_matches_checkpoint
main	call	get_train_examples
printable_text	followed_by	close
main	call	get_dev_examples
main	call	input_fn_builder
load	followed_by	is_whitespace
FullTokenizer.__init__	call	BasicTokenizer.__init__
from_json_file	followed_by	get_labels
write_predictions	call	get_final_text
main	call	read_squad_examples
validate_case_matches_checkpoint	followed_by	from_json_file
close	followed_by	input_fn_builder
.bert-master.modeling_test	call	main
tokenize	followed_by	_strip_spaces
round	has_arg0	6
from_json_file	followed_by	model_fn_builder
convert_examples_to_features	call	printable_text
run	followed_by	FullTokenizer.__init__
convert_examples_to_features	call	_improve_answer_span
Session	followed_by	run
FullTokenizer.__init__	followed_by	read_examples
Example	followed_by	close
main	call	shuffle
FullTokenizer.__init__	followed_by	get_train_examples
from_json_file	call	from_dict
create_int_feature	followed_by	create_int_feature
tokenize	call	_run_strip_accents
tokenize	call	_tokenize_chinese_chars
main	call	model_fn_builder
main	call	validate_case_matches_checkpoint
read_squad_examples	call	whitespace_tokenize
create_int_feature	call	Feature
file_based_input_fn_builder	followed_by	get_test_examples
main	call	get_labels
input_fn_builder	followed_by	get_test_examples
FullTokenizer.__init__	followed_by	read_squad_examples
close	call	close
model_fn_builder	followed_by	input_fn_builder
_check_is_max_context	call	min
whitespace_tokenize	followed_by	_run_strip_accents
input_fn_builder	followed_by	round
main	followed_by	main
file_based_convert_examples_to_features	followed_by	file_based_input_fn_builder
create_training_instances	call	range
group	has_arg0	1
range	followed_by	create_instances_from_document
convert_examples_to_features	call	convert_single_example
_tokenize_chinese_chars	call	_is_chinese_char
get_train_examples	followed_by	model_fn_builder
main	call	read_examples
range	followed_by	_check_is_max_context
get_final_text	call	_strip_spaces
is_whitespace	followed_by	whitespace_tokenize
write_predictions	call	split
get_dev_examples	followed_by	file_based_convert_examples_to_features
get_labels	followed_by	create_tokenizer_from_hub_module
_clean_text	followed_by	_tokenize_chinese_chars
FeatureWriter.__init__	followed_by	convert_examples_to_features
_truncate_seq_pair	followed_by	printable_text
from_json_file	followed_by	validate_flags_or_throw
printable_text	followed_by	InputFeatures.__init__
range	followed_by	range
shuffle	followed_by	model_fn_builder
BasicTokenizer.__init__	followed_by	WordpieceTokenizer.__init__
get_final_text	call	BasicTokenizer.__init__
convert_to_unicode	followed_by	convert_to_unicode
convert_to_unicode	followed_by	InputExample.__init__
bert-master.run_classifier.MrpcProcessor.get_dev_examples	call	_create_examples
_create_examples	followed_by	_read_tsv
bert-master.run_classifier.MrpcProcessor.get_dev_examples	call	_read_tsv
_create_examples	call	InputExample.__init__
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	InputExample.__init__
bert-master.run_classifier.ColaProcessor.get_train_examples	call	_read_tsv
convert_to_unicode	followed_by	convert_to_unicode
bert-master.run_classifier.ColaProcessor.get_train_examples	call	_create_examples
_create_examples	call	InputExample.__init__
_create_examples	followed_by	_read_tsv
_create_examples	call	convert_to_unicode
bert-master.tokenization_test.TokenizationTest.test_is_punctuation	call	_is_punctuation
_is_punctuation	followed_by	_is_punctuation
to_json_string	call	to_dict
bert-master.modeling_test.BertModelTest.test_config_to_json_string	call	BertConfig.__init__
BertConfig.__init__	followed_by	to_json_string
bert-master.modeling_test.BertModelTest.test_config_to_json_string	call	to_json_string
_get_variable_name	followed_by	get_variable
bert-master.optimization_test.OptimizationTest.test_adam	call	reduce_mean
bert-master.optimization_test.OptimizationTest.test_adam	call	gradients
bert-master.optimization_test.OptimizationTest.test_adam	call	run
apply_gradients	call	_get_variable_name
reduce_mean	call	square
apply_gradients	call	get_variable
get_variable	followed_by	get_variable
multiply	followed_by	multiply
multiply	call	square
assign	followed_by	assign
run	followed_by	run
apply_gradients	call	sqrt
gradients	followed_by	get_or_create_global_step
assign	followed_by	group
apply_gradients	call	group
bert-master.optimization_test.OptimizationTest.test_adam	call	range
sqrt	followed_by	_do_use_weight_decay
group	has_arg0	1
bert-master.optimization_test.OptimizationTest.test_adam	call	constant
bert-master.optimization_test.OptimizationTest.test_adam	call	get_variable
_get_variable_name	call	group
bert-master.optimization_test.OptimizationTest.test_adam	call	AdamWeightDecayOptimizer.__init__
bert-master.optimization_test.OptimizationTest.test_adam	call	group
bert-master.optimization_test.OptimizationTest.test_adam	call	apply_gradients
apply_gradients	call	assign
multiply	followed_by	sqrt
bert-master.optimization_test.OptimizationTest.test_adam	call	get_or_create_global_step
apply_gradients	call	_do_use_weight_decay
range	has_arg0	100
get_variable	followed_by	constant
get_variable	followed_by	multiply
apply_gradients	call	multiply
bert-master.optimization_test.OptimizationTest.test_adam	call	trainable_variables
group	followed_by	run
get_variable	has_arg0	w
AdamWeightDecayOptimizer.__init__	followed_by	apply_gradients
_do_use_weight_decay	followed_by	assign
constant	followed_by	reduce_mean
apply_gradients	followed_by	group
get_or_create_global_step	followed_by	AdamWeightDecayOptimizer.__init__
trainable_variables	followed_by	gradients
run	followed_by	range
range	followed_by	run
reduce_mean	followed_by	trainable_variables
bert-master.tokenization_test.TokenizationTest.test_full_tokenizer	call	convert_tokens_to_ids
FullTokenizer.__init__	call	BasicTokenizer.__init__
BasicTokenizer.__init__	followed_by	WordpieceTokenizer.__init__
FullTokenizer.__init__	call	load_vocab
tokenize	followed_by	convert_tokens_to_ids
load_vocab	followed_by	BasicTokenizer.__init__
tokenize	followed_by	tokenize
convert_tokens_to_ids	call	convert_by_vocab
bert-master.tokenization_test.TokenizationTest.test_full_tokenizer	call	tokenize
tokenize	call	tokenize
FullTokenizer.__init__	call	WordpieceTokenizer.__init__
bert-master.tokenization_test.TokenizationTest.test_full_tokenizer	call	FullTokenizer.__init__
FullTokenizer.__init__	followed_by	tokenize
_read_tsv	followed_by	convert_to_unicode
bert-master.run_classifier.XnliProcessor.get_dev_examples	call	_read_tsv
convert_to_unicode	followed_by	InputExample.__init__
bert-master.run_classifier.XnliProcessor.get_dev_examples	call	convert_to_unicode
convert_to_unicode	followed_by	convert_to_unicode
bert-master.run_classifier.XnliProcessor.get_dev_examples	call	InputExample.__init__
convert_tokens_to_ids	call	convert_by_vocab
bert-master.tokenization_test.TokenizationTest.test_convert_tokens_to_ids	call	convert_tokens_to_ids
convert_to_unicode	followed_by	InputExample.__init__
_create_examples	followed_by	_read_tsv
_create_examples	call	convert_to_unicode
_create_examples	call	InputExample.__init__
bert-master.run_classifier.MnliProcessor.get_train_examples	call	_read_tsv
bert-master.run_classifier.MnliProcessor.get_train_examples	call	_create_examples
convert_to_unicode	followed_by	convert_to_unicode
bert-master.tokenization.FullTokenizer.convert_ids_to_tokens	call	convert_by_vocab
convert_examples_to_features	call	_improve_answer_span
write_predictions	call	_get_best_indexes
write_instance_to_example_files	call	Example
_improve_answer_span	call	range
main	call	get_dev_examples
range	followed_by	range
FullTokenizer.__init__	call	WordpieceTokenizer.__init__
_tokenize_chinese_chars	call	_is_chinese_char
convert_single_example	call	InputFeatures.__init__
create_training_instances	followed_by	write_instance_to_example_files
write_instance_to_example_files	call	close
get_dev_examples	followed_by	file_based_convert_examples_to_features
read_squad_examples	call	load
input_fn_builder	followed_by	get_test_examples
main	call	validate_case_matches_checkpoint
input_fn_builder	followed_by	read_squad_examples
_run_split_on_punc	followed_by	whitespace_tokenize
get_test_examples	followed_by	file_based_convert_examples_to_features
create_tokenizer_from_hub_module	call	run
write_instance_to_example_files	call	create_int_feature
tokenize	followed_by	_strip_spaces
Feature	call	Int64List
model_fn_builder	followed_by	file_based_convert_examples_to_features
_check_is_max_context	followed_by	printable_text
get_labels	followed_by	create_tokenizer_from_hub_module
create_instances_from_document	followed_by	shuffle
create_training_instances	call	range
main	call	FeatureWriter.__init__
convert_to_unicode	followed_by	_clean_text
input_fn_builder	followed_by	input_fn_builder
BasicTokenizer.__init__	followed_by	tokenize
main	followed_by	main
create_int_feature	followed_by	create_float_feature
_strip_spaces	followed_by	_strip_spaces
from_json_file	followed_by	get_labels
shuffle	followed_by	List
create_training_instances	call	List
main	call	read_examples
convert_examples_to_features	call	printable_text
read_squad_examples	followed_by	shuffle
tokenize	call	_clean_text
FullTokenizer.__init__	call	BasicTokenizer.__init__
List	followed_by	create_int_feature
create_tokenizer_from_hub_module	call	Session
from_json_file	followed_by	FullTokenizer.__init__
_check_is_max_context	call	min
create_tokenizer_from_hub_module	followed_by	get_train_examples
create_int_feature	followed_by	Example
FullTokenizer.__init__	followed_by	create_training_instances
List	followed_by	range
InputFeatures.__init__	followed_by	_truncate_seq_pair
main	call	write_instance_to_example_files
_clean_text	followed_by	_tokenize_chinese_chars
get_dev_examples	followed_by	convert_examples_to_features
create_tokenizer_from_hub_module	call	FullTokenizer.__init__
FullTokenizer.__init__	followed_by	read_squad_examples
is_whitespace	followed_by	whitespace_tokenize
printable_text	followed_by	InputFeatures.__init__
.bert-master.optimization_test	call	main
convert_examples_to_features	call	convert_single_example
tokenize	call	whitespace_tokenize
round	has_arg0	6
model_fn_builder	followed_by	convert_examples_to_features
FullTokenizer.__init__	followed_by	get_train_examples
from_json_file	followed_by	model_fn_builder
main	call	input_fn_builder
FullTokenizer.__init__	call	load_vocab
_compute_softmax	call	exp
tokenize	call	convert_to_unicode
main	call	read_squad_examples
range	followed_by	create_instances_from_document
read_squad_examples	call	SquadExample.__init__
create_float_feature	call	Feature
_improve_answer_span	followed_by	range
_truncate_seq_pair	followed_by	printable_text
main	call	get_test_examples
get_labels	followed_by	FullTokenizer.__init__
main	call	get_train_examples
write_instance_to_example_files	call	create_float_feature
main	call	from_json_file
write_predictions	call	_compute_softmax
main	call	get_labels
create_training_instances	call	shuffle
from_json_file	call	from_dict
convert_examples_to_features	followed_by	close
split	followed_by	get_final_text
_tokenize_chinese_chars	followed_by	whitespace_tokenize
write_instance_to_example_files	call	printable_text
main	call	create_tokenizer_from_hub_module
file_based_input_fn_builder	followed_by	get_dev_examples
input_fn_builder	followed_by	get_dev_examples
main	call	write_predictions
create_training_instances	call	create_instances_from_document
group	has_arg0	1
get_final_text	call	BasicTokenizer.__init__
model_fn_builder	followed_by	input_fn_builder
input_fn_builder	followed_by	round
validate_case_matches_checkpoint	followed_by	from_json_file
Example	call	Features
write_predictions	call	split
get_final_text	call	tokenize
convert_examples_to_features	call	_truncate_seq_pair
convert_examples_to_features	followed_by	input_fn_builder
validate_flags_or_throw	call	validate_case_matches_checkpoint
shuffle	followed_by	model_fn_builder
Int64List	call	List
main	call	convert_examples_to_features
_run_strip_accents	followed_by	_run_split_on_punc
printable_text	followed_by	printable_text
close	followed_by	input_fn_builder
convert_examples_to_features	call	InputFeatures.__init__
Feature	call	FloatList
run	followed_by	FullTokenizer.__init__
validate_case_matches_checkpoint	call	group
_get_best_indexes	followed_by	_get_best_indexes
create_float_feature	followed_by	create_int_feature
whitespace_tokenize	call	split
file_based_convert_examples_to_features	followed_by	file_based_input_fn_builder
convert_single_example	call	_truncate_seq_pair
_run_split_on_punc	call	List
convert_examples_to_features	followed_by	model_fn_builder
main	call	close
range	followed_by	_check_is_max_context
get_train_examples	followed_by	model_fn_builder
tokenize	call	_tokenize_chinese_chars
model_fn_builder	followed_by	FeatureWriter.__init__
from_dict	call	BertConfig.__init__
tokenize	call	_run_split_on_punc
from_json_file	followed_by	validate_flags_or_throw
main	call	shuffle
get_final_text	call	_strip_spaces
Example	followed_by	close
List	followed_by	List
tokenize	call	_run_strip_accents
close	call	close
read_squad_examples	call	whitespace_tokenize
file_based_input_fn_builder	followed_by	get_test_examples
Example	followed_by	printable_text
main	call	model_fn_builder
convert_single_example	followed_by	create_int_feature
FloatList	call	List
FeatureWriter.__init__	followed_by	convert_examples_to_features
file_based_convert_examples_to_features	call	Example
file_based_convert_examples_to_features	call	convert_single_example
main	call	file_based_convert_examples_to_features
main	call	FullTokenizer.__init__
read_squad_examples	call	is_whitespace
main	call	file_based_input_fn_builder
main	call	create_training_instances
_clean_text	call	_is_whitespace
Session	followed_by	run
convert_single_example	call	printable_text
printable_text	followed_by	close
read_squad_examples	followed_by	FeatureWriter.__init__
create_int_feature	followed_by	create_int_feature
file_based_convert_examples_to_features	call	close
convert_examples_to_features	call	_check_is_max_context
whitespace_tokenize	followed_by	_run_strip_accents
main	call	round
FullTokenizer.__init__	followed_by	read_examples
file_based_convert_examples_to_features	call	create_int_feature
_get_best_indexes	call	range
load	followed_by	is_whitespace
create_int_feature	call	Feature
load_vocab	followed_by	BasicTokenizer.__init__
BasicTokenizer.__init__	followed_by	WordpieceTokenizer.__init__
write_instance_to_example_files	call	List
write_predictions	call	get_final_text
whitespace_tokenize	followed_by	SquadExample.__init__
main	call	validate_flags_or_throw
get_final_text	followed_by	_compute_softmax
convert_examples_to_features	call	range
_get_best_indexes	followed_by	split
validate_flags_or_throw	followed_by	FullTokenizer.__init__
read_examples	followed_by	convert_examples_to_features
input_fn_builder	followed_by	write_predictions
bert-master.tokenization.convert_ids_to_tokens	call	convert_by_vocab
_clean_text	followed_by	_tokenize_chinese_chars
bert-master.tokenization_test.TokenizationTest.test_basic_tokenizer_lower	call	BasicTokenizer.__init__
tokenize	call	_run_split_on_punc
BasicTokenizer.__init__	followed_by	tokenize
_run_split_on_punc	followed_by	whitespace_tokenize
tokenize	call	convert_to_unicode
tokenize	call	_clean_text
tokenize	call	whitespace_tokenize
tokenize	followed_by	tokenize
convert_to_unicode	followed_by	_clean_text
_run_strip_accents	followed_by	_run_split_on_punc
_tokenize_chinese_chars	followed_by	whitespace_tokenize
_clean_text	call	_is_whitespace
bert-master.tokenization_test.TokenizationTest.test_basic_tokenizer_lower	call	tokenize
_tokenize_chinese_chars	call	_is_chinese_char
whitespace_tokenize	call	split
whitespace_tokenize	followed_by	_run_strip_accents
_run_split_on_punc	call	List
tokenize	call	_run_strip_accents
tokenize	call	_tokenize_chinese_chars
convert_to_unicode	followed_by	convert_to_unicode
_create_examples	call	InputExample.__init__
_create_examples	followed_by	_read_tsv
bert-master.run_classifier.ColaProcessor.get_test_examples	call	_create_examples
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	InputExample.__init__
bert-master.run_classifier.ColaProcessor.get_test_examples	call	_read_tsv
bert-master.run_classifier.ColaProcessor.get_dev_examples	call	_read_tsv
_create_examples	followed_by	_read_tsv
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	convert_to_unicode
_create_examples	call	InputExample.__init__
convert_to_unicode	followed_by	InputExample.__init__
bert-master.run_classifier.ColaProcessor.get_dev_examples	call	_create_examples
flatten_recursive	call	List
List	followed_by	flatten_recursive
run	followed_by	run
run_tester	followed_by	BertModelTester.__init__
run_tester	call	run
run_tester	call	assert_all_tensors_reachable
bert-master.modeling_test.BertModelTest.test_default	call	run_tester
run_tester	call	group
flatten_recursive	call	flatten_recursive
run	followed_by	assert_all_tensors_reachable
group	followed_by	run
get_unreachable_ops	call	flatten_recursive
bert-master.modeling_test.BertModelTest.test_default	call	BertModelTester.__init__
assert_all_tensors_reachable	call	get_unreachable_ops
BasicTokenizer.__init__	followed_by	tokenize
_tokenize_chinese_chars	call	_is_chinese_char
tokenize	call	_run_strip_accents
tokenize	call	whitespace_tokenize
whitespace_tokenize	followed_by	_run_strip_accents
tokenize	call	_run_split_on_punc
_clean_text	call	_is_whitespace
whitespace_tokenize	call	split
_run_split_on_punc	call	List
tokenize	call	convert_to_unicode
tokenize	call	_clean_text
bert-master.tokenization_test.TokenizationTest.test_basic_tokenizer_no_lower	call	tokenize
_run_split_on_punc	followed_by	whitespace_tokenize
tokenize	call	_tokenize_chinese_chars
_tokenize_chinese_chars	followed_by	whitespace_tokenize
convert_to_unicode	followed_by	_clean_text
_run_strip_accents	followed_by	_run_split_on_punc
_clean_text	followed_by	_tokenize_chinese_chars
bert-master.tokenization_test.TokenizationTest.test_basic_tokenizer_no_lower	call	BasicTokenizer.__init__
printable_text	followed_by	printable_text
__repr__	call	printable_text
bert-master.run_squad.SquadExample.__str__	call	__repr__
_create_examples	call	InputExample.__init__
convert_to_unicode	followed_by	InputExample.__init__
convert_to_unicode	followed_by	convert_to_unicode
_create_examples	call	convert_to_unicode
bert-master.run_classifier.MnliProcessor.get_test_examples	call	_read_tsv
bert-master.run_classifier.MnliProcessor.get_test_examples	call	_create_examples
_create_examples	followed_by	_read_tsv
bert-master.run_classifier.MrpcProcessor.get_train_examples	call	_create_examples
bert-master.run_classifier.MrpcProcessor.get_train_examples	call	_read_tsv
_create_examples	call	InputExample.__init__
_create_examples	followed_by	_read_tsv
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	InputExample.__init__
convert_to_unicode	followed_by	convert_to_unicode
bert-master.tokenization_test.TokenizationTest.test_is_control	call	_is_control
_is_control	followed_by	_is_control
bert-master.run_classifier.MrpcProcessor.get_test_examples	call	_create_examples
bert-master.run_classifier.MrpcProcessor.get_test_examples	call	_read_tsv
_create_examples	call	InputExample.__init__
convert_to_unicode	followed_by	InputExample.__init__
_create_examples	call	convert_to_unicode
convert_to_unicode	followed_by	convert_to_unicode
_create_examples	followed_by	_read_tsv
bert-master.tokenization_test.TokenizationTest.test_is_whitespace	call	_is_whitespace
_is_whitespace	followed_by	_is_whitespace
printable_text	followed_by	printable_text
bert-master.create_pretraining_data.TrainingInstance.__repr__	call	__str__
__str__	call	printable_text
