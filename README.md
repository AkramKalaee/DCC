# Deep Code Curation (DCC)

Deep learning (DL) is a field growing at an explosive rate. Despite being a discipline of Computer Science, DL is still considered an *“art”* due to the intricacies of translating complex mathematical concepts to highly efficient computer code. Today, DL models are mostly hand-curated by experts who spend their time reading through the latest publications, making sense of the claims and results, and using their expertise to create links between the body of knowledge to validate new ideas. To ease the process, the DL academic community has encouraged its members to share both the source code and data sets they use in their papers. However, many authors have the tendency to re-write the same underlying equations or diagrams in different ways in an attempt to appear novel. This causes unnecessary clutter that DL experts need to parse through to distinguish relevant ideas. Similarly, the available source code is often a *“hack”* and works only for a specific set of parameters, data sets, or library versions. Making matters worse, there is a shortage of DL talent in industry and academia. 

Our project, the **Deep Code Currator (DCC)**, will use state-of-the-art learning methods to *machine-curate* DL scientific papers and source code. The resulting multi-modal model, referred to as *Deep Knowledge Graph (DKG)*, will provide a reference model useful to cross-reference the various modalities of DL knowledge, including text, equations, figures, and source code. The DKG, for the first time, will provide a model to quantify novelty, similarity, and validate existing and new DL architectures. DCC will instantiate the DKG by parsing papers and source code from public sources such as arXiv.org (cs.AI) and PapersWithCode.com. For each paper, we will fuse the various modalities (text, figures/diagrams, equations) in a homogeneous DKG representation using a combination of deep learning and logic reasoning methods. The DKG could be queried by users for rich explanations and hypothesis on the similarity, novelty, and equation and code completion.

The impact of the Deep Code Curator is that it will dramatically decrease the time, effort and cost spent in curating the deep learning literature and associated source code. This will drive faster adoption of AI techniques in products and services that are vital to national security and industrial and scientific innovation. 

The overall architecture of our system is shown in the following figure. 
![generalarchitecture](https://user-images.githubusercontent.com/48071735/53749388-41f23980-3e75-11e9-8eca-9c31fc7ce9b1.png)

As can be seen it consists of two main parts. The first shows the details for the proposed curation process, where publications describing deep learning methods along with their source codes are sent to a graph extraction module. This module is able to extract atomic facts from the three major modalities: text, images, and code. These facts can be quite diverse and contain overlapping and complementary information. To convert this to a standard Knowledge Graph representation model, they are sent to a mapping system, that converts these facts to RDF triples with the right vocabularies. Next the knowledge graphs are aligned based on their structural similarities. The final aligned graph is then stored in a data store known as *triple stores*. These triple stores can be queried by pattern query languages (e.g. SPARQL), that would allow expressive querying with complex conditions. 
In the second component (at the bottom of the overall architecture) we show how the curated knowledge graphs can be used to automatically infer code for papers that are not accompanied by implementation details (source code). First an aligned knowledge graph, without the components associated with the source code is extracted from those papers. Next the corresponding code graph is inferred by using previous related papers that are accompanied with code. 

More details can be found in the "Reports" folder.
